\documentclass[sigplan,10pt,review,anonymous]{acmart}

% Packages {{{
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{graphs}
\usetikzlibrary{cd}
\usepackage{bussproofs}
\usepackage{stmaryrd}
% }}}

% Macros {{{

\newcommand{\kw}[1]{\ensuremath{ \textsf{#1} }}
\newcommand{\ifr}[1]{\ [{#1}]\ }
\newcommand{\ifrw}[2]{\ [{#1}]_{#2}\ }
\newcommand{\alt}{\ |\ }

\newcommand{\EC}{\kw{C}}
\newcommand{\simrel}{\kw{simrel}}

% Moves
\newcommand{\mcall}[3]{\kw{#1}({#2})@{#3}}
\newcommand{\pcall}[3]{%
  \underline{\mcall{#1}{#2}{#3}}%
}
\newcommand{\mret}[2]{{#1}@{#2}}
\newcommand{\pret}[2]{%
  \underline{\mret{#1}{#2}}%
}
\newcommand{\mretx}[3]{{#1}@{#2}/{#3}}
\newcommand{\pretx}[3]{%
  \underline{\mretx{#1}{#2}{#3}}%
}

% Pointers for justified sequences %{{{

% Parameters
\newcommand{\pshift}{1.6ex}
\newcommand{\pcdist}{2.5}
\newcommand{\pcangle}{60}

% Pointer hook
\newcommand{\ph}[1]{%
  \tikz[remember picture]{\coordinate (#1);}}

% Pointer to
\newcommand{\pt}[1]{%
  \rule{0pt}{1.4em}%
  \tikz[remember picture, overlay]{
    \draw[->]
      let \p{dest} = (#1),
          \n1 = {ln(veclen(\x{dest}, \y{dest}) + 1)},
          \p1 = ($(0,0)+(0,\pshift)$),
          \p4 = ($(#1)+(0,\pshift)$),
          \p2 = ($(\p1)!\n1*\pcdist!-\pcangle:(\p4)$),
          \p3 = ($(\p4)!\n1*\pcdist!+\pcangle:(\p1)$) in
        (\p1) .. controls (\p2) and (\p3) .. (\p4);}}

%}}}

% }}}

% Various parameters {{{
\bibliographystyle{ACM-Reference-Format}
\citestyle{acmnumeric}
%}}}

\begin{document}

\title[%
  A Compiler for Certified Software%
]{%
  A Compiler Suitable for Building Certified Software%
}

\author{J\'er\'emie Koenig}
\affiliation{
  \department{Computer Science}
  \institution{Yale University}
}
\email{jeremie.koenig@yale.edu}

\begin{abstract} %{{{
\end{abstract}
%}}}

\maketitle

\section{Introduction} %{{{

\subsection{[Context]}

The state of the art in formal verification is,
we can verify individual artefacts of decent size
(Compcert, CertiKOS, seL4, file systems, CPUs, network protocols).
But the grand challenge is figuring out
how to connect such components together
to obtain large-scale, end-to-end verified systems.

[name-drop cyber-physical systems etc.]

\subsection{[Gap]}

Most research has focused on how to make
single components more reliable.
But this leaves out the important concern of
where these component sit within a larger system.
We do not just want \emph{system components that are certified},
instead we want \emph{components of certified systems}.

To do that,
we need to inject into our thinking
a more systematic view of
how large-scale systems are constructed and
how to reason about them,
and use that insight to 
and develop design principles for
components of certified systems
and the mathematical tools we use to analyze them.

In that context,
compilers are particularly interesting and relevant
because they are a ubiquitous tool
involved at every layer
in the construction of large-scale software systems.
The work around Compcert illustrates the needed shift:
the frame has been ``how can I make sure my compiler doesn't have bug?'',
``how can I make sure my compiler doesn't have bugs
in terms of individual modules rather than whole programs?''
(still an open problem kind of).
To achive end-to-end verification we need to answer the question
``how can I use my compiler to build a certified system''.

\subsection{[Innovation]}

We identify six criteria for theories of systems (ie. semantic domains)
to be suitable in the context of building larger systems:
expressivity, abstraction, refinement,
compositionality, open systems, resources.

We apply this analytical framework (or rather 1-5)
to the problem of certified compilation,
and to Compcert in particular.
Our criteria suggest natural,
minimal ways in which the semantic framework
used by Compcert should be extended.

We show that we can extend the correctness proof of Compcert
to fit this new framework and
demonstrate how the resulting artefact
can be used to construct fancy certified stuff.

Our analytical framework is also a good way to:
evaluate the limitations of our work
(we need more expressivity for concurrency,
we don't have a good story for resources);
classify and compare previous work on compositional compilation;
map out promising directions for future work.

{\color{gray} %{{{

\subsection{Obsolete ramblings}

[XXX: Redistribute into the main ideas section]

Challenges:
\begin{enumerate}
\item Expressivity.
  [Need not be achieved all at once,
  if we can embed the semantic domain used to analyse a system
  into a broader one.
  The bar should be,
  our formalism should be rich enough to account for
  the full range of possible interaction of a system with its environment
  in the real world.
  Then there should be a way to embed in the formalism
  used to analyze / build any larger system of which it is a component.
  We demonstrate this when we build our richer semantics of Compcert in Sec~4
  and embed our minimalistic one from Sec~3.]
\item Compositionality.
  [Complex systems can only be understood
  as the relationship of simpler components,
  which can be reasoned about in isolation.]
\item Open systems.
  [Compositionality should work from the bottom up, not top down.
  Components should not be understood as fragments of a fixed whole.
  There is no whole system.
  Every system is a component.
  \emph{But},
  it may be reasonable to partially close a subsystem
  after we built it up from components,
  as long as the resulting one
  is still able to interact with its environment]
\item Abstraction.
  [Reductionism is shit.
  You can't \emph{understand} a book
  as a collection of atoms of ink, paper and glue.
  Let alone how that relates to the corresponding e-book.
  Let alone its place within a genre of literature.
  Every level of abstraction exists in its own right.
  Its nature cannot be explained by a particular realization.
  The relation between them ]
\item Refinement.
  [The role of a component is realized in a specific way.
  We don't want to sweat the details when looking at the big picture.]
\item Resources.
  [The role of a component is realized in an imperfect way.
  An ideal, infinite model only exists in the real world
  as a series of finite approximations;
  resource limitations introduce a discontinuity.
  Abstractions break down beyond certain threshold:
  we run out of stack space,
  insufficient network capacity introduces congestion,
  a heap becomes too fragmented to satisfy
  requests for a contiguous block of memory.

  A satisfactory treatment of resources
  allows us to caracterize the range of conditions
  under which refinement and abstraction hold,
\end{enumerate}

In the context of the theory programming languages,
[enumerate things that have tackled combinations of these
challenges:
subtyping -> refinement;
traditional game semantics -> expressivity, compositionality, open systems;
interface automata -> compositionality, $\approx$ open systems, refinement;
certikos -> abstraction, refinement;
lax modality -> compositionality, resources;
logical relations -> abstraction and refinement (sometimes), compositionality;
...]

Context of Compcert:
original compcert uses relatively expressive model
(event traces are fairly general),
and has a notion of refinement (inclusion of sets of behaviors, Vundef),
but not compositional (whole program),
not that open (it's unclear how to connect all kinds of interesting things),
no abstraction (behavior of source and target expressed in same model),
no account of resources (infinite stack at Asm).

[mention that Compcert's model of the userspace is very naive;
impossible to encode a specification such as POSIX
as external function semantics]

Various works seek to extend to support some of these things:
CompCompCert, separate compcert, CompCertX, Qompcert

In this paper:
sketch something for challenges 2-5
out of (traditional game semantics + interface automata + abstraction layers).
Illustrate how these ideas play out in the context of compilers,
and apply them to solve the open problem of
compositional certified compilation.

\subsection{Contributions}

Specifically,
we identify six principles [...]
Give a clear ``test'' for each.
Can guide design.

We apply our analytical framework to the problem of certified compilation:
\begin{itemize}
\item analyze previous work in pl semantics and certified compilation
  in this framework to assess and explain the strenghts and weaknesses of each
  [sec. 12 Related Work]
\item show [in sec. 3 Semantics with External Calls]
  how these ideas yield a natural solution
  when applied to the open problem of compositional compilation
\item formulate new challenge / next step:
  that of a compiler which can be used as a component
  for end-to-end verification;
\item In Sec.~4 [richer semantics],
  show that our semantic model can be made more expressive
  in a way that our compiler remains correct in that setting;
\item In Sec.~5 [applications],
  illustrate with some applications
  the ways in which our compiler can be connected
  to a larger system
\item Assess the remaining gap between our compiler
  and our new challenge (concurrency, resources),
  and apply our analytical framework to suggest
  what a solution might look like.
\end{itemize}

[Basic claim: the answer to compositional verification
is an undestanding of abstraction and refinement in the context of games.]
The present work applies this analysis to
solve the open problem of certified compositional compilation of low-level languages
plus an understanding of refinement and abstraction in that context.]

\subsection{Limitations}

No concurrency
(not expressive enough:
accesses to memory between external calls are not observable
--- this being said existing work on Concurrent CertiKOS shows
it may be possible to map our model in a more general one),
no good story for resources.

In Sec.~N [Future Work] [spell out some leads to fill these gaps.]

} %}}}

%}}}

\section{Main ideas} %{{{

\subsection{Languages} %{{{

Formally,
a programming language $L$ is understood as
a set of programs $p$
which are assigned a meaning $\llbracket p \rrbracket$
in a semantic domain $\mathbb{D}$.
We call \emph{programs} or \emph{modules} the elements of $L$,
preferring \emph{module} when
the language supports
a notion of horizontal composition (see Sec.~\ref{sec:cccomp}).
We call \emph{behaviors} or \emph{specifications}
the elements of $\mathbb{D}$,
preferring \emph{specification} when
the semantic domain is equipped with
a notion of refinement,
and the specification under consideration
may be refined by more specific behaviors.

As a running example,
we present two simple languages
whose programs define single-argument integer functions.

[I was hoping this could provide a way to illustrate
the concepts below,
but now I'm not sure if it's worth the space
and reader's mental energy]

\begin{example}[$\kw{AlgExp}$] %{{{
We consider algebraic expressions
built out of integer constants, $+$, $\times$,
and a single variable $x$.
This language $\kw{AlgExp}$ is defined by the following grammar:
\[
  e \in \kw{AlgExp} ::= c \:|\: x \:|\: (e + e) \:|\: (e \times e) \,,
\]
where $c \in \mathbb{Z}$ is any constant,
and $x$ is a terminal symbol.
One ``program'' in this language is the following expression:
\[
  e_1 := ((x \times (3 + x)) + 2)
\]
An expression $e$ in \kw{AlgExp} is assigned a meaning
$\llbracket e \rrbracket : \mathbb{Z} \rightarrow \mathbb{Z}$
as follows:
to compute $\llbracket e \rrbracket(n)$,
substitute $n$ for $x$ in $e$,
then evaluate the resulting integer expression.
For example:
\[
  \llbracket e_1 \rrbracket (n) = n^2 + 3n + 2 \,.
\]
\end{example}
%}}}

\begin{example}[$\kw{RegProg}$] %{{{
Consider a machine with an infinite number of registers,
which can perform elementary algebraic operations.
The grammar of programs is:
\begin{align*}
  r \in \kw{RegName} &::= \kw{r1} \:|\: \kw{r2} \:|\: \cdots \\
  e \in \kw{RegExpr} &::= c \:|\: r \:|\: r + r \:|\: r \times r \\
  p \in \kw{RegProg} &::= r \leftarrow e; \, p \:|\: \bullet \,,
\end{align*}
where $\bullet$ denotes the empty program.
An example is:
\[
  p_1 :=
  \kw{r2} \leftarrow 1; \,
  \kw{r2} \leftarrow \kw{r1} + \kw{r2}; \,
  \kw{r3} \leftarrow \kw{r2} \times \kw{r2}; \,
  \kw{r1} \leftarrow \kw{r2} + \kw{r3};
\]
Each assignment $r \leftarrow e$ evaluates the computation $e$,
then stores the result in register $r$.
To interpret programs as functions,
we initialize $\kw{r1}$ with the value of the argument,
execute the program's assignments in sequence,
then read out the answer from the new contents of $\kw{r1}$.
The reader is invited to check that
the behavior $\llbracket p_1 \rrbracket$ associated to the program above
is the same as that of $e_1$.
\end{example}
%}}}

The $\kw{AlgExp}$ and $\kw{RegOps}$ languages
are sufficient to illustrate a number of interesting phenomena.
Because of their simplicity,
it will be possible to do so with some degree of formality.
Concurrently,
we will discuss in a more casual manner
the way in which these phenomena play out
in the context of the compilation of C to assembly languages.

%}}}

\subsection{Compilers} %{{{

Given a source language $L_s$
and a target language $L_t$,
a compiler can be understood as a function
$C : L_s \rightarrow L_t$
which transforms a program $p \in L_s$
into a program $C(p) \in L_t$.
When the two languages are interpreted into
a common semantic domain,
the correctness of the compiler can be stated as:
\[
  \llbracket p \rrbracket_s = \llbracket C(p) \rrbracket_t \,.
\]

\begin{example}[\kw{AlgExp} to \kw{RegOps}] %{{{
To illustrate this definition,
we define a simple compiler $C_1 : \kw{AlgExp} \rightarrow \kw{RegOps}$.
Compiling simple expressions is straightforward:
\[
  C_1(c) := \kw{r1} \leftarrow c; \qquad
  C_1(x) := \bullet
\]
Compiling binary operations is more involved,
because we need to evaluate each operand,
making sure that the first computation does not overwrite
the input value,
and that the second computation does not overwrite
the result of the first.
To this end,
we define a \emph{shift} operator ${\uparrow} p$,
which replaces every register name in $p$ by its successor,
so that for example:
\[
  {\uparrow} p_1 =
  \kw{r3} \leftarrow \kw{r2} + 1; \,
  \kw{r4} \leftarrow \kw{r3} \times \kw{r3}; \,
  \kw{r2} \leftarrow \kw{r3} + \kw{r4};
\]
We can now define for each binary operation $* \in \{+, \times\}$:
\begin{align*}
  C(e_1 * e_2) = \quad  % XXX overloading e_1
    &\kw{r2} \leftarrow \kw{r1}; \, % XXX copy not allowed
    {\uparrow} C(e_1); \\
    &\kw{r3} \leftarrow \kw{r1}; \,
    {\uparrow\uparrow} C(e_2); \\
    &\kw{r1} \leftarrow \kw{r2} * \kw{r3};
\end{align*}
The subprogram ${\uparrow} C(e_1)$
will use $\kw{r2}$ for its input and output,
and use registers $\kw{r3}$ and above for its intermediate results,
leaving the contents of $\kw{r1}$ unchanged.
Likewise,
${\uparrow\uparrow} C(e_2)$
will operate on registers $\kw{r3}$ and above,
leaving both $\kw{r1}$ and $\kw{r2}$ unchanged.
The last instruction in the program performs
the top-level operation and stores the result
in the output register $\kw{r1}$.
\end{example}
%}}}

To formally establish the correctness of $C_1$,
we seek to prove that
$\llbracket C_1(e) \rrbracket = \llbracket e \rrbracket$
by structural induction on $e$.
It is easy enough to check that:
\[
  \llbracket \bullet \rrbracket = \llbracket x \rrbracket \qquad
  \llbracket \kw{r1} := c \rrbracket = \llbracket c \rrbracket
\]
However,
the inductive cases for $e_1 + e_2$ and $e_1 \times e_2$
are less obvious to handle.
To articulate why,
it is useful [to be overly pedantic:]

When compiling $(e_1 * e_2)$,
$C_1$ constructs a $\kw{RegProg}$ system
in terms of the simpler components $C_1(e_1)$ and $C_1(e_2)$.
As we try to understand the behavior of the resulting artefact,
the mathematical theory we use to specify and analyse
the behavior of these components,
namely the semantic domain $\mathbb{Z} \rightarrow \mathbb{Z}$,
should help us connect
the behavior of the components to
the behavior of the whole,
ultimately allowing us to prove
the conclusion $\llbracket C_1(e_1 * e_2) \rrbracket = \llbracket e_1 * e_2 \rrbracket$
from our induction hypotheses
$\llbracket C_1(e_1) \rrbracket = \llbracket e_1 \rrbracket$ and
$\llbracket C_1(e_2) \rrbracket = \llbracket e_2 \rrbracket$.
However,
our theory is not rich enough to support this process:
$\mathbb{Z} \rightarrow \mathbb{Z}$
is not the right semantic domain
to describe $\kw{RegProg}$ modules.

%This is because while the program $C(e_1 * e_2)$
%was built by stitching together more elementary components,
%we have not yet defined a corresponding notion of \emph{composition}
%at the level of our semantic domain (Sec.~\ref{sec:cccomp}).
%[Condense the following tease]
%Moreover,
%the semantic domain $\mathbb{Z} \rightarrow \mathbb{Z}$
%is not \emph{expressive} enough to account for the behavior of $\uparrow$:
%using our convention for the meaning of $\kw{RegOps}$ programs,
%$\llbracket {\uparrow}p \rrbracket$ is always the identity function!
%This is addressed in Sec.~\ref{sec:ccexpr} by defining
%a richer semantics for $\kw{RegOps}$.
%Because this richer semantics no longer matches
%the domain used to interpret the source language $\kw{AlgExp}$,
%we then need to account for the \emph{abstraction} relationship
%between the behaviors of $\kw{AlgExp}$ programs
%and the behaviors of $\kw{RegOps}$ ones (Sec.~\ref{sec:ccabs}).
%[refinement, open system whatever that means in this context].

%}}}

\subsection{Expressivity} %{{{

The expressivity of our theories
should be assessed
[real-world,
from the point of view of the outside.]

The first problem we encounter when we try to understand $C_1(e_1 * e_2)$
is the lack of expressivity of our semantic domain.
For instance,
$\mathbb{Z} \rightarrow \mathbb{Z}$
fails to even explain the effect of the shift operator $\uparrow$:
in fact, for any $\kw{RegProg}$ program $p$,
the program ${\uparrow}p$ leaves $\kw{r1}$ unchanged,
so that it is indistinguishable from the empty program!
Not that this problem did not appear
when defining our notion of compiler correctness.
In fact,
given a sufficiently precise semantics of $\kw{RegProg}$,
there is no doubt we could work around the problem:
bite the bullet, treat $C_1(e_1 * e_2)$ as a whole,
and prove our compiler correct.
However,
the situation illustrates an important point:
by overfitting our choice of semantics to the problem at hand,
we have failed to account for important ways in which
$\kw{RegProg}$ programs can interact with their environments
in ways that $\kw{AlgExp}$ programs cannot.
This limits the usefulness of our target semantics,
and as such that of the compiler correctness theorem
(the induction hypotheses in this case).

A better approach would have been to
assess the expressivity of our semantics,
not in terms of formulating the problem at hand,
but in terms of the ways in which our system
could be used.
What is a $\kw{RegProg}$ machine good for?
With a different convention,
it could be used to compute function of several variables.
It could be a component in a handheld calculator,
or the control system of an airplane.

[it seems like pandora's box,
but maximal expressivity in how system communicates with its environment
+ open system = everything you want]

[Likewise, C programs are used to do all kind of things,
but we don't need to know what:
only the basic interaction principles
between the C program and its environment that are involved
in those things.]

Although communication model between program
and the underlying system is simple and uniform
(system calls / external function invocation),
this protocol is general enough that
when we link with the outside world,
the source/target machine models are enough to
implement all kinds of stuff:
network services, GUIs, drivers, control software,
operating systems, distributed computations, \ldots
The same compiler is used in all of these contexts.

%}}}

\subsection{Abstraction} %{{{

The assumption that the source and target programs
can be naturally assigned meanings in the same semantic domain
is reasonable for some individual compiler passes and optimizations
where the source and target languages are fairly similar.
It can also hold when observable behaviors are simple:
if we are only interested in the final result produced by a program,
then the corresponding notion of behavior
can be fairly language-independent.
However,
in most practical cases we are interested in
the program's interaction with its environment
as well as its ultimate outcome,
and this interaction is often understood very differently
in the context of the source and target languages.

For instance,
in the C programming language
the memory is understood in terms of independent objects.
Each object corresponds to a variable declared in the program,
or to a chunck of heap-allocated memory.
A function call is performed by allocating new objects
corresponding to the callee's arguments and local variables,
then initializing the callee's arguments
using the values provided by the caller.
By contrast,
an assembly program
operates in a single address space:
the memory is essentially seen as a large array of bytes;
the very notion of a function call in assembly
is largely conventional as opposed to a primitive notion,
and their mechanics are understood in very different terms.
Consequently,
semantic domains that naturally reflect and accurately account
for module interaction will necessarily be distinct in C and assembly.

This underscore importance of \emph{abstraction}:
C compilers operate in the context of a given \emph{calling convention},
which establishes a relationship between the behaviors of C modules
and that of assembly modules.
This calling convention can be modelled as a function
$\mathbb{C} : \mathbb{D}_s \rightarrow \mathbb{D}_t$,
which is in some sense the semantic counterpart to the compiler $C$.
Our correctness criterion becomes:
\[
  \mathbb{C}(\llbracket p \rrbracket_s) =
  \llbracket C(p) \rrbracket_t
\]

[Compcert goes to great length to ensure $\mathbb{D}_s = \mathbb{D}_t$,
defining unified memory model etc.
Much compositional Compcert works keeps with this approach
but this creates problems.
In fact, even when $\mathbb{D}_s = \mathbb{D}_t$ formally,
the source and target behaviors may be distinct
and it can be important/useful to define
$\mathbb{C} : \mathbb{D} \rightarrow \mathbb{D}$.
In Sec.~\ref{sec:callconv} we show how taking abstraction seriously
solves the extcall\_args problem.]

%}}}

\subsection{Refinement} %{{{

In and of itself,
abstraction is insufficient to reflect
the relationship between the behaviors of C and assembly modules.
This is because there is more than one way to realize
a given C call at the level of assembly.
For instance,
a typical calling convention specifies a classification of machine registers
into \emph{callee-save} registers,
which are guaranteed to be left unchanged by the function being invoked,
and \emph{caller-save} registers,
which the function being invoked may modify at will.
Two assembly functions
which modify the caller-save registers differently
may implement the same C behavior
but they will be observationally distinct at the level of assembly.

To account for this situation,
the target semantic domain $\mathbb{D}_t$
should contain specifications
allowing a range of possible behaviors,
and be equipped with a notion of refinement
in the form of a transitive relation $\sqsupseteq$.
The target behavior $\mathbb{C}(\sigma_s)$
corresponding to the source behavior $\sigma_s$
can then be broad enough to state,
for instance,
that the callee-save registers
may contain any value after a function
specified in $\sigma_s$ returns.
Target programs
which leave specific values in these registers
but otherwise agree with $\mathbb{C}(\sigma_s)$
will be considered valid implementations
because their behavior will refine that specification.
Taking this into account,
the correctness statement becomes:
\[
  \mathbb{C}(\llbracket p \rrbracket_s) \sqsubseteq
  \llbracket C(p) \rrbracket_t \sqsubseteq
\]

Note that while abstraction and refinement are distinct concepts,
it can sometimes be advantageous to unify them
in the form of a single, heterogenous relation
${\sqsupseteq_\mathbb{C}} = {\sqsupseteq} \circ {\mathbb{C}}$.
[Reference popl15's abstraction relations]
[Forward reference to where we do that in this work].
[Explain that confusing the two is the root of some limitation
in existing work].

[Comment on induced equivalence relation?]

[Role of refinement/abstraction in permitting optimizations?]

%}}}

\subsection{Open systems} %{{{

subsets of behaviors is not good enough,
we need to take distinction between system and environment seriously.
-> this is how we define a notion of refinement in Sec.~3
that solves some of the issues

Discuss contextual refinement,
which is not enough \emph{a priori}, and \emph{practically},
because we don't want to limit ourselves to a set of systems we're gonna connect with.
But also contextual refinement might be enough
if the interaction of all the environments we would ever want to connect with
can be "coobservationally equivalent" to a context in our set
from the point of view of the program (aka oracle).

[The compilation of one module
makes no assumption as to what other modules are going to be using it.
Library. Contextual refinement.
The operating system can do all kinds of crazy stuff to you:
signals, memory-mapped I/O, fork, etc,
but the compiler remains correct in the context of that intereference.

Specs need to be able to express constraints on the environment.]

[Fit somewhere:]
This calls into question
the centrality of contextual refinement and contextual equivalence
in standard approaches to compositionality.
[Use game semantics and a smarter refinement instead;
you'll get contextual refinement for free
for whatever that's worth.]
[There is no completed system.
At the very least the context itself
should have facilities to communicate with the larger world
that are expressive enough.]

%}}}

\subsection{Compositionality} %{{{

[Large programs are split into compilation units,
which are compiled independently,
then linked to produce the final artefact.]

Syntactic composition
(the way we stitch together components to build a system in the real world)
should have corresponding notion
in the semantics
(the theory we use to analyse systems).

%}}}

\subsection{Resources} %{{{

[The source model is usually much more idealized than target.
Example: infinite stack vs. finite address space.]

%}}}

%}}}

\section{Open Modules for Compcert} %{{{

Applying our principles to the labeled transition systems
used in Compcert yields a natural extension
able to support compositional compilation.

Labeled transition systems (LTS)
are the main kind of semantic objects used by Compcert.
A Compcert LTS specifies a set of states $S$,
a labeled transition relation ${\rightarrow}$,
and distinguished subsets of initial and final states,
the latter of which are associated with a final integer result.
Compcert LTS support a notion of interaction with the environment
in the form of event traces:
a transition $s \stackrel{t}{\rightarrow} s'$,
indicates that the state $s$ may transition to state $s'$
through an interaction recorded as the event trace $t$.

\subsection{Language Interfaces} %{{{

[Not sure about the terminology]

This model is insufficient to express
the semantics of C and assembly modules.
Compcert models whole programs with a single entry point,
which terminate by producing a single numerical result.
By contrast,
control can enter and exit open modules
in a variety of language-dependent configurations.
These language-dependent aspects are captured
by the following notion.

\begin{definition}[Language interface]
A \emph{language interface} $\tau$ consists in a pair $(V, R)$
which specifies
a set $V$ of \emph{invocation configurations} and
a set $R$ of \emph{return configurations}.
Invocation configurations
correspond to the states in which
a module can be entered, whereas
return confgurations
correspond to the states in which
the module can return control
to its caller.
\end{definition}

For C,
invocation configurations
will consist of the name of the function being invoked,
the values of its arguments,
and the state of the memory at the point of entry.
Return configurations
will consist of the function's return value
and the state of the memory at the point of exit.
For assembly,
the values of registers and the state of the memory
may specify both invocation and return configurations.

In practice,
the language interfaces used in Compcert
happen to be formally identical,
however it is important to understand them
as distinct interfaces
connected by the C calling convention.

%}}}

\subsection{Calling conventions} \label{sec:callconv} %{{{

The simulation relations used by Compcert
such as memory extensions and memory injections
unify abstraction and refinement.
In accordance with this approach,
we use a single notion
to connect two distinct language interfaces,
but also to specify the notion of refinement to use
for pairs of initial and final configurations
within a single language interface.

\begin{definition}[Calling convention]
Given a source language interface $\tau_1 = (V_1, R_1)$ and
a target language interface $\tau_2 = (V_2, R_2)$,
a \emph{calling convention} is a pair
$\mathbb{C} = (\mathbb{C}_V, \mathbb{C}_R)$.
The component
$\mathbb{C}_V \subseteq V_1 \times V_2$
relates invocation configurations of the source language to
corresponding invocation configurations of the target language.
Given related source and target invocation configuration,
the component
$\mathbb{C}_R(v_1, v_2) \subseteq R_1 \times R_2$
relates return configurations of the source language to
corresponding return configurations of the target language.

A \emph{refinement convention} $\sqsupseteq$
is a calling convention for which
the source and target language interfaces are identical ($\tau_1 = \tau_2$)
and $\sqsupseteq_V$, $\sqsupseteq_R$ are transitive.
\end{definition}

Compcert does not use explicit notions of
language intefaces and calling convenentions.
However,
its model of external calls
(\kw{extcall\_sem}),
the requirements it imposes on their semantics
(\kw{extcall\_properties}),
and the internal invariants used by its simulation proofs
can be read in this light
and used to define
compatible notions of refinement and calling conventions
across Compcert languages.

For simplicity,
Compcert uses a common language interface $\tau$
for all of its languages.
Invocation configurations are of the form:
\[
  V = \{ f(\vec{v})@m : (f, v, m) \in
    \kw{ident} \times \kw{val}^* \times \kw{mem} \} \,,
\]
where $f$ identifies the function being called,
$\vec{v}$ provides values for the function's arguments, and
$m$ specifies the global memory's initial state for the call.
Return configurations are of the form:
\[
  R = \{ v'@m' : (v', m') \in
    \kw{val} \times \kw{mem} \} \,,
\]
where $v'$ gives the function's return value and
$m'$ specifies the global memory's new state.

The refinement convention associated with this language interface
specifies that
related initial configurations should carry the same function identifier,
and that the values of arguments and the states of the memories
can be related in one of three ways:
\begin{itemize}
\item they can be equal;
\item the source initial configuration may be
  \emph{less defined} than the target configuration,
  meaning that the special value $\kw{Vundef}$
  in the components of the source configuration
  can be refined into concrete values in the target configuration,
  and that the target memory,
  in addition to having more defined contents,
  may carry more liberal permissions
  (a relation known as \emph{memory extension});
\item they can related by a \emph{memory injection} $\iota$,
  allowing memory addresses to be remapped
  between the source and target configurations.
\end{itemize}
Memory injections only constrain the contents of memories
at addresses which are related by $\iota$.
At these addresses,
the contents of the target memory is furthermore
allowed to be more defined than the contents of the source
memory at the corresponding address.

Given two initial configurations related by
the Compcert refinement convention,
matching final states
must be related in a compatible way:
\begin{itemize}
\item if the initial configurations were equal,
  the final configurations must be equal as well;
\item if the source initial configuration was
  \emph{less defined} than the target initial configuration,
  then this must be true of the corresponding
  final configurations as well;
\item if the source and target initial configurations
  were related through a memory injection $\iota$,
  then the corresponding final configurations
  must be relation through a potentially \emph{larger}
  injection $\iota' \ge \iota$.
\end{itemize}
In addition,
previously allocated regions of the target memory
must remain unchanged
at any addresses that were not mapped to valid locations
of the initial source memory.
This allows the target language to allocate additional,
private memory locations (such as stack frames),
which are not visible at the level of the source language.
Compcert's refinement convention
expresses the expectation that such locations
will be left unchanged by the environment.

Although the Compcert refinement convention is used as
the calling convention for most compiler passes,
a more specific calling convention is used
at the point where function call arguments
are marshalled onto the stack.
There,
in addition to the criteria defined above,
[arguments must be encoded into the target memory,
but \emph{only} the target memory,
and only contain pointers to addresses visible in the source].

%}}}

\subsection{Semantic objects} %{{{

Extending Compcert LTS to account for
this extra structure yields the following definition.

\begin{definition}[Labeled transition system]
Given a language interface $\tau = (V, R)$,
a \emph{labeled transition system} for $\tau$
is a tuple $\sigma = (S, \rightarrow, I, F)$ where
$S$ is a set of states,
${\rightarrow} \subseteq S \times L_E^* \times S$
is a ternary \emph{transition relation},
$I \subseteq V \times S$
assigns a set of \emph{initial states} to each invocation configuration, and
$F \subseteq S \times R$
is a relation designating \emph{final states}
together with the return configurations they produce.
\end{definition}

[Introduce determinacy?]

We have not yet specified the set $L_E$ of events
used by a language interface $L$.
They will consist of the same events already present in Compcert,
augmented with a set of external call events.

%}}}

\subsection{External call events} %{{{

The use of language-specific invocation and return configurations
in the definition of transition systems
takes care of the variety of ways
in which the environment may invoke an open module,
but does not account for the ways in which
the module may itself invoke the environment.
We remedy this by introducing \emph{external call events}.

Following \cite{cpp2015},
we use external call events of the form $\kw{extcall}[v, r]$,
recording the fact that an extern call was performed
with the invocation configuration $v$,
and produced the return configuration $r$.

Compcert does account for the fact that the program
may call the outside world in the form of a global parameter
specifying the semantics of external calls:
\[
  \kw{external\_call} :
    \kw{ident} \rightarrow
    \kw{val}* \times \kw{mem} \rightarrow
    \mathbb{E}^* \rightarrow
    \mathcal{P}(\kw{val} \times \kw{mem})
\]
In addition,
the predicate $\kw{extcall\_properties}$ defines
requirements which must be satisfied by the semantics
of every external function.
The correctness proof of Compcert is contingent
on the fact that for every external function $f$,
the assertion $\kw{extcall\_properties}(\kw{extcall\_sem}(f))$ holds.

We can instantiate $\kw{external\_call}$ for the C language as:
\[
  \AxiomC{$e = \kw{extcall}[f(\vec{v})@m, v'@m']$}
  \UnaryInfC{$\kw{external\_call}(f) :
    (\vec{v}, m) \stackrel{e}{\rightarrow} (v', m')$}
  \DisplayProof
\]
This technique allows us to reuse Compcert's existing infrastructure
for handling external calls,
and ensures that our changes to the
definition of semantics and the correctness proof are minimal.
However,
to make it work we need to update $\kw{extcall\_properties}$
to account for trace refinement.

%}}}

\subsection{Refinement} %{{{

Compcert uses \emph{backward simulations}
as its primary notion of refinement between labeled transition systems.
Our version takes into account the notion of calling convention
introduced in Sec.~\ref{sec:callconv}:
because our events may expose language-specific details,
we cannot assume events in the two LTS can be directly compared.
Instead,
we use the calling convention to compute an appropriate relation.
Furthermore,
the calling convention may introduce
constraints on the environment in the target specification:
for instance,
when compiling a C module performing external calls,
the target assembly module is not required to handle
all possible final assembly configuration,
but only those corresponding to the possible final C configurations
handled by the source module.

[NB what we're doing in the next paragraph
is allowing our semantic objects
to place constraints on the environment.
This departs from receptivity requirement in Compcert,
and we should be more explicit about that]

To handle this,
it is important to understand that Compcert events
denote two actions:
a query by the module,
which may additionally some output information,
followed by a response from the environment,
which may simply acknowledge the query
but may also carry any input associated with the event.
To show that
a given target source behavior is refined by
a given target behavior,
we need to show that any query performed by the target behavior
is already present in the source behavior,
and that any response permitted of the source environment
is properly reflected by
a response permitted of the target environment.
This insight is used to define a notion of alternating simulation,
similar for instance to the notion of refinement used in \cite{ia}.

This leads us to introduce two distinct relations on events,
separating the contribution of the module and that of the environment.
The relation $\succeq_\mathbb{C}$
relates events
which have a related query component,
but may differ in their response component.
For regular Compcert events,
this corresponds to the usual $\kw{match\_traces}$ relation.
[exapand?]
For external call events, we define:
\[
  \kw{extcall}[v_1, r_1] \succeq_\mathbb{C} \kw{extcall}[v_2, r_2]
  \stackrel{\text{def.}}{\Leftrightarrow}
  (v_1, v_2) \in \mathbb{C}_V
\]
The second relation $\sqsupseteq_\mathbb{C}$
relates events whose query and response components
both correspond to one another.
For regular Compcert events,
$\sqsupseteq_\mathbb{C}$ coincides with equality.
For external call events, we define:
\[
  \kw{extcall}[v_1, r_1] \sqsupseteq_\mathbb{C} \kw{extcall}[v_2, r_2]
  \stackrel{\text{def.}}{\Leftrightarrow}
  (v_1, v_2) \in \mathbb{C}_V \wedge
  (r_1, r_2) \in \mathbb{C}_R(v_1, v_2)
\]
These relations are extended to traces in the natural way
[maybe not: if environment breaks the deal at any point
then the two traces should still be related by $\succeq$
even though only a common prefix matches]
and allow us to define
our notion of refinement as follows.

\begin{definition}[Alternating simulation]
Given a calling convention $\mathbb{C}$ between
a source language interface $\tau_1 = (V_1, R_1)$
a target language interface $\tau_2 = (V_2, R_2)$,
an \emph{alternating simulation} between
an LTS $\sigma_1 = (S_1, \rightarrow_1, I_1, F_1)$ and
an LTS $\sigma_2 = (S_2, \rightarrow_2, I_2, F_2)$
is a relation $R \subseteq S_1 \times S_2$
with the following properties:
\begin{description}
\item[Preservation of system half-steps]
  For any pair of related states $(s_1, s_2) \in R$,
  if the target behavior takes a step $s_1 \stackrel{t_1}{\rightarrow} s_1'$,
  then there exist $e_2$, $s_2'$ such that
  $s_2 \stackrel{t_2}{\rightarrow^*} s_2'$,
  $t_1 \succeq_\mathbb{C} t_2$, and
  $(s_1', s_2') \in R$.
\item[Preservation of environment half-steps]
  For any pair of related states $(s_1, s_2) \in R$ such that
  the source behavor takes a step
  $s_1 \stackrel{t_1}{\rightarrow} s_1'$,
  the target behavior takes a step
  $s_2 \stackrel{t_2}{\rightarrow} s_2'$, and
  the event traces
  $t_1 \succeq_\mathbb{C} t_2$ have related module behaviors,
  there exist $t_2', s_2''$ such that
  $s_2 \stackrel{t_2'}{\rightarrow} s_2''$
  and $t_1 \sqsupseteq_\mathbb{C} t_2'$.
\item[Compatibility of initial states]
  Given two related invocation configurations $(v_1, v_2) \in \mathbb{C}_V$
  and a corresponding source state $s_1$ such that $(v_1, s_1) \in I_1$,
  there exists a target state $s_2$ such that $(v_2, s_2) \in I_2$ and $(s_1, s_2) \in R$.
\item[Compatibility of final states]
  Given two related states $(s_1, s_2) \in R$ and
  a target return configuration $r_2$ such that $(s_2, r_2) \in F_2$,
  there exits
  a source return configuration $r_1$ such that $(s_1, r_1) \in F_1$
  such that $(r_1, r_2) \in \mathbb{C}_F$.
\end{description}
\end{definition}

[Forward simulations.
Now we only need to flip module steps,
so we still use determinacy but don't need receptivity.]

%}}}

\subsection{Composition of passes} %{{{

[Define composition of calling conventions,
show that alternating simulations compose accordingly,
-> our refinement is transitive too]

Also, show how to define $\mathbb{C}(-)$
for transition systems
and relationship to refinement by $\mathbb{C}$.

%}}}

\subsection{Relation to Compcert semantics} %{{{

[If we forget extcall events,
set $\tau = (\kw{unit}, \kw{int})$ and $\mathbb{C} = \kw{id}$,
and consider only receptive semantics
(that have every possible environment half-steps),
then everything boils down to the original Compcert framework.]

%}}}

\subsection{Per-Module Compiler correctness} %{{{

Changes to Compcert:
\begin{itemize}
\item Update the LTS framework (\kw{Smallstep.v})
  in the way we have indicated
\item Update \kw{extcall\_semantics} to take into account
  refinement of events.
\item Connect invocation/return configurations
  with the simulation relations in each pass
\end{itemize}  

%}}}

\subsection{Composition of transition systems} %{{{

[Need to express domain of modules and discriminate
internal from external calls.
Store in the module type / language interface?
Provide as a parameter of composition?
We can also simply check whether there's at least
one state for the invocation configuration,
though this may introduce some non-monotonicity.
Or modify $I$ in transition systems
to be a partial function $V \rightharpoonup \mathcal{P}(S)$
identifying which invocation configurations are supported.
Or just leave the extcall events there even when
they are resolved,
though the consequences of that are unclear.]

[NB: our transition systems could be indexed by two language interfaces,
one for the top-level interaction, and one for nested external calls]

\paragraph{Flat composition} %{{{

Flat composition
is a first-order approximation of horizontal composition
where two transition system $\sigma_1$ and $\sigma_2$
are laid out side by side,
but cannot interact with one another.
An execution of the resulting machine $\sigma_1 + \sigma_2$
will nondeterministically proceed,
either as an execution of $\sigma_1$,
or as an execution of $\sigma_2$.
If the two transition systems
handle disjoint sets of initial configurations,
then $+$ will nonetheless preserve determinacy.

\begin{definition}[Flat composition]
Given two labeled transition systems
$\sigma_1 = (S_1, \rightarrow_1, I_1, F_1)$ and
$\sigma_2 = (S_2, \rightarrow_2, I_2, F_2)$
for a common language interface $\tau = (V, R)$,
their flat composition is defined as
the labeled transition system
$\sigma_1 + \sigma_2 = (S, \rightarrow, I, F)$
where:
\begin{align*}
  S &:= S_1 + S_2 \\
  {\rightarrow} &:= {\rightarrow_1} + {\rightarrow_2} \\
  I &:= \{ (v, \iota_1(s_1)) : (v, s_1) \in I_1 \}
   \cup \{ (v, \iota_2(s_2)) : (v, s_2) \in I_2 \} \\
  F &:= \{ (\iota_1(s_1'), r) : (s_1, r) \in F_1 \}
   \cup \{ (\iota_2(s_2'), r) : (s_2, r) \in F_2 \}
\end{align*}
\end{definition}

To go from flat composition to vertical composition,
we will need to introduce a resolution operator $\mathcal{R}(\sigma)$
which allows $\sigma$ to call back into itself.
To illustrate the concepts involve,
we first introduce vertical composition.

%}}}

\paragraph{Vertical composition} %{{{

[It seemed like a good idea to take things one step at a time,
but $\mathcal{R}$ is simple enough (actually more simple than $\circ$)
that we probably don't need to do that and could skip to it directly.
Maybe a section with more explanations about continuations would
be a better fit here.]

In the vertical composition of transition systems,
external calls performed by a transition system
$\sigma_1 = (S_1, \rightarrow_1, I_1, F_1)$
are replaced by executions of a second transition system
$\sigma_2 = (S_2, \rightarrow_2, I_2, F_2)$.
The transition system
$\sigma_1 \circ \sigma_2 = (S, \rightarrow, I, F)$
will have two execution modes:
In the first,
it will execute $\sigma_1$ normally and
operate on states of type $S_1$.
When $\sigma_1$ would perform an external call,
$\sigma_1 \circ \sigma_2$ will switch to a second mode,
performing a nested execution of $\sigma_2$ instead.
In addition to the state of $\sigma_2$,
in this mode the machine will store a \emph{continuation}
$\kappa \subseteq R \times S_1$
of $\sigma_1$,
associating to each possible final configuration of $\sigma_2$
a set of resumption states for $\sigma_1$.

Accordingly,
the states of the composite LTS are:
\[
  S := S_1 + \mathcal{P}(R \times S_1) \times S_2
\]
Invocation and return configurations
correspond to the top-level interaction of $\sigma_1$ with the environment:
\begin{align*}
  I &:= \{ (v, \iota_1(s_1)) : (v, s_1) \in I_1 \} \\
  F &:= \{ (\iota_1(s_1), r) : (s_1, r) \in I_1 \}
\end{align*}
During normal execution,
steps of $\sigma_1$ are used as-is:
\[
  \AxiomC{$s_1 \stackrel{e}{\rightarrow_1} s_1'$}
  \AxiomC{$e \notin \kw{extcall}$}
  \BinaryInfC{$\iota_1(s_1) \stackrel{e}{\rightarrow} \iota_1(s_1')$}
  \DisplayProof
\]
To express the switch to nested execution of $\sigma_2$,
we first give the set of continuations associated with a state $s$ of $\sigma_1$
about to perform an external call with initial configuration $v$:
\[
  K(s, v) =
    \{ (r, s') : s \stackrel{e}{\rightarrow_1} s',
                     e = \kw{extcall}(v, r) \}
\]
The switch is triggered when \emph{at least one} external call event
can occur at the current point in the execution:
\[
  \AxiomC{$K(s_1, v_2) \ne \varnothing$}
  \AxiomC{$(v_2, s_2) \in I_2$}
  \BinaryInfC{$\iota_1(s_1) \rightarrow \iota_2(K(s_1, v_2), s_2)$}
  \DisplayProof
\]
Once the machine is in the second mode,
execution proceeds according to $\sigma_2$,
leaving the continuation unchanged:
\[
  \AxiomC{$s_2 \stackrel{t}{\rightarrow_2} s_2'$}
  \UnaryInfC{$
    \iota_2(\kappa, s_2)
    \stackrel{t}{\rightarrow}
    \iota_2(\kappa, s_2')$}
  \DisplayProof
\]
When a final state of $\sigma_2$ is reached,
the machine uses the continuation to switch back
to normal execution:
\[
  \AxiomC{$(s_2, r_2) \in F_2$}
  \AxiomC{$(r_2, s_1') \in \kappa$}
  \BinaryInfC{$\iota_2(\kappa, s_2) \rightarrow \iota_1(s_1')$}
  \DisplayProof
\]

%}}}

\paragraph{Resolution} %{{{

Horizontal composition may be understood as the infinite composition:
\[
  \mathcal{R}(\sigma_1 + \sigma_2) =
    (\sigma_1 + \sigma_2) \circ (\sigma_1 + \sigma_2) \circ \cdots
\]
In the following
we define the resolution operator $\mathcal{R}$ explicitely,
and show that it computes a fixpoint of $\sigma \circ -$.

The transition system $\mathcal{R}(\sigma)$
works similarly to vertical composition,
however instead of just two modes,
the machine will contain a stack of continuations,
allowing an arbitrary depth of calls.
We use the same definition of continuation $K(s, x)$
as for vertical composition.

\begin{definition}[Resolution operator]
Given a transition system $\sigma = (S, \rightarrow, I, F)$,
the closure [or whatever] of $\sigma$ is the transition system
$\mathcal{R}(\sigma) :=
  (S_\mathcal{R}, \rightarrow_\mathcal{R}, I_\mathcal{R}, F_\mathcal{R})$,
where:
\begin{align*}
  S_\mathcal{R} &:= S \times \mathcal{P}(L_F \times S)^* \\
  I_\mathcal{R} &:= \{ (v, s, \kw{nil}) : (v, s) \in I \} \\
  F_\mathcal{R} &:= \{ (s', \kw{nil}, r) : (s', r) \in F \} \,,
\end{align*}
and where $\rightarrow_\mathcal{R}$ is defined
by the following rules.
Normal execution operates on the top-level state:
\[
  \AxiomC{$s \stackrel{e}{\rightarrow} s'$}
  \AxiomC{$e \notin \kw{extcall}$}
  \BinaryInfC{$(s, \vec{\kappa}) \rightarrow (s', \vec{\kappa})$}
  \DisplayProof
\]
Function calls are handled by pushing the current continuation
and starting a new top-level execution:
\[
  \AxiomC{$K(s, v) \ne \varnothing$}
  \AxiomC{$(v, s') \in I$}
  \BinaryInfC{$(s, \vec{\kappa}) \rightarrow (s', K(s, v) :: \vec{\kappa})$}
  \DisplayProof
\]
Conversely,
whenever we reach a final state
and the stack is non-empty,
we resume the topmost continuation:
\[
  \AxiomC{$(s, r) \in F$}
  \AxiomC{$(r, s') \in \kappa$}
  \BinaryInfC{$(s, \kappa :: \vec{\kappa}) \rightarrow (s', \vec{\kappa})$}
  \DisplayProof
\]
\end{definition}

\begin{theorem}[$\mathcal{R}(\sigma)$ is a fixed point]
$\sigma \circ R(\sigma) \equiv R(\sigma)$.
It is true and reassuring but do we need this in any way?
\end{theorem}

%}}}

\paragraph{Horizontal composition} %{{{

Now that we have defined $\mathcal{R}$,
we can define horizontal composition as:
\[
  \sigma_1 \bullet \sigma_2 := \mathcal{R}(\sigma_1 + \sigma_2)
\]

%}}}

%}}}

\subsection{Separate Compilation} %{{{

[Show the SepCompCert theorem
from lemmas that will have been introduced earlier]
\[
  \mathbb{C}(\llbracket M_1 + M_2 + \cdots \rrbracket) \sqsupseteq
  \llbracket C(M_1) + C(M_2) + \cdots \rrbracket
\]
We will need:
\begin{itemize}
\item compositionality of C and assembly:
  $\llbracket M_1 + M_2 \rrbracket \equiv
   \llbracket M_1 \rrbracket \bullet \llbracket M_2 \rrbracket$
\item monotonicity of $\mathbb{C} : \sqsupseteq \rightarrow \sqsupseteq$
  (not if $\mathbb{C}$ is bundled into $\subseteq$)
\item monotonicity of $\bullet : \sqsupseteq \times \sqsupseteq \rightarrow \sqsupseteq$
\item per-module correctness theorem $\mathbb{C}(\llbracket M_i \rrbracket) \sqsupseteq \llbracket C(M_i) \rrbracket$.
\end{itemize}

%}}}

\subsection{[Stuff to remove/redistribute]}

{ \color{gray}

Our analytical framework
yields a small number of changes
which can be applied to Compcert for
turning it into a compiler suitable for use
in the context of end-to-end verification:
\begin{enumerate}
\item Compcert models the interaction of programs
  with their environment in terms of event traces.
  However,
  these event traces are not expressive enough
  to model the interaction between modules in a satisfactory manner.
  Following \cite{cpp2015},
  the addition of \emph{external call events} to traces
  and the generalization of our program semantics
  to operate on a function-by-function basis
  bridges this gap in expressivity.
\item Because external calls are realized differently
  at the level of C and assembly modules,
  the source and target semantics
  will use different kinds of external call events.
  To account for this discrepancy,
  the formulation of compiler correctness
  will involve an explicit \emph{calling convention},
  relating external calls in the source and target languages.
\item 
  Open-system refinement: understand Compcert events
  as two actions: one by the system, one by then environment in reaction.
  Then correponding mixed-variance refinement of events.
  Follows the work on Interface Automata \cite{ia},
  avoiding many of the complications.
\item
  Then composition
\item
  Work out what is necessary to support this
  at the level of transition systems / give an operational account
  (generalized initial and final states,
  appropriate notion of simulation)
\end{enumerate}

}

%}}}

\section{A Richer Semantic Domain} %{{{

\subsection{Games}
\subsection{Strategies}


%}}}

\section{Applications} %{{{

\subsection{Non-Local Jumps}
\subsection{Context Switching}
\subsection{Thread library}
\subsection{File-System Interaction}

%}}}

\section{Future Work} %{{{

\subsection{Signals}
\subsection{Concurrency}
\subsection{Relaxed Memory Models}
\subsection{Abstraction of Values} %parametricity? can use pointers + empty mem block?

%}}}

\section{Related Work} %{{{

\begin{table*}
  \begin{tabular}{lcccccc}
    \hline
    & Expressivity & Abstraction & Refinement & Compositionality & Open systems & Resources \\
    \hline
    Compcert \cite{compcert}
      &           &           & $\bullet$ &           &           & \\
    CompCertX \cite{popl2015}
      &           & $\circ$   & $\bullet$ & $\circ$   &           & \\
    Compositional CompCert \cite{compcompcert}
      & $\circ$   &           & $\bullet$ & $\circ$   & $\bullet$ & \\
    Ramananandro et al. \cite{cpp2015}
      & $\circ$   &           & $\bullet$ & $\bullet$ & $\bullet$ & \\
    QompCert \cite{qompcert}
      &           &           & $\bullet$ &           &           & $\bullet$ \\
    SepCompCert \cite{lwsc}
      &           &           & $\bullet$ & $\circ$   &           & \\
    This work
      & $\circ$   & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & \\
    \hline
  \end{tabular}
\end{table*}

%}}}

\bibliography{lwcc}

\end{document}
