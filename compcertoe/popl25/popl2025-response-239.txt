> POPL 2025 Paper #239 Reviews and Comments
> ===========================================================================
> Paper #239 Unifying compositional verification and certified compilation
> with a three-dimensional refinement algebra

We are grateful for the reviewers' hard work and feedback.
Since cross-cutting concerns include the motivation and framing of our contributions,
the relationship of our work to other relevant research, and the status of our
implementation, we will discuss these issues first. We then include a proposed
revision plan for the paper and provide responses to reviewers' individual concerns.

# Motivation and novelty

Our long-term goal is to facilitate
the construction of large-scale certified systems.
In the same way a present-day system administrator
would set up an infrastructure by
selecting a combination of existing hardware and software components
and configuring them to work together,
we envision a situation where
future certified systems architects
would build complex systems
by assembling off-the-shelf certified components,
and would be able to obtain end-to-end proofs of correctness
for very large systems
with little additional effort.

Figuring out whether and how this vision can be realized
will require research in several directions,
including (i) improving the range of proof automation techniques
and streamlining the experience of verification engineers
(as suggested in Review #239C)
as well as (ii)
conducting more integration verification case studies
(like those referenced by Review #239A).
Simultaneously,
we must (iii) draw from these experiences
to develop a systematic understanding
of the principles underlying these successes,
and distil them into new mathematical tools
which can then serve as a foundation for
the development of next-scale certified systems.

Although our **§6 Applications** section illustrates
concrete verification tasks carried out using our framework,
we would consider that our work and claim to novelty
falls squarely under (iii).

**Illustration with integration verifiction.**
The Bedrock2 case studies mentioned by Review #239A
can be used to illustrate this point.
Erbsen et al. [PLDI '24] presents the end-to-end verification of
a minimal but sophisticated embedded system,
which mediates access to an external actuator
(in their demonstration, the opening mechanism for
a miniature replica of a garage door)
through a cryptographically authenticated network connection.
The top-level correctness statement takes the form:
$$
\mathtt{always\:\:run1\:\:(eventually\:\:run1\:\:}
(q \mapsto \mathtt{io\_spec}\:\:q.\mathtt{getLog}))
\:\:q_0\,,\quad(1)
$$
where
`run1` models one execution step of the low-level RISC-V system being verified,
$q_0$ is the initial state of the system,
`getLog` extracts from the state
the accumulated record of external interactions,
which are expected to satisfy the predicate `io_spec`.
An equivalent formulation within our framework may look something like:
$$
\mathtt{io\_spec}\:\le_{R \twoheadrightarrow S}\:
[q_0\rangle \odot (\cdots \mathtt{run1} \cdots) :
\mathcal{A} \twoheadrightarrow \mathcal{N}\,,
\quad(2)
$$
where $\mathcal{A}$ and $\mathcal{N}$
describe the respective interfaces of the actuator and ethernet network,
and where $R$ and $S$ can be used to explain how the left-hand side specification
is realized by the system model outlined on the right-hand side.

Logically,
the formulations (1) and (2) are in fact very close.
Both our model and (1) use external interaction traces
to specify the system's observable behavior,
and it may in fact be relatively straightforward
to establish a property like (2)
from the statement of (1) found in the existing development.
At the same time,
reformulating the correctness property in this ``standardized'' form
presents some advantages:

  * First, our composition principles would be immediately available
    to someone who were trying to *prove* the property again, and they may
    lessen the need to use other kinds of compositional techniques.
    As we understand it, the Bedrock proof relies on forms of separating
    conjunction and data abstraction which could conceivably fall under our
    spatial and vertical composition infrastructure in a different version.
    This approach may also make it easier to reuse the CompCertO correctness
    theorem, CAL methodology, and any other embedded tools as needed for the proof.
  * More to the point (since the proof already exists),
    consider a *user* of this certified network actuator, seeking
    to use it as a component within (say) a larger certified industrial control
    network. From their point of view, only the external specification matters.
    At first, they will want to reason about the correctness of their overall
    network in terms of `io_spec` alone. At a later stage, they may choose
    among several competing vendors for certified `io_spec` implementations,
    and would simply be able to "plug" the refinement (2) provided by the
    selected vendor within their own proof, replacing `io_spec` by their chosen
    implementation in a correctness-preserving manner.

This mirrors what happens in our paper with Fig 1 / Example 1.1 / §6.3,
where correctness results obtained for individual programs
can be reused and made part of a "next-scale" system of interacting processes,
where the details of their internal semantics and execution can be encapsulated
and abstracted away when we reason about their coarse-grained interaction patterns.

## Relationship to other work

**Interaction Trees.**

  * An interaction tree `t : itree E X` can be embedded as
    a strategy $\langle t \rangle : E \twoheadrightarrow \{* : X\}$.
  * Our version does not need to be constructive,
    so we can formulate specifications in a more declarative way.
    (relate to the reviewers' desire for "just using the proof assistant's
    logic to specify things" -- yes we can do that!)
  * ITrees focus on equivalences vs. we focus on refinement.
    ITree refinements and data abstraction would have to be defined ad-hoc
    (as with *network refinement* in that particular work)

**Game Semantics.**

Horizontal fragment is a very simple form of game semantics,
but OTOH game semantics does not have vertical abstraction and
spatial composition.

**Refinement Calculus.**

Dual nondeterminism provides very powerful abstraction mechanisms,
but models like predicate transformers cannot deal with
compositional external interactions, or state encapsulation.

## Coq Implementation

**It exists.**

**It's reasonably straightforward and interesting in some ways!**

**We don't need to modify CompCertO because it was designed for this kind of
applications.**

## Proposed plan

Explain that we will do:

  * X
  * Y
  * Z

> 
> 
> Review #239A
> ===========================================================================
> 
> Overall merit
> -------------
> 3. Weak accept
> 
> Reviewer expertise
> ------------------
> 4. Expert
> 
> Paper summary
> -------------
> This paper systematizes some recent results in modular verification of software, producing a unified framework for proving interactive systems with modules written in different languages and compiled by verified compilers.  The central ideas are very much in the vein of interaction trees, but there are a lot of details to get right.

[TODO item #1: comparison with interaction trees]

> Comments for authors
> --------------------
> This sort of unified theory is critical to application at scale of the most rigorous formal methods, and I'm impressed at the thoroughness of the design effort reported in the paper.  One strange thing is that the paper doesn't include an evaluation section; we don't get a clear description of which verifications have been carried out, which is handy to validate design choices.

We intend §6
to demonstrate the suitability of our framework
for a range of practical verification tasks
and serve as an evaluation of sorts.
We have fully mechanized in Coq the framework described in the paper
and will submit a polished artifact covering all of our results,
including the applications discussed in §6.
The preliminary artifact we submitted
shows that our framework's compositional structures
can be used to facilitate them.

With that said,
we acknowledge that the paper should go into more details
regarding the actual Coq development,
both regarding the framework itself
and when it comes to the applications discussed in §6.
As a start, we can mention the following:

  * The framework described in §2--5 can be mechanized in approximately
    X,000 lines of Coq definitions and Y,000 lines of proof, per `coqwc`.
  * As we stated in the paper, CompCertO does not require any change.
    [XXX Yu how true is that?]
    We deliberately sought to design a framework whose capabilities
    would encompass those of the CompCertO model.  As a result the
    language interfaces, language semantics, simulation conventions and
    correctness results provided by CompCertO can embedded
    as effect signatures, strategies, refinement conventions and
    refinement squares in a structure-preserving way,
    and used in the context of our framework.
  * ...

> The previous papers that this project built on were notable for significant mechanized case studies of software artifacts of inherent interest, and the lack of such validation here gives the work less appeal.  I also think the paper oversells the novelty of addressing this particular combination of problems.

We have tried to articulate in the two paragraph ll. 85--97 that
existing work does address this combination of problem
(certainly the Bedrock work you reference should be mentioned there),
and to explain why we believe a more systematic approach
would be beneficial.
However it is true that the framing could be improved
and [.... more accurate ...]

> # Crisply formulating what new problem is being solved
> 
> At line 38, there is one bullet list of aspects that are important to address in a single reasoning system: *compilation*, *several different languages*, and *external interactions*.  Then line 166 gives a separate list: program-module correctness, compiler correctness, separation logic-style memory modularity, and data abstraction.  Fig. 1 also shows a motivating example that includes mixing C and assembly and calling I/O system routines.  I agree there is very little work that ties all of these challenges together, but there is at least one project that checks all of those boxes.  See:
> 
> - Andres Erbsen, Jade Philipoom, Dustin Jamner, Ashley Lin, Samuel Gruetter, Clément Pit-Claudel, Adam Chlipala. Foundational Integration Verification of a Cryptographic Server. PLDI'24.
> - Andres Erbsen, Samuel Gruetter, Joonwon Choi, Clark Wood, Adam Chlipala. Integration Verification Across Software and Hardware for a Simple Embedded System. PLDI'21.
> 
> That line of work is centered around the Bedrock2 framework.  There are a few advantages that could be argued for it over the framework presented in this paper:
> 1. Mechanized, modular reasoning is pushed lower, to include connection to verified hardware (and perhaps also compilation to machine code rather than assembly code counts as a difference, though I'm not sure if this paper uses any of the work on generating binaries from CompCert).
> 2. Mechanized, modular reasoning starts higher, with several different verified and proof-generating compilers from a functional source language (Gallina).
> 3. Total correctness is proved for all modules, ruling out unintended divergence or memory exhaustion.
> 4. The shared formal framework is arguably simpler: every imperative programming language gets a weakest-precondition semantics in terms of a common notion of memory and traces of external interactions, and module-correctness theorems are stated directly in terms of such semantics, with no fixed program logic *or* algebra of components like in the present paper.  Higher-order logic does the job directly, adopting design patterns from separation logic as convenient.
> 
> I can also think of some advantages for this paper vs. work with Bedrock2:
> 1. The framework is much more general, with nothing remotely language-specific fixed in the basic definitions, whereas the common specification style in the work with Bedrock2 fixes a particular state type of memory plus log of external interactions.  We never see one (software) module implement the methods that are being logged in the trace of another.
> 2. This paper and the ones it builds most closely on work with CompCert and roughly full-ISO-standard C, while Bedrock2 fixes a simple C subset with a simple semantics (e.g., assembly-style flat memory model).
> 3. The formalism supports a flexible notion of refinement even for infinitely reactive modules, which may allow well-structured proof of more interestingly interactive systems than in Bedrock2, which has only been shown to support interactive systems as event loops where each event handler terminates.
> 4. It seems that some higher-level results about multiple programs piped together UNIX-style can be proved, though it is unclear, from the writing of the paper, if those mechanized proofs have been completed.  (An example like Fig. 1's is easily verified modularly [down to program binaries with their separate specifications] with the techniques that the cited papers demonstrated, though I don't think they demonstrated reasoning about groups of processes running simultaneously.  I would not be surprised either way if an extra layer to reason about process compositions turned out to be simple or to require serious new research.)
> 
> Though actually, real UNIX process compositions involve concurrent executions that synchronize through byte streams, while this paper seems to study a restricted notion of piping that looks like method calls across processes, where only one process runs instructions at a time.  In that sense, Fig. 1 may be misleading when it comes to what class of systems can be verified with the new ideas.  Presumably generalization to truly concurrent process execution is nontrivial future work.

Thank you for this detailed comparison.
We will be sure to incorporate the work you reference
and the points you raise into
the relevant parts of the *Introduction* and *Related Work* sections.

It is true that supporting concurrency would be necessary
to provide a complete and accurate model of Unix processes.
Our goal with Fig. 1 is to illustrate some of the issues that come up
when the horizon of verification is pushed beyond the boundary
of a fixed language or model,
yet have an example that remains simple and that
most reader will be familiar with.

> So, overall, I think the framework presented here may represent a strong contribution, but the conditions of its novelty and payoff over other ideas should be reformulated, in my opinion.
> 
> One more question about limits of the framework: refinement conventions seem to enforce one-to-one matching of events.  Wouldn't it often happen that one event in a higher-level system is compiled to multiple events in a lower-level system?  Or these conventions really deal only with events at the rough semantic granularity of function calls, where lock-step bisimulation holds throughout compilation?  One example would be that what looks like a monolithic system call at the source level looks like multiple memory-mapped-IO accesses in compiled code, with each memory access a separate event.

Indeed, refinement conventions
are intended mainly to deal with changes in data representation.
They are very flexible in this regard and can handle:

  * *relational* ($n$-to-$m$) abstraction relationships between
    source- and target-level moves (questions and answers),
    which are needed in particular to formulate
    the CompCertO calling convention;
  * relationships that evolve through time and take into account
    the history of the computation;
  * different degrees of state encapsulation between the source
    and target;

but as you note they can only deal with synchronized
source- and target- external interactions.

With that said,
a transformation like the one you suggest
can still be dealt with through a combination of
horizontal composition and refinement conventions.
Suppose we have a source-level strategy
$\sigma : \mathcal{S} \twoheadrightarrow \{* : \mathbf{1}\}$
which uses an abstract system call interface $\mathcal{S}$,
whereas in the target-level implementation
$\tau : \mathcal{M} \twoheadrightarrow \{* : \mathbf{1}\}$
each system call is expanded into multiple memory-mapped accesses
represented as operations in the signature $\mathcal{M}$.
It may be possible to first represent this expansion
using a strategy $x : \mathcal{M}' \twoheadrightarrow \mathcal{S}$,
then (if necessary)
further translate the data representation of memory-mapped accesses
using a refinement convention $R : \mathcal{M}' \leftrightarrow \mathcal{M}$.
The correctness property would then be formulated as:
$$\sigma \odot x \:\le_{R \twoheadrightarrow \{* : \mathbf{1}\}}\: \tau .$$

In principle,
it should be possible to design a framework where
horizontal strategies and vertical refinement conventions
would be subsumed by a single notion of
dually non-deterministic *strategy specifications*,
as is attempted in [Koenig & Shao, LICS 2020].
In such a framework,
refinement conventions would become
formal adjunctions of strategy specifications
and would be capable of representing the kind of transformations
you are getting at.
However we have found that general dual nondeterminism
(represented using sets of sets of traces)
brings subtle issues of commutation between angelic and demonic choices
that are very difficult to deal with
and significantly increase complexity.
By contrast,
our refinement conventions represent a kind of "sweet spot" where
in exchange for restricting the shape of the correspondence,
we can incorporate both the angelic and demonic choices
necessary for relational abstraction
while remaining within the bounds of a "set of traces" approach
and escape the need for full-blown dual nondeterminism.

> # More minorly...
> 
> Section 4 introduces a backslash operator, which I had trouble interpreting.  Line 626 finally explains through an example.
> 
> I think "ie." is properly written "i.e.".
> 
> L211: "Layered composition (...) acts horizontally": that's a surprising sentence!  I'm used to thinking of vertical composition as being all about layered interfaces, like in the past CAL work cited here.
> 
> L265: "is value": add "the" in the middle.
> 
> L335: At this point, I don't remember what $\mathcal S$ and $\mathcal P$ are, so I get lost interpreting the explanation.
> 
> L423: careful with "single-file"; it means something else colloquially in English, which led me to expect that we were talking about sequentializing a process that began as more parallel.
> 
> L499: "that same way": probably "the same way" is what you meant?
> 
> L617: "pays": should be "plays"?
> 
> L641: "rediduals"?
> 
> L750: "a sequence values": make it "of values".
> 
> L977: "Also, The": rogue capital letter
> 
> L988: "mange": should be "manage"
> 
> L988: "shared passed": probably don't want both of these words
> 
> L1016: "from the CompCertO's": remove "the".  Happens twice; same issue with "the RAX".
> 
> L1048: "Reasoning the process behavior": missing "about"
> 
> L1050: "Therefore, We": rogue capital letter
> 
> L1072: "the CompCertO's": remove "the"
> 
> L1180: "languauges"
> 
> L1202: "consist solely events": missing "of"
> 
> 
> 
> Review #239B
> ===========================================================================
> 
> Overall merit
> -------------
> 3. Weak accept
> 
> Reviewer expertise
> ------------------
> 3. Knowledgeable
> 
> Paper summary
> -------------
> This paper presents a refinement system that allows the combination of various proofs at different levels of abstraction.  The system uses an almost game semantics like approach with traces over questions and answers.  The program receives questions from its context and can provide an answer or a question in response. If it questions the context, then the context must provide an answer before the program continues.  This is like the modelling of higher-order functions in game semantics.
> 
> The system provides operations to compose components by
> * routing questions from one component to another component, and
> * by allowing two components to handle the questions.
> 
> It also provides refinement on components by relating actions of one component to actions of another component, and by allowing internal state access to be hidden.
> 
> The overall system seems quite compelling, however, I found the paper hard to follow in places, and do not feel confident that I fully understood the results.  I would love to read an improved version of this paper.
> 
> Comments for authors
> --------------------
> # Understanding
> 
> At a high-level I really like the ideas in the paper, but I found I got stuck a lot of times reading it.  I have provide a detailed suggestion here, and lots of minor suggestions below. I really hope you will improve the presentations of the maths, as I would really like to understand the paper better.
> 
> My biggest stumbling block on reading the paper was the description of a strategy (defn 3.1).  It took more a long time to realise what this meant, and how to read it in subsequent definitions.  $s^q$ is not a modified version of $s$, but a different syntactic category that ranges over strategies expecting an answer to $q$.  Using the superscript in this way is very compact, but I found difficult to read.  The $q$ in $s^q$ is effectively a parameter to the syntactic category.  I think this might be easier to understand if you have defined it with rules:
> 
> $\frac{}{epsilon \in P_{E,F}}$
> 
> $\frac{s \in P^q_{E,F}   \quad q \in F}
> {q\, s \in P_{E,F} }$
> 
> $\frac{s \in P^{qm}_{E,F}   \quad m \in E}
> {\underline{m}\, s \in P^q_{E,F} }$
> 
> The parameterised syntactic categories I found extremely hard to read. They are expressing dependencies as well, that similarly is challenging.
> 
> If you did it the other way, you probably just use $s$ every where in Definition 3.3, and I think it would be much easier to follow.
> 
> Your current definition also leads to complexity for instance line 525-534.  Here you are using t instead of s. But you haven't defined this.  If you defined the sets, then you could just say $t \in P_{E,F}$.  After a while of trying to understand I got this, but the presentation really confused me.

Thank you for articulating this and for your suggestion in how to address it.
We agree this would improve the presentation significantly
and will adopt this approach in any revision.

> # Related work
> 
> The model feels very close to the models of game semantics, and I would have like the paper to contain a deeper comparison.  There is also the refinement calculus that has very similar aims to this work, and I think should be compared with.  There is also the Concurrent Kleene Algebra of Hoare et al.  2011 which provides various trace like composition operations. 
> 
> Overall, I was quite surprised how few references to other works were contained in this paper.  I think relating to some of these other approaches would help the reader to set context, and understand how your operators can be viewed.

We acknowledge that the discussion of related work and the broader context
is insufficient and we would need to improve this aspect in the final version.

In regards to the specific lines of work you mention:

  * We would indeed consider the horizontal fragment of our framework
    a simple form of game semantics, where effect signatures serve as
    a very restricted form of game (one question followed by one answer).
    Future work could explore whether the vertical and spatial components
    of our framework could be extended to support more complex games.
  * The refinement calculus is very relevant as well, and part of our
    motivation is indeed to explore how some of the ideas transpose to the
    richer / more interactive world of game semantics. As discussed above,
    in formulating refinement conventions we have gone to some length to
    figure out how to retain the relational abstraction capabilities of the
    refinement calculus while avoiding the unrestricted forms of dual
    nondeterminism found in the predicate transformer and
    multirelation-based models.

[XXX say something about CKA?]

> # Minor comments
> 
> Line 275: _strategy_ it is not clear what this is at this point.  You don't provide enough information here to get an intuition of what a strategy is.  I think you need to give more semantic definition at this point.   I was wondering is it a state machine, a set of traces/trees of actions, a game?
> 
> Fig 5.  This ring buffer can overflow.  You should say why this doesn't matter.
> 
> Line 389 "$E_1$ and $E_1$"
> 
> Line 417: $\iota$ what is this?  Is it an injection into a sum type?
> 
> Line 567 $\underline{m}n\setminus \sigma$ is this all the traces in $\sigma$ which begin $\underline{m}n$ with that part removed from the trace?  You need to explain this here.
> 
> Line 579: $\underline{m}_1 \in \sigma$ means there exists a starting with $\underline{m}_1$.  This works because the strategies are prefix closed?
> 
> Line 579 $\underline{m}_1$ I think really you should have $\underline{m_1}$ as you have really defined underlining as part of the syntax, so you are applying it to the $m_1$ identifier.
> 
> Line 617: "until pays of"  should be "plays"
> 
> Line 644 $q_2 \setminus \tau$ I guess it is okay for this to be the empty set, as this will never match one of the $epsilon$ cases?
> 
> Line 693: $\iota$ assuming this is some form of injection into a sum type, why is it only required on the first sum type. Why are their no injections on $m_2$, $n_1$ and $n_2$?
> 
> Line 705: "in the Appendix A", drop either "the" or "A".
> 
> Line 718: $@$ is this just syntax for pairing here?
> 
> Line 900: $Y^{s_1}$: here $s_1$ is from $S$, but I was expecting the superscript to be from $A$ based on the type?
> 
> Line 989: "mange" 
> 
> Line 989: "memory shared passed"
> 
> 
> 
> Review #239C
> ===========================================================================
> 
> Overall merit
> -------------
> 3. Weak accept
> 
> Reviewer expertise
> ------------------
> 3. Knowledgeable
> 
> Paper summary
> -------------
> This paper proposes a new compositional semantics framework, which allows us to reason about multi-language systems, where each language has its own (operational) semantics and may or may not be compiled to another language, and where programs in various languages interact with each other.  The problem is that although we know how to reason about one language in isolation, or about a compiler from one language to another, or even about interactions between abstract processes, there is no universal formalism that allows us to do all these uniformly.  In that vein, the paper introduces a generic semantic model based on refinement squares, which combines horizontal, vertical and spatial composition.  The model is shown to be versatile enough to capture / embed various semantics and verification proofs in the context of the CompCert certified compilation framework.  The work has been formalized in Coq (but the reviewers cannot see it).
> 
> To describe interfaces between components, the paper uses effect signatures, which are essentially sets E of questions with predefined sets of answers, ar: E -> Set, which can be regarded as black-boxes that one can interact with through a given interface.
> 
> To describe the behavior of program components, strategies are used, where a strategy L connects two effect signatures, say L : E -->> F.  The intuition is that a strategy abstractly captures what a program component does by telling how to model operations / questions of F using operations of E.  Strategies can be naturally composed.
> 
> To capture the fact that a component can be viewed at different levels of abstraction by other components in the system, a new notion of refinement convention between effect signatures is introduced, which generalizes an existing notion of simulation convention used in CompCertO.
> 
> The three ingredients above lead to a natural notion of refinement square / property / proof, which says that two strategies at two different levels of abstraction are strongly connected, in that no matter if one of the strategies is applied first followed by the other refinement abstraction, or viceversa, we get the same behavior.
> 
> Now refinement squares can combine horizontally along refinement edges, or vertically along strategy edges, or spatially by adding structural / state framing, and this way have a uniform way to capture various formal verification properties of interest and compose them.
> 
> The work is formalized in Coq, which supposedly means that all the theorems about CompCert, CompCertO, CompCertX, and CLight claimed in the paper are mechanically proved.  If I understand it correctly, one particular aspect of this work is that the existing operational semantics of the involved languages does not need to be modified.  If this is indeed true, then this is a major benefit.
> 
> Comments for authors
> --------------------
> The paper was pleasant to read initially, but it became heavier and heavier later on.  That is expected normally, but what made it heavier to digest, I believe, is the lack of motivation once the technical details started.  The example in Fig 1 is very simple and can be handled many different ways using existing solutions.  There may not be any one particular approach that handles the example completely, but combinations of approaches and tools can certainly work together to prove it, even modulo acceptable assumptions.  Then the discussion jumped to CompCertO and CLight and claimed that the same approach applies to those. This creates a gap for the reader who does not know those semantics and their challenges.  For example, I am not convinced that their simulation convention was not good enough and required to be generalized into a refinement convention.
> 
> Once the gap happened, then it was harder and harder to build a mental model for the refinement squares that stayed for the rest of the paper.  Since there are so many different ways to construct such refinement squares, one wonders what is the benefit of enforcing them first place.  Why not let formal verification engineers formalize the various levels of abstraction in their adhoc way, on a case by case basis.  For the example in Fig 1, then one can certainly formalize the semantics of read/write buffers and external interaction, etc., and then still prove the desired property somehow compositionally, using the existing semantics of x86 and C and the guarantees offered by CompCert.  Yes, it could be a bit more tedious, but it gives the verification engineer full freedom of choice for abstractions, without having to obey the required formalism with refinement squares, refinement conventions, etc. 
> 
> I give the paper a positive score because I believe this line of work is important and generalizations like those proposed are important.  I have reservations, though, because: 1. the Coq code was not available, so we don't know how complex the integration with CompCert was and how non-intrusive wrt the existing operational semantics; and 2. other applications outside of C were not attempted.  Also, at a very high level, I am not convinced that formal verification is not more broadly used because of missing types of compositionality that the paper resolves; lack of automation and heavy formalisms trump compositionality, I believe.
 
> Finally, I have a curiosity question: does the framing property really hold for the actual C language?  Functions like malloc have different behaviors depending on how much memory is available, no?

The CompCert memory model does not enforce any availability constraints
and as a result `malloc` always succeeds, so this was not an issue in our
implementation. We should also not that this issue is a broader concern which
applies to separation logic and separation algebra as well as our framework.
With that said, there multiple ways in which this could be handled.

In a more accurate model, a memory state $(m, k)$ could incorporate
a number $k$ of bytes that remain to be allocated
(or perhaps the size of the largest contiguous memory region available,
to take fragmentation into account),
and have `malloc` trigger an undefined behavior
if the user attempts to allocate a region of size ${} > k$.
This would still work since the separation algebra
could be defined along the lines
$$(m_1, k_1) \bullet (m_2, k_2) = (m_1 \circ m_2, k_1 + k_2),$$
and incorporating an additional memory share could only
increase the amount of memory available.
Consequently the behavior of `malloc` on the larger share
would refine that of the original one,
validating our frame rule.

Another option would be to model memory allocation as a permission,
for example by constraining $k_1 = 0 \vee k_2 = 0$ in the definition above,
so that only one share at a time would be able to allocate new memory
and so that additional shares could never increase the available amount.
This would make it possible to model an "out of memory" error as an additional
positive behavior rather than treating it as an undefined behavior.

In either case, reasoning about memory allocation would probably require some
kind of ownership transfer (of the available memory or OF the "allocation"
permission). This could likely be modeled in our framework without difficulty.
For example our refinement convention $\mathbf{Y}$ allows source-level
shares to migrate across time between the two branches.

