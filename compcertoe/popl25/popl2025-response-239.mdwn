We are grateful for the reviewers' hard work and valuable feedback.
Since cross-cutting concerns include the motivation and framing of our contributions,
the relationship of our work to other relevant research, and the status of our
implementation, we will discuss these issues first. We then include a proposed
revision plan for the paper and provide responses to reviewers' individual concerns.

## I. Motivation and Novelty

Our long-term goal is to facilitate
the construction of large-scale certified systems.
In the same way a present-day system administrator
would set up an infrastructure by
selecting a combination of existing hardware and software components
and configuring them to work together,
we envision a situation where
future certified systems architects
would build complex systems
by assembling off-the-shelf certified components,
and would be able to obtain end-to-end proofs of correctness
for very large systems
with little additional effort.

Figuring out whether and how this vision can be realized
will require research in several directions,
including (i) improving the range of proof automation techniques
and streamlining the experience of verification engineers
(as suggested in Review #239C)
as well as (ii)
conducting more integration verification case studies
(like those referenced by Review #239A).
Simultaneously,
we must (iii) draw from these experiences
to develop a systematic understanding
of the principles underlying these successes,
and distill them into new mathematical tools
which can then serve as a foundation for
the next round of ground-breaking challenges and research.
Although our **§6 Applications** section illustrates
concrete verification tasks carried out using our framework,
we would consider that our work and claim to novelty
falls squarely under (iii).

**Illustration with integration verification.**
The Bedrock2 case studies mentioned by Review #239A
can be used to illustrate this point.
Erbsen et al. [PLDI '24] presents the end-to-end verification of
a minimalistic but sophisticated embedded system,
which mediates access to an external actuator
(in their demonstration, the opening mechanism for
a miniature replica of a garage door)
through cryptographically authenticated network commands.
The top-level correctness statement takes the form:
$$
\mathtt{always\:\:run1\:\:(eventually\:\:run1\:\:}
(q \mapsto \mathtt{io\_spec}\:\:q.\mathtt{getLog}))
\:\:q_0\,,\quad(1)
$$
where
`run1` models one execution step of the low-level RISC-V system being verified,
$q_0$ is the initial state of the system,
`getLog` extracts from the state
the accumulated record of external interactions,
which must satisfy the predicate `io_spec`.
An equivalent formulation within our framework may look something like:
$$
\mathtt{io\_spec}\:\le_{R \twoheadrightarrow S}\:
[q_0\rangle \odot (\cdots \mathtt{run1} \cdots)\,,
\quad(2)
$$
where $\mathtt{io\_spec}:\mathcal{A}\twoheadrightarrow\mathcal{N}$
specifies which actuator commands ($\mathcal{A}$)
should result from given network interactions ($\mathcal{N}$),
where the right-hand side of $\le$ models the implemented system,
and where $R$ and $S$ can be used to spell out any abstraction relationships
between this implementation and the high-level `io_spec`.

Logically,
the formulations (1) and (2) are quite close.
Both use external interaction traces
to specify the system's observable behavior,
and it may in fact be relatively straightforward
to establish something like (2)
from the property (1) proven in the existing development.
At the same time,
reformulating the correctness property in this ``standard refinement'' form
presents some advantages:

  * First, our composition principles would be immediately available
    to someone who were trying to *prove* the property again, and they may
    lessen the need to use other kinds of compositional techniques.
    As we understand it, the Bedrock proof relies on forms of separating
    conjunction and data abstraction, which in a different version
    could conceivably fall under our spatial and vertical composition infrastructure.
    This approach may also make it easier to reuse the CompCertO correctness
    theorem, CAL methodology, ... as needed.
  * More to the point (since the proof already exists),
    consider a *user* of this certified network actuator, seeking
    to use it as a component within (say) a larger certified industrial control
    network.
    At first, they will want to reason about the correctness of their overall
    network in terms of `io_spec` alone, and they should not be forced to deal with
    the implementation details spelled out in `run1` and $q_0$.
    At a later stage, they may choose
    among several competing vendors for certified `io_spec` implementations,
    and would simply be able to "plug" the refinement (2) provided by the
    selected vendor within their own proof, replacing `io_spec` by their chosen
    implementation in a correctness-preserving manner.

This mirrors what happens in our paper with Fig 1 / Example 1.1 / §6.3,
where correctness results obtained for individual programs
can be reused and made part of a "next-scale" system of interacting processes,
where the details of their internal semantics and execution can be encapsulated
and abstracted away when we reason about their coarse-grained interaction patterns.

In summary,
our goal is not to supplant or out-do
any prior verification effort or technique.
As noted in Reviews #239A and #239C, and alluded to on ll. 93--117,
our main example could be tackled using existing techniques,
and we intend it primarily for illustration purposes.
It is also true that extending the framework to support concurrency
would significantly increase its power and
will be necessary to tackle many complex systems.
However,
by consolidating a broad range of existing
verification techniques, tools and results
under the scope of a single semantic model
where they can be made open-ended and interoperable,
we claim our work represents strong contribution towards
a state-of-the-art where
the verification of even larger-scale
realistic systems can be undertaken.

## II. Relationship to other work

Since our paper is missing an extensive comparison between our model
and several existing approaches, we briefly discuss below some
of work mentioned by the reviews.

**Interaction Trees.**
As a "semantic tool" of sorts,
interaction trees share some goals and techniques with our model.
In fact, an interaction tree `t : itree E X` can be intepreted
into our framework as
a strategy $\langle t \rangle : E \twoheadrightarrow \{* : X\}$.
However, strategies generalize ITrees in several ways:

  * Strategies are two-sided and encode incoming as well as outgoing
    interactions, forming the base for horizontal composition
  * By design, ITrees must be executable programs, whereas strategies can be
    described logically using more declarative and arbitrary Coq specifications.
  * Strategies which exhibit the same external behavior are formally *equal*.
    By contrast, ITrees are compared using *bisimlation equivalences*.
    Equational reasoning requires Coq's setoid support, which can be slower and
    more fragile than rewriting with `eq`.
  * Our strategies come with built-in notions of partial definition, refinement
    and data abstraction, whereas similar notions for ITrees have to be
    defined and tailored to a particular application.

**Game Semantics.**
Review #239B mentions the similarity of our approach to game semantics.
Indeed we would consider the horizontal fragment of our framework
as a particularly simple form of game semantics.
The novelty resides in the vertical and spatial fragments,
for which to our knowledge there exists no precedent in the game semantics literature.
In particular,
refinement conventions involve alternations of angelic and demonic choices;
we were surprised to find they can be modeled using a fairly standard approach,
though a rather unconventional ordering of plays must be used accordingly.
An interesting question for further research would be to investigate
how far this can be pushed and whether games more complex than effect signatures
could admit their own forms of refinement conventions.

**Refinement Calculus.**
Review #239B further mentions the refinement calculus as another point of comparison,
and indeed it was a major source of inspiration for our framework.
One defining feature of the refinement calculus is dual nondeterminism,
which provides very powerful abstraction mechanisms.
At the same time, models like predicate transformers cannot deal with
external interactions or state encapsulation in the way we want,
and while there have been attempts to incorporate dual nondeterminism into game semantics,
we have found that arbitrary alternations of angelic and demonic choices
introduce a high degree of complexity that is difficult to deal with in our context.
This complexity can be avoided by performing data abstraction in a more controlled way
in the form of refinement conventions.

**CompCertO.**
Finally it is worth pointing out the ways in which our framework
generalizes the CompCertO model,
especially when it comes to refinement conventions
(Review #239C):

  * CompCertO transition systems and simulation conventions
    use explicit states and Kripe worlds in their definitions,
    whereas strategies and refinement conventions provide canonical
    representations for the components' observable behaviors.
  * Effect signatures are more general than the language interfaces
    used in CompCertO, which force all questions to use the same set of
    answers.
  * CompCertO transition systems do not retain any history between 
    successive incoming questions; as such, they cannot support the kind of
    state encapsulation which our framework enables. Likewise, simulation
    conventions only specify 4-way relationships between isolated pairs of
    questions and answers, but unlike refinement conventions they cannot be
    sensitive to the history of the computation.

## III. Coq Implementation

All of the definitions, theorems and results presented in the paper
have been mechanized in Coq, including the applications outlined in §6
which incorporates the CompCertO semantics and correctness theorem
within our framework.
We will submit a polished artefact
to go with our paper.
The implementation of the framework is straightforward,
fits within a few thousand lines of Coq code,
and follows the definitions given in the paper quite closely. 

CompCertO was designed to be open-ended and the capabilities of our
framework encompass those of the CompCertO model.
As a result CompCertO transition systems,
simulation conventions and simulation proofs can be embedded as-is.

**One interesting aspect** of this embedding concerns the way
Kripe worlds are handled. In CompCertO simulations, worlds are chosen upfront.
But refinement conventions eliminate this form of branching,
and as a result the refinement squares embedded
from CompCertO simulations must keep track of all the possible worlds
corresponding to a pair of questions until they are answered.
We factor out this aspect of the proof by making the choice of world explicit
in the embedding $\bigvee_w [R]_w$ of a simulation convention $R$.
We can then use proof principles such as:
$$(\forall w\cdot\sigma\le_{R \twoheadrightarrow S_w}\tau)\Rightarrow
\sigma\le_{R \twoheadrightarrow\bigvee_w S_w}\tau$$
to help us reduce our proof obligations to consider
only a single world at a time.

## IV. Proposed edits and plan

We would be happy to incorporate these clarifications
and revise our paper based on the reviewers' feedback.
In particular, we will

  * In the introduction,
    clarify our contributions and the novel aspects of our work
    following the discussion under §I above.
    We will also strive to give a more comprehensive account of
    existing verification work combining multiple languages,
    compilation and external interactions, including Bedrock2
    integration verification projects.
  * More generally, we will strive to cover a broader range of
    Related Work, including but not limited to the comparisons
    we have outlined outlined under §II.
  * We will augment §6 with a qualitative and quantitative discussion and
    evalulation of our implementation (§III)
  * We will use the reviewers' excellent suggestions to improve the
    presentation, and generally strive to make the paper easier to follow.

In the remainder of this response,
we address each reviewer's remaining questions and observations individually.

----------

> Review #239A
> ===========================================================================
> 
> (...)
>
> Comments for authors
> --------------------
> This sort of unified theory is critical to application at scale of the most rigorous formal methods, and I'm impressed at the thoroughness of the design effort reported in the paper.  One strange thing is that the paper doesn't include an evaluation section; we don't get a clear description of which verifications have been carried out, which is handy to validate design choices.

We intend §6
to demonstrate the suitability of our framework
for a range of practical verification tasks,
and to serve as an evaluation of sorts,
but we acknowledge the implementation should be discussed in more detail.
We hope the clarifications under §III above were helpful.

> The previous papers that this project built on were notable for significant mechanized case studies of software artifacts of inherent interest, and the lack of such validation here gives the work less appeal.  I also think the paper oversells the novelty of addressing this particular combination of problems.

We have tried to articulate in the two paragraph ll. 85--97 that
existing work does address this combination of problem
(certainly the Bedrock work you reference should be mentioned there),
and to explain why we believe a more systematic approach
would be beneficial,
but we will clarify our contributions and better frame our work
per the discussion under §I above.

> # Crisply formulating what new problem is being solved
> 
> At line 38, there is one bullet list of aspects that are important to address in a single reasoning system: *compilation*, *several different languages*, and *external interactions*.  Then line 166 gives a separate list: program-module correctness, compiler correctness, separation logic-style memory modularity, and data abstraction.  Fig. 1 also shows a motivating example that includes mixing C and assembly and calling I/O system routines.  I agree there is very little work that ties all of these challenges together, but there is at least one project that checks all of those boxes.  See:
> 
> - Andres Erbsen, Jade Philipoom, Dustin Jamner, Ashley Lin, Samuel Gruetter, Clément Pit-Claudel, Adam Chlipala. Foundational Integration Verification of a Cryptographic Server. PLDI'24.
> - Andres Erbsen, Samuel Gruetter, Joonwon Choi, Clark Wood, Adam Chlipala. Integration Verification Across Software and Hardware for a Simple Embedded System. PLDI'21.
> 
> That line of work is centered around the Bedrock2 framework.  There are a few advantages that could be argued for it over the framework presented in this paper:
> 1. Mechanized, modular reasoning is pushed lower, to include connection to verified hardware (and perhaps also compilation to machine code rather than assembly code counts as a difference, though I'm not sure if this paper uses any of the work on generating binaries from CompCert).
> 2. Mechanized, modular reasoning starts higher, with several different verified and proof-generating compilers from a functional source language (Gallina).
> 3. Total correctness is proved for all modules, ruling out unintended divergence or memory exhaustion.
> 4. The shared formal framework is arguably simpler: every imperative programming language gets a weakest-precondition semantics in terms of a common notion of memory and traces of external interactions, and module-correctness theorems are stated directly in terms of such semantics, with no fixed program logic *or* algebra of components like in the present paper.  Higher-order logic does the job directly, adopting design patterns from separation logic as convenient.
> 
> I can also think of some advantages for this paper vs. work with Bedrock2:
> 1. The framework is much more general, with nothing remotely language-specific fixed in the basic definitions, whereas the common specification style in the work with Bedrock2 fixes a particular state type of memory plus log of external interactions.  We never see one (software) module implement the methods that are being logged in the trace of another.
> 2. This paper and the ones it builds most closely on work with CompCert and roughly full-ISO-standard C, while Bedrock2 fixes a simple C subset with a simple semantics (e.g., assembly-style flat memory model).
> 3. The formalism supports a flexible notion of refinement even for infinitely reactive modules, which may allow well-structured proof of more interestingly interactive systems than in Bedrock2, which has only been shown to support interactive systems as event loops where each event handler terminates.
> 4. It seems that some higher-level results about multiple programs piped together UNIX-style can be proved, though it is unclear, from the writing of the paper, if those mechanized proofs have been completed.  (An example like Fig. 1's is easily verified modularly [down to program binaries with their separate specifications] with the techniques that the cited papers demonstrated, though I don't think they demonstrated reasoning about groups of processes running simultaneously.  I would not be surprised either way if an extra layer to reason about process compositions turned out to be simple or to require serious new research.)

Thank you for this detailed comparison.
We will incorporate these points into
the relevant parts of the *Introduction* and *Related Work* sections,
and hope you found our above discussion of the PLDI'24 Erbsen et al. work useful as well.

> Though actually, real UNIX process compositions involve concurrent executions that synchronize through byte streams, while this paper seems to study a restricted notion of piping that looks like method calls across processes, where only one process runs instructions at a time.  In that sense, Fig. 1 may be misleading when it comes to what class of systems can be verified with the new ideas.  Presumably generalization to truly concurrent process execution is nontrivial future work.

Yes this is true,
and we will clarify that we do not claim
to provide a complete and accurate model of Unix processes.
Rather, our goal with Fig. 1 is to illustrate some of the issues that come up
when the horizon of verification is pushed beyond the boundary
of a fixed language or model,
with an example that remains simple and that
most readers would find familiar and easy to understand.

> So, overall, I think the framework presented here may represent a strong contribution, but the conditions of its novelty and payoff over other ideas should be reformulated, in my opinion.
> 
> One more question about limits of the framework: refinement conventions seem to enforce one-to-one matching of events.  Wouldn't it often happen that one event in a higher-level system is compiled to multiple events in a lower-level system?  Or these conventions really deal only with events at the rough semantic granularity of function calls, where lock-step bisimulation holds throughout compilation?  One example would be that what looks like a monolithic system call at the source level looks like multiple memory-mapped-IO accesses in compiled code, with each memory access a separate event.

Indeed, refinement conventions
are intended mainly to deal with changes in data representation.
They are very flexible in this regard and can handle:

  * *relational* ($n$-to-$m$) abstraction relationships between
    source- and target-level moves (questions and answers),
    which are needed in particular to formulate
    CompCertO simulation conventions;
  * relationships that evolve through time and take into account
    the history of the computation;
  * different degrees of state encapsulation between the source
    and target;

but as you note they can only deal with synchronized
source- and target- external interactions.

With that said,
a transformation like the one you suggest
can still be dealt with through a combination of
horizontal composition and refinement conventions.
Suppose we have a source-level strategy
$\sigma : \mathcal{S} \twoheadrightarrow \{* : \mathbf{1}\}$
which uses an abstract system call interface $\mathcal{S}$,
whereas in the target-level implementation
$\tau : \mathcal{M} \twoheadrightarrow \{* : \mathbf{1}\}$
each system call is expanded into multiple memory-mapped accesses
represented as operations in the signature $\mathcal{M}$.
It may be possible to first represent this expansion
using a strategy $x : \mathcal{M}' \twoheadrightarrow \mathcal{S}$,
then (if necessary)
further translate the data representation of memory-mapped accesses
using a refinement convention $R : \mathcal{M}' \leftrightarrow \mathcal{M}$.
The correctness property would then be formulated as:
$$\sigma \odot x \:\le_{R \twoheadrightarrow \{* : \mathbf{1}\}}\: \tau .$$

In principle,
it should be possible to design a framework where
horizontal strategies and vertical refinement conventions
would be subsumed by a single notion of
dually non-deterministic *strategy specifications*,
as is attempted in [Koenig & Shao, LICS 2020].
In such a framework,
refinement conventions would become
formal adjunctions of strategy specifications
and would be capable of representing the kind of transformations
you are getting at.
However we have found that general dual nondeterminism
(represented using sets of sets of traces)
brings subtle issues of commutation between angelic and demonic choices
that are very difficult to deal with
and significantly increase complexity.
By contrast,
our refinement conventions represent a kind of "sweet spot" where
in exchange for restricting the shape of the correspondence,
we can incorporate both the angelic and demonic choices
necessary for relational abstraction
while remaining within the bounds of a "set of traces" approach
and escape the need for full-blown dual nondeterminism.

> # More minorly...
> 
> Section 4 introduces a backslash operator, which I had trouble interpreting.  Line 626 finally explains through an example.
> 
> I think "ie." is properly written "i.e.".
> 
> L211: "Layered composition (...) acts horizontally": that's a surprising sentence!  I'm used to thinking of vertical composition as being all about layered interfaces, like in the past CAL work cited here.
> 
> L265: "is value": add "the" in the middle.
> 
> L335: At this point, I don't remember what $\mathcal S$ and $\mathcal P$ are, so I get lost interpreting the explanation.
> 
> L423: careful with "single-file"; it means something else colloquially in English, which led me to expect that we were talking about sequentializing a process that began as more parallel.
> 
> L499: "that same way": probably "the same way" is what you meant?
> 
> L617: "pays": should be "plays"?
> 
> L641: "rediduals"?
> 
> L750: "a sequence values": make it "of values".
> 
> L977: "Also, The": rogue capital letter
> 
> L988: "mange": should be "manage"
> 
> L988: "shared passed": probably don't want both of these words
> 
> L1016: "from the CompCertO's": remove "the".  Happens twice; same issue with "the RAX".
> 
> L1048: "Reasoning the process behavior": missing "about"
> 
> L1050: "Therefore, We": rogue capital letter
> 
> L1072: "the CompCertO's": remove "the"
> 
> L1180: "languauges"
> 
> L1202: "consist solely events": missing "of"

Thank you for noting those,
we will be sure to fix them.

> Review #239B
> ===========================================================================
> 
> (...)
> 
> Comments for authors
> --------------------
> # Understanding
> 
> At a high-level I really like the ideas in the paper, but I found I got stuck a lot of times reading it.  I have provide a detailed suggestion here, and lots of minor suggestions below. I really hope you will improve the presentations of the maths, as I would really like to understand the paper better.
> 
> My biggest stumbling block on reading the paper was the description of a strategy (defn 3.1).  It took more a long time to realise what this meant, and how to read it in subsequent definitions.  $s^q$ is not a modified version of $s$, but a different syntactic category that ranges over strategies expecting an answer to $q$.  Using the superscript in this way is very compact, but I found difficult to read.  The $q$ in $s^q$ is effectively a parameter to the syntactic category.  I think this might be easier to understand if you have defined it with rules:
> 
> $\frac{}{epsilon \in P_{E,F}}$
> 
> $\frac{s \in P^q_{E,F}   \quad q \in F}
> {q\, s \in P_{E,F} }$
> 
> $\frac{s \in P^{qm}_{E,F}   \quad m \in E}
> {\underline{m}\, s \in P^q_{E,F} }$
> 
> The parameterised syntactic categories I found extremely hard to read. They are expressing dependencies as well, that similarly is challenging.
> 
> If you did it the other way, you probably just use $s$ every where in Definition 3.3, and I think it would be much easier to follow.
> 
> Your current definition also leads to complexity for instance line 525-534.  Here you are using t instead of s. But you haven't defined this.  If you defined the sets, then you could just say $t \in P_{E,F}$.  After a while of trying to understand I got this, but the presentation really confused me.

Thank you for articulating this and for your suggestion in how to address it.
We agree this would improve the presentation significantly
and would adopt this approach in the revised version.

> # Related work
> 
> The model feels very close to the models of game semantics, and I would have like the paper to contain a deeper comparison.  There is also the refinement calculus that has very similar aims to this work, and I think should be compared with.  There is also the Concurrent Kleene Algebra of Hoare et al.  2011 which provides various trace like composition operations. 
> 
> Overall, I was quite surprised how few references to other works were contained in this paper.  I think relating to some of these other approaches would help the reader to set context, and understand how your operators can be viewed.

We acknowledge that the discussion of related work and the broader context
is insufficient and we would need to improve this aspect in the final version.
We hope our commentary in §II above was helpful
and would be sure to discuss all three models you mention.

> # Minor comments
> 
> Line 275: _strategy_ it is not clear what this is at this point.  You don't provide enough information here to get an intuition of what a strategy is.  I think you need to give more semantic definition at this point.   I was wondering is it a state machine, a set of traces/trees of actions, a game?

You are right, we will include some high-level overview of basic game semantics notions at this point.

> Fig 5.  This ring buffer can overflow.  You should say why this doesn't matter.

Right. The available space is sufficient to implement a queue with capacity at most $N$.

> Line 389 "$E_1$ and $E_1$"
> 
> Line 417: $\iota$ what is this?  Is it an injection into a sum type?

Yes! We will clarify.

> Line 567 $\underline{m}n\setminus \sigma$ is this all the traces in $\sigma$ which begin $\underline{m}n$ with that part removed from the trace?  You need to explain this here.

Yes, sorry for this oversight. We will explain.

> Line 579: $\underline{m}_1 \in \sigma$ means there exists a starting with $\underline{m}_1$.  This works because the strategies are prefix closed?

Correct.

> Line 579 $\underline{m}_1$ I think really you should have $\underline{m_1}$ as you have really defined underlining as part of the syntax, so you are applying it to the $m_1$ identifier.

This is a good point. We will do that.

> Line 617: "until pays of"  should be "plays"
> 
> Line 644 $q_2 \setminus \tau$ I guess it is okay for this to be the empty set, as this will never match one of the $epsilon$ cases?

Yes! If $\tau$ does not trigger the move $q_2$, this will be empty and cause the simulation to fail.

> Line 693: $\iota$ assuming this is some form of injection into a sum type, why is it only required on the first sum type. Why are their no injections on $m_2$, $n_1$ and $n_2$?

Sorry, we mixed up this definition with the tupling of
$R_1 : E \leftrightarrow F_1$ and
$R_2 : E \leftrightarrow F_2$ into
$\langle R_1, R_2 \rangle : E \leftrightarrow F_1 \oplus F_2$.
We will sort things out.

> Line 705: "in the Appendix A", drop either "the" or "A".
> 
> Line 718: $@$ is this just syntax for pairing here?

In $m@u$, yes! We will clarify.

> Line 900: $Y^{s_1}$: here $s_1$ is from $S$, but I was expecting the superscript to be from $A$ based on the type?

We will clarify by describing $Y$ in the list above (l. 898) as
"a family $(Y^s)_{s \in S}$ of relations $Y^s \subseteq A^\bullet \times S$".

> Line 989: "mange" 
> 
> Line 989: "memory shared passed"

Thank you for the remaining issues you found,
we will be sure to correct them.

> Review #239C
> ===========================================================================
> 
> (...)
> 
> Comments for authors
> --------------------
> The paper was pleasant to read initially, but it became heavier and heavier later on.  That is expected normally, but what made it heavier to digest, I believe, is the lack of motivation once the technical details started.  The example in Fig 1 is very simple and can be handled many different ways using existing solutions.  There may not be any one particular approach that handles the example completely, but combinations of approaches and tools can certainly work together to prove it, even modulo acceptable assumptions.  Then the discussion jumped to CompCertO and CLight and claimed that the same approach applies to those. This creates a gap for the reader who does not know those semantics and their challenges.  For example, I am not convinced that their simulation convention was not good enough and required to be generalized into a refinement convention.

(We hope the discussion of CompCertO under §II above was useful in this regard.)

> Once the gap happened, then it was harder and harder to build a mental model for the refinement squares that stayed for the rest of the paper.  Since there are so many different ways to construct such refinement squares, one wonders what is the benefit of enforcing them first place.  Why not let formal verification engineers formalize the various levels of abstraction in their adhoc way, on a case by case basis.  For the example in Fig 1, then one can certainly formalize the semantics of read/write buffers and external interaction, etc., and then still prove the desired property somehow compositionally, using the existing semantics of x86 and C and the guarantees offered by CompCert.  Yes, it could be a bit more tedious, but it gives the verification engineer full freedom of choice for abstractions, without having to obey the required formalism with refinement squares, refinement conventions, etc. 
>
> I give the paper a positive score because I believe this line of work is important and generalizations like those proposed are important.  I have reservations, though, because: 1. the Coq code was not available, so we don't know how complex the integration with CompCert was and how non-intrusive wrt the existing operational semantics; and 2. other applications outside of C were not attempted.  Also, at a very high level, I am not convinced that formal verification is not more broadly used because of missing types of compositionality that the paper resolves; lack of automation and heavy formalisms trump compositionality, I believe.

Thank you for articulating this.
We will do our best to improve the presentation and
hope that our discussion in the main part of this response
addresses some of your objections and concerns.

> Finally, I have a curiosity question: does the framing property really hold for the actual C language?  Functions like malloc have different behaviors depending on how much memory is available, no?

The CompCert memory model does not enforce any availability constraints
and as a result `malloc` always succeeds, so this was not an issue in our
implementation. We should also not that this issue is a broader concern which
applies to separation logic and separation algebra as well as our framework.
With that said, there multiple ways in which this could be handled.

In a more accurate model, a memory state $(m, k)$ could incorporate
a number $k$ of bytes that remain to be allocated
(or perhaps the size of the largest contiguous memory region available,
to take fragmentation into account),
and have `malloc` trigger an undefined behavior
if the user attempts to allocate a region of size ${} > k$.
This would still work since the separation algebra
could be defined along the lines
$$(m_1, k_1) \bullet (m_2, k_2) = (m_1 \circ m_2, k_1 + k_2),$$
and incorporating an additional memory share could only
increase the amount of memory available.
Consequently the behavior of `malloc` on the larger share
would refine that of the original one,
validating our frame rule.

Another option would be to model memory allocation as a permission,
for example by constraining $k_1 = 0 \vee k_2 = 0$ in the definition above,
so that only one share at a time would be able to allocate new memory
and so that additional shares could never increase the available amount.
This would make it possible to model an "out of memory" error as an additional
positive behavior rather than treating it as an undefined behavior.

In either case, reasoning about memory allocation would probably require some
kind of ownership transfer (of the available memory or OF the "allocation"
permission). This could likely be modeled in our framework without difficulty.
For example our refinement convention $\mathbf{Y}$ allows source-level
shares to migrate across time between the two branches.

