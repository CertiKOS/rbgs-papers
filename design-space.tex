\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}

\hyphenation{CompCert}

\title{Behaviors in Composable CompCert}

\begin{document}

Below I outline the design space for
semantic domains that can help us
formulate the meta-theory for Composable CompCert.

%\section{What's our objective?} %{{{
%
%There are several things we could shoot for
%that could be called ``composable compilation''.
%All of them would be an improvement on
%the state of the art in terms of
%having a compiler that helps us build certified systems
%in a compositional way,
%given that:
%\begin{itemize}
%  \item Compositional Compcert \cite{compcompcert}
%    does not actually define the composite artefact
%    that the proof is about;
%  \item CompCertX can only be used in some ad-hoc ways
%    in the context of CertiKOS;
%  \item Separate CompCert \cite{sepcompcert}
%    \emph{does} actually give a proof about
%    the composite assembly program,
%    but has no semantic expression of composition.%
%    \footnote{%
%      In that respect,
%      maybe we can see the Separate CompCert theorem
%      as analoguous to the Concurrent CertiKOS thread linking theorem,
%      if we consider the active thread set to be part of the program syntax.
%    }
%\end{itemize}
%
%%}}}

\section{Component syntax} %{{{

I will avoid the loaded terms \emph{program} and \emph{module}
and use the term \emph{component}
to mean the kind of objects subject to horizontal composition.

\paragraph{CompCert programs} %{{{

The obvious approach is to follow Separate CompCert and
use CompCert programs (list of definitions) as our components.
Syntactic composition, as defined in recent versions of CompCert,
is the disjoint union of programs (taken as sets of definitions),
where internal and external definitions of the same global
are collapsed into a single internal definition
if they are compatible.

%}}}

\paragraph{Explicit modules} %{{{

Another approach is to define a component as a set of modules,
with each module a CompCert program (list of definitions).
This way we can define a semantics where
internal, cross-module communication is observable
(perhaps as $\tau$ transitions).
This simplifies horizontal composition
from the fact that no new silent moves
would be introduced or hidden,
but does not address linking.

However we we could introduce another syntactic operation
$L : \mathsf{component} \rightarrow \mathsf{module}$,
which would represent linking (in the sense of \textsf{ld}),
which would collapse back a multi-module component
into a single module.
Linking would look much like syntactic composition in the previous approach,
but possibly simpler because some aspects of it may have been decoupled
into the previously described
syntactic linking of multi-module components.

Another advantage of decoupling linking,
is that it would be legitimate
to limit linking to assembly,
which might be significantly simpler
than in the general case.

%}}}

\paragraph{Multi-level approach} %{{{

Take components as sets of CompCert LTS,
which could be computer from CompCert programs as modules,
or given as specifications.
Then we define the semantics of such
``semantic programs'' by taking the (semantic)
horizontal composition of the modules.
This side-steps the need for the syntactic composition theorem;
but this is essentially Compositional CompCert.

%}}}

%}}}

\section{Semantic model} %{{{

There are two questions here:
\begin{enumerate}
\item What behaviors does our model express/distinguish?
\item How are they represented?
\end{enumerate}

\subsection{Notion of behavior} %{{{

As far as the first question goes,
we probably want to keep in line with what CompCert does,
so that:
\begin{itemize}
\item Only externally observable differences count (trace equivalence);
\item Finite amounts of internal activity ($\tau$) are unobservable;
  however,
\item Divergence ($\Delta$) is observable,
  and incomparable with other behaviors;
\item Unsafe behavior ($\lightning$) is taken as $\top$;
\item Input and outputs are distinguished.
\end{itemize}
It would be conceivable to weaken this
(for instance to identify $\Delta$ with $\lightning$,
as is done in CertiKOS).
On the other hand we cannot strengthen it
(for instance by making $\tau$ observable)
since we wouldn't be able to embed
the CompCert correctness proof in a stronger setting.
We could, however,
use stronger preorders as proof methods
for the weaker version
(as with forward simulations).

The second question (representation) is more open-ended.
We probably want to stay within the general realm of LTS.
Then, there's a question of
whether we allow internal actions ($\tau$),
how $\lightning$, $\Delta$ are represented,
and how inputs are handled.
Each combination of choices yields
its own definition of simulation for
expressing the CompCert preorder described above
in that particular setting.

The critical point here is:
The choices made in CompCert
are first and foremost oriented towards
making it easy to define languages.
They make the notion of simulation
very complicated (backward simulations),
but this can be avoided in most cases
through the use of stronger forms of simulations
as proof methods
(forward simulations and
lemmas for using stronger but simpler simulation diagrams).
Simulations are only ever used as a hypothesis
for proving trace containement
and vertical composition,
which gets slightly hairy but is manageable
and a one-off thing.

In our case,
we care a lot about monotonicity properties in particular,
because in addition to the compiler correctness proof,
we want to do a fair amount of meta-theory.
This is why we're looking to embed everything
into a setting where simulations are simple.

%}}}

\subsection{Internal activity} %{{{

Here there are basically two choices.

\paragraph{$\tau$ transitions}

Internal activity is recorded as $\tau$ transitions,
but the preorder does not distinguish
between any two finite sequences in $\tau^*$.

The advantage is that it makes it easier to define
transition systems where not every step
records an externally observable event.
On the other hand,
the notion of simulation becomes more complex
because we need to be able to skip over
$\tau$ transitions on both sides.

This being said, simple simulations,
which consider $\tau$ as an observable event
and operate in a lockstep fashion,
are a special case and can be used as a proof method.
It might also be the case that:
\begin{itemize}
  \item monotonicity wrt. simple simulations
    entails monotonicity wrt. the more complex simulations;
  \item we can always factor a complex simulation into
    one ``$\tau$-shuffling'' simulation and
    one simple simulation.
\end{itemize}
\emph{This would be very useful to know.}

\paragraph{No transitions}

There is no explicit internal activity.
The advantage is that
the associated notion of simulation is simple.
The inconvenient is that
the LTS must be defined so that
internal activity is bunched up into one transition
together with some externally observable event,
which may not yield a very natural/tractable definition
if it involves some internal ``big-stepping''.

%}}}

\subsection{Divergence} %{{{

A related but distinct question is
how we represent silently diverging behavior.

There is no such thing as an infinite amount of time,
so that divergence can never be observed directly:
for a process given as a black box,
there is no experiment that can determine for sure
whether that process is silently divergent.
The best we can do is run the black box for a specific, finite amount of time
and observe that nothing has happened so far.
We can denote such an observation by $\tau^n$,
where $n$ is the number of execution steps steps we are willing to wait for.

Unfortunately,
such observations are not invariant under
the addition or compression of internal actions ($\tau$)---%
which happens between the source and target programs
of CompCert and
which we usually seek to abstract away
when we write specifications---%
whereby any finite sequences of $\tau$ transitions
may be replaced by any other such finite sequence.

Divergence ($\Delta$) is the property that we can observe $\tau^n$
for any $n \in \mathbb{N}$,
and is invariant under such substitutions,
since replacing one finite sequence of $\tau$s by another
will simply replace some $\tau^n$ with some $\tau^m$,
equally observable in the original.
We can also regard divergence as a pseudo-observation
representing the limit of
the decreasing sequence of observations $(\tau^n)_n$,
thereby abstracting away from any specific $n$.

However this introduces some complexity.
As long as we were only considering the actually observable,
finite prefixes of behaviors,
we could interpret the behavior $\tau^\omega$
as the set of observations $\tau^*$.
Now we need to understand $\tau^\omega$ as $\tau^* \cup \{\Delta\}$,
which complicates aspects of the theory.
Choosing a different representation of $\Delta$ is tricky because,
while divergence is preserved
when we substitute \emph{finite} sequences of $\tau$s for one another,
some combinators like horizontal composition
can produce diverging behavior out of reactive parts,
and if divergence is not represented implicitely by $\tau^\omega$
we have to handle this explicitely in some way.

Ultimately, [I think Galois connexions
between different representations of divergence---%
so that operators can be defined in a natural way,
and understanding what makes operators well-behaved in that context---%
so that our proofs can carry through the mappings,
is how to approach this.]
Below I outline some representations of $\Delta$
we've been thinking about.

\paragraph{Infinite sequence of $\tau$ transitions}

The most direct way is to use an infinite sequence of $\tau$ transitions,
since $\tau^\omega$ is essentially where divergence comes from.
Obviously this can only work if the model we're working with
permits $\tau$ transitions.

The advantage of this is that it is very natural and straightforward.
When defining operators we can just pass along $\tau$ transitions from
the LTS being combined,
maybe introduce our own $\tau$ transitions,
but we never have to handle divergence explicitely
in the definition,
including divergence introduced by the operator itself.

The major drawback is that we have to use \emph{measured simulations},
with the well-founded preorder and all that,
and that's very painful to use as a hypothesis
when proving monotonicity and similar properties.

\paragraph{Explicit $\Delta$ event}

Another approach is to represent divergence
with an explicit $\Delta$ event (I've also called it $\Uparrow$ before).
The associated definition of simulation is very straightforward,
since we want divergence to be on par with other behaviors.

This is possible with or without $\tau$ transitions.
In a context where we have \emph{both} $\Delta$ events and $\tau$ transitions,
it would make it possible for $\tau^\omega$ to denote $\tau^*$
(any \emph{finite} number of $\tau$'s)
but not divergence.

\paragraph{Represented as $\varnothing$}

Finally representing divergence as $\varnothing$ (no transitions)
would be possible, though probably ill-advised in the general case.

One reason to do that is
to make some constructions easier to define,
for example the idea of defining composition
by projecting out the observable components of an interaction
(as is done in many typical game semantics papers):
in the case of an infinite mutual recursion with no
externally observable events,
we would just get nothing.

However there are some drawbacks.
The simulation would be slightly more complicated:
we would need to add a condition that
if the implementation has no transition,
then the specification also must have no transition.
Another issue is the loss of expressivity,
since it is no longer possible to give a specification
permitting both divergence and another behavior.
Finally this would be incompatible with
representing $\lightning$ as $\varnothing$.

%}}}

\subsection{Unsafe behavior} %{{{

Unsafe behavior
corresponds to deadlock ($\delta$) in the context of process algebra,
or undefined behavior ($\lightning$) in the context of CompCert.
Since anything correctly implements undefined behavior,
we want it to be a greatest element in our preorder.

\paragraph{Explicit $\lightning$ event}

In this approach we use an explicit representation of
unsafe behavior as its own event.
The corresponding definition of simulation needs to single out $\lightning$
so that whenever the specification hits it,
the simulation otherwise holds unconditionally.
With this representation,
I expect it will be relatively easy
for operators to preserve $\lightning$ where appropriate,
however it makes the definition of $\sqcap$
more complicated because we need to make sure that
$\lightning \sqcap x = x$ for all specifications $x$.

\paragraph{Representation as $\top$}

Since the distinguishing feature of $\lightning$ is that
it can be implemented by any possible behavior,
we could just \emph{represent it as}
the specification which permits any possible behavior.
Adding a $\top$ state to LTS that transitions to itself
with all possible outputs is straightforward enough.
This also makes the definition of $\sqcap$ easy,
and means that the simulation doesn't have to handle $\lightning$
in any special way.

One downside is that we lose the distinction between
the ``all possible \emph{safe} behaviors'' specification,
vs. ``undefined behavior'',
although this can be remedied by keeping a $\lightning$ event
that is only ever used for $\top$.
Another is that in some contexts,
non-increasing operators 
may have some trouble preserving $\top$;
for instance, consider what happens if you
inline external calls in $\top$.

\paragraph{Representation as $\varnothing$}

This is what happens in CompCert,
where it makes sense when defining small-step semantics:
if there is no rule for it,
then it is undefined behavior.
Also it fits in well with forward simulations,
since $\varnothing$ naturally becomes $\top$ in that context.

However, for regular (backward) simulations,
this means unsafe behavior has to be singled out:
in CompCert this adds both the $\mathsf{safe}(s)$
guards everywhere in the definition of backward simulations
(similar to the condition dicussed above for $\lightning$ events),
but also the progress condition,
which ensures that the implementation behavior
can only be unsafe whenever the specification is.

Another issue is that
two safe but distinct behaviors no longer have a meet,
so that we cannot define $\sqcap$.
In other words there is no ``all behaviors prohibited'' specification.
This may not seem like a specification we would want to write,
but it can be useful to foreclose some execution paths \emph{in restrospect},
especially depending on the result of an interaction with the environment.
And I suspect having a formal lattice structure
can be useful in many other ways.

%}}}

\subsection{Normal termination} %{{{

There is no such thing in our model,
since final states are merely a way to emit
a specific kind of event,
and any terminating we do is temporary,
simply waiting for the execution to be resumed
using the provided continuation.

%}}}

\subsection{System vs. environment actions} %{{{

The question of how to handle system vs. envronment actions is twofold:
how do we encode the distinction between them, and
how do we take it into account to define our simulations.

\subsubsection{Encoding} %{{{

\paragraph{With a separate polarity assignment}

This is what I have been doing so far.
We have a single type for events,
and define a separate polarity assignment function
which tells us whether a given event
is a system or environment action.

In theory this can gives us the flexibility to shift between
different ``views'' --- active sets of components to be regarded as ``the system''.
However in my model I still have distinct events
for outgoing and incoming calls,
because this is how my language interfaces are defined.
They could conceivably be collapsed
when we do the embedding of CompCert semantics,
with the understanding that we would have an explicit domain
for each component
which would determine what calls are incoming vs. outgoing.

\paragraph{As a $\mathsf{match\_traces}$ relation}

A variation on that theme is to
distinguish implicitely between inputs and outputs
by introducing a relation similar to CompCert's
$\mathsf{match\_traces}$.
It would relate any two environment actions,
but would relate system actions only to themselves.
The idea is that from a given state,
any concurrently possible transitions must be related
(output determinism),
and all related events must have possible transitions
(receptiveness).

This approach also makes it possible to define
composite events that have both a system and environment component,
as is done in CompCert.
It is most appropriate for the
``receptiveness as side-condition''
approach (see below).

\paragraph{As two different types}

This is what happens at the level of my language interfaces,
where queries and replies are given
as two different types.
The advantage is that we don't have to carry around
an extra polarity assignment.
This also gives the possibility to define
a type of
inherently receptive transition systems (see below).

%}}}

\subsubsection{Simulation relations} %{{{

\paragraph{Alternating simulation}

The most general and symmetric way to
handle the difference between system and environment actions
is to use an alternating simulation.
In that approach,
the system actions of the implementation must be \emph{permitted}
by corresponding system actions of the specification,
whereas environment actions of the implementation are \emph{required}
by corresponding environment actions in the specification.

The downside of this general approach is
the added complexity of simulations,
as well as the added difficulty of defining
monotonic operators
through rules that include
both system and environment actions
of their arguments.

\paragraph{Receptiveness as side-condition}

One way to eliminate this problem is to limit our attention to
transitions systems that always have all possible input transitions.
The loss of expressivity is not too big,
because we can always replace a misssing input $x$
by an $x$-labeled transition to an unsafe state
(this behavior is immediately below the original one in the preorder).
We could define a closure operator to that effect
(well, technically an opening operator I suppose).

In a context where transition systems are
both deterministic and receptive,
then the direction of the environment component of the simulation
does not matter anymore,
since there will be exactly one of each environment transition
in both the source and target,
and we only need to show that they will preserve
the simulation relation.
Hence, we can fall back on a simple simulation
that does not have a complicated polarity inversion.
This is morally the approach taken in CompCert.

One downside is that it still requires us to prove and carry around
receptiveness theorems,
and either show that our operators preserve receptiveness,
or close them in some way that will make their definition more complex.

\paragraph{Receptiveness by construction}

An alternative would be to make receptiveness intrisic
in our definition of transition systems.
A labeled transition system over
a type of environment actions $I$, and
a type of system actions $O$,
would be defined by giving a set of states $S$ together with:
\begin{itemize}
\item a system transition relation $\alpha^O \subseteq S \times O \times S$;
\item an environment transition function $\alpha^I : S \times I \rightarrow S$.
\end{itemize}
A simulation relation between $\alpha_1$ and $\alpha_2$
would be an $R \subseteq S_1 \times S_2$ such that:
\begin{itemize}
\item $\forall (s_1, s_2) \in R,
  \forall o \, s_1', (s_1, o, s_1') \in \alpha_1^O \rightarrow
  \exists s_2', (s_2, o, s_2') \in \alpha_2^O \wedge (s_1', s_2') \in R$;
\item $\forall s_1 s_2 i, (s_1, s_2) \in R \rightarrow
  (\alpha^I_1(s_1, i), \alpha^I_2(s_2, i)) \in R$.
\end{itemize}

%}}}

%}}}

\subsection{Omniscient vs. boundary views} %{{{

I have been careful to reserve the term \emph{output} (resp. \emph{input})
for the subset of system actions (resp. environment actions)
which cross the boundary between the system and the environment.

\paragraph{Boundary log}

In the ``boundary log'' paradigm,
outputs are the only system actions, and
inputs are the only environment actions,
so that these concepts coincide.
This enforces locality by construction:
the behavior of the system
cannot depend on non-local environment actions,
and the environment
cannot observe internal details of the system.

One issue with that approach is that
composition hides internal events,
hence introduces potential divergence,
which may be problematic depending on the
framework in which it is formulated.
In particular,
in traditional game semantics
divergence usually operates as $\bot$,
so this is easy to deal with,
but in our context it is a behavior on par with others.

\paragraph{Global log}

In the ``global log'' paradigm,
all events are part of the system behavior's description,
and whether they are considered as system or environment actions
depends on the view we take,
or an active set of components we're looking at.

One complication with global logs is that
in the absence of extra information,
we need to keep track of the execution stack whenever
we interpret a log,
so as to match returns with their corresponding function calls.

% means that actually everything is observable to everyone as far as the
% formalism is concerned but hey.

\paragraph{Mixed approaches}

These two paradigms exist on a spectrum and
it is possible to combine some apects of them.
The traditional game semantics approach
is to describe the behavior of components in terms of boundary logs,
but composition is defined by taking a global log of sorts
of the three games involved,
then hiding the interaction ``middle game''
to recover a boundary log for the composite system.

Another possibility is to have a ``structurally global'' log
where internal system events appear, but are hidden as $\tau$ moves.
Dually we can represent
non-local environment action as $\bar{\tau}$ moves.

%}}}

%}}}

\end{document}
