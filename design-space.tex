\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{tikz-cd}

\hyphenation{CompCert}

\title{Components and behaviors in Composable CompCert}

\begin{document}

\maketitle

%\section{What's our objective?} %{{{
%
%There are several things we could shoot for
%that could be called ``composable compilation''.
%All of them would be an improvement on
%the state of the art in terms of
%having a compiler that helps us build certified systems
%in a compositional way,
%given that:
%\begin{itemize}
%  \item Compositional Compcert \cite{compcompcert}
%    does not actually define the composite artefact
%    that the proof is about;
%  \item CompCertX can only be used in some ad-hoc ways
%    in the context of CertiKOS;
%  \item Separate CompCert \cite{sepcompcert}
%    \emph{does} actually give a proof about
%    the composite assembly program,
%    but has no semantic expression of composition.%
%    \footnote{%
%      In that respect,
%      maybe we can see the Separate CompCert theorem
%      as analoguous to the Concurrent CertiKOS thread linking theorem,
%      if we consider the active thread set to be part of the program syntax.
%    }
%\end{itemize}
%
%%}}}

\section{Component syntax} %{{{

I will try to use the loaded terms \emph{program} and \emph{module} carefully,
and use the more general term \emph{component}
to mean the kind of syntactic objects subject to horizontal composition.

\paragraph{CompCert programs} %{{{

The obvious approach is to follow Separate CompCert and
use CompCert programs (list of definitions) as our components.
Syntactic composition, as defined in recent versions of CompCert,
is the disjoint union of programs (taken as sets of definitions),
where internal and external definitions of the same global
are collapsed into a single internal definition
if they are compatible.

%}}}

\paragraph{Explicit modules} %{{{

Another approach is to define a component as a set of modules,
with each module a CompCert program (list of definitions).
This way we can define a semantics where
internal, cross-module communication is observable
(perhaps as $\tau$ transitions).
This simplifies horizontal composition
from the fact that no new silent moves
would be introduced or hidden,
but does not address linking.

However we could introduce another syntactic operation
$L : \mathsf{component} \rightarrow \mathsf{module}$
representing linking (in the sense of \textsf{ld}),
which would collapse back a multi-module component
into a single module.
Linking would look much like syntactic composition in the previous approach,
but may be simpler because
some aspects of it may have been decoupled into
the $L$ operator.

Another advantage of decoupling linking,
is that it would be legitimate
to limit linking to assembly,
which is significantly simpler
than in the general case
given the absence of an explicit stack.

%}}}

\paragraph{Multi-level approach} %{{{

Take components as sets of CompCert LTS,
which could be computed from CompCert programs-modules,
or given as specifications.
Then we define the semantics of such
``semantic programs'' by taking the (semantic)
horizontal composition of the modules.
This side-steps the need for the syntactic composition theorem;
but this is essentially Compositional CompCert.

%}}}

%}}}

\section{Semantic model} %{{{

There are two questions here:
\begin{enumerate}
\item What behaviors does our model express/distinguish?
\item How are they represented?
\end{enumerate}

\subsection{Notion of behavior} %{{{

As far as the first question goes,
we probably want to keep in line with what CompCert does,
so that:
\begin{itemize}
\item Only externally observable differences count (trace equivalence);
\item Finite amounts of internal activity ($\tau$) are unobservable;
  however,
\item Divergence ($\Delta$) is observable,
  and incomparable with other behaviors;
\item Unsafe behavior ($\lightning$) is taken as $\top$;
\item Input and outputs are distinguished.
\end{itemize}
It would be conceivable to weaken this
(for instance to identify $\Delta$ with $\lightning$,
as is done in CertiKOS).
On the other hand we cannot strengthen it
(for instance by making $\tau$ observable)
since we wouldn't be able to embed
the CompCert correctness proof in a stronger setting.
We could, however,
use stronger preorders as proof methods
for the weaker version
(as with forward simulations).

The second question (representation) is more open-ended.
We probably want to stay within the general realm of LTS.
Then, there's a question of
whether we allow internal actions ($\tau$),
how $\lightning$, $\Delta$ are represented,
and how inputs are handled.
Each combination of choices yields
its own definition of simulation for
expressing the CompCert preorder described above
in that particular setting.

The critical point here is:
The choices made in CompCert
are first and foremost oriented towards
making it easy to define languages.
They make the notion of simulation
very complicated (backward simulations),
but this can be avoided in most cases
through the use of stronger forms of simulations
as proof methods
(forward simulations and
lemmas for using stronger but simpler simulation diagrams).
Simulations are only ever used as a hypothesis
for proving trace containement
and vertical composition,
which gets slightly hairy but is manageable
and a one-off thing.

In our case,
we care a lot about monotonicity properties in particular,
because in addition to the compiler correctness proof,
we want to do a fair amount of meta-theory.
This is why we're looking to embed everything
into a setting where simulations are simple.

%}}}

\subsection{Internal activity} %{{{

Here there are basically two choices.

\paragraph{$\tau$ transitions}

Internal activity is recorded as $\tau$ transitions,
but the preorder does not distinguish
between any two finite sequences in $\tau^*$.

The advantage is that it makes it easier to define
transition systems where not every step
records an externally observable event.
On the other hand,
the notion of simulation becomes more complex
because we need to be able to skip over
$\tau$ transitions on both sides.

This being said, simple simulations,
which consider $\tau$ as an observable event
and operate in a lockstep fashion,
are a special case and can be used as a proof method.
It might also be the case that:
\begin{itemize}
  \item monotonicity wrt. simple simulations
    entails monotonicity wrt. the more complex simulations;
  \item we can always factor a complex simulation into
    one ``$\tau$-shuffling'' simulation and
    one simple simulation.
\end{itemize}
\emph{This would be very useful to know.}

\paragraph{No transitions}

There is no explicit internal activity.
The advantage is that
the associated notion of simulation is simple.
The inconvenient is that
the LTS must be defined so that
internal activity is bunched up into one transition
together with some externally observable event,
which may not yield a very natural/tractable definition
if it involves some internal ``big-stepping''.

%}}}

\subsection{Divergence} %{{{

A question related to but distinct from
the status of silent transitions is
how we represent silently diverging behavior.

There is no such thing as an infinite amount of time,
so that divergence can never be observed directly:
for a process given as a black box,
there is no experiment that can establish
whether that process is silently divergent.
The best we can do is run the black box for a specific, finite amount of time
and observe that nothing has happened so far.
We can denote such an observation by $\tau^n$,
where $n$ is the number of execution steps we are willing to wait for.

Unfortunately,
such observations are not invariant under
the addition or removal of internal actions ($\tau$),
whereby a finite sequence of $\tau$ transitions
may be replaced by any other such finite sequence.
These additions and removals
happen between the source and target programs
in CompCert, and
we usually seek to abstract them away
when we write specifications.

By contrast,
divergence ($\Delta$) is the property that we can observe $\tau^n$
for any $n \in \mathbb{N}$,
and is invariant under such substitutions.
We can also regard divergence as a pseudo-observation
representing the limit of
the decreasing sequence of observations $(\tau^n)_n$,
thereby abstracting away from any specific $n$.

However this introduces some complexity.
As long as we were only considering the actually observable,
finite prefixes of behaviors,
we could interpret the behavior $\tau^\omega$
as the set of observations $\tau^*$.
Now we need to understand $\tau^\omega$ as $\tau^* \cup \{\Delta\}$,
which complicates aspects of the theory.
Choosing a different representation of $\Delta$ is tricky because,
while divergence is preserved
when we substitute \emph{finite} sequences of $\tau$s for one another,
some combinators like horizontal composition
can produce diverging behavior out of reactive parts,
and if divergence is not represented implicitely by $\tau^\omega$
we have to handle this explicitely in some way.

Below I outline some representations of $\Delta$
we have been thinking about.

\paragraph{Infinite sequence of $\tau$ transitions}

The most direct way is to use an infinite sequence of $\tau$ transitions,
since $\tau^\omega$ is essentially where divergence comes from.
Obviously this can only work if the model we're working with
permits $\tau$ transitions.

The advantage of this is that it is very natural and straightforward.
When defining operators we can just pass along $\tau$ transitions from
the LTS being combined,
maybe introduce our own $\tau$ transitions,
but we never have to handle divergence explicitely
in the definition,
including divergence introduced by the operator itself.

The main inconvenient is that we have to use \emph{measured simulations},
with the well-founded preorder and all that,
and that's very painful to use as a hypothesis
when proving monotonicity and similar properties.

\paragraph{Explicit $\Delta$ event}

Another approach is to represent divergence
with an explicit $\Delta$ event (I've also called it $\Uparrow$ before).
Then simulations don't need any specific handling of divergence,
since we want it to be on par with other behaviors.

This is possible with or without $\tau$ transitions.
In a context where we have \emph{both} $\Delta$ events and $\tau$ transitions,
it would make it possible for $\tau^\omega$ to denote $\tau^*$
(any \emph{finite} number of $\tau$'s)
but not divergence.

\paragraph{Represented as $\varnothing$}

Representing divergence as $\varnothing$ (no transitions)
would be possible, though probably ill-advised in the general case.

One reason to do that is
to make some constructions easier to define,
for example the idea of defining composition
by projecting out the observable components of an interaction
(as is done in many typical game semantics papers):
in the case of an infinite mutual recursion with no
externally observable events,
we would just get nothing.

However there are some drawbacks.
The simulation would be slightly more complicated:
we would need to add a condition that
if the implementation has no transition,
then the specification also must have no transition.
Another issue is the loss of expressivity,
since it is no longer possible to give a specification
permitting both divergence and another behavior.
Finally this would be incompatible with
representing $\lightning$ as $\varnothing$.

\paragraph{Heartbeat events}

In practice,
processes are usually run under some scheduling discipline,
so that even a silently divergent process will be
in an ongoing dialogue with its environment.
We can model this by inserting \emph{heartbeat events} $h$
in the behavior under consideration,
representing the points in time where
the process would be interrupted by
or yield back control to its environment.
The conjecture is that because this representation would
unify silent divergence and reactive behavior,
operators would not have to handle divergence
in a particular way and
would become simpler to define and/or reason about.

We expect the schedule to be fair,
so that the process eventually has an opportunity
to make progress,
but can only ever run for a finite amount of time.
Under that assumption,
writing $E$ for the set of non-heartbeat events,
infinite behaviors would be in $x (h^+ E^+)^\omega$,
and divergence in particular would be modeled as $x (h^+ \tau^+)^\omega$,
where $x$ is any finite prefix.
A behavior containing
an infinite sequence where no heartbeat event occurs ($E^\omega$)
would represent only the set of finite prefixes
of that sequence as possible observations,
but would never denote divergence.

Heartbeat events would influence the preorder in the following way.
Since the schedule used to insert heartbeat events is arbitrary,
a simple simulation between two such behaviors would be incomplete,
distinguishing behaviors that differ only in their schedule.
To remedy this,
we will need our semantic objects to
include all possible schedules.
This can be accomplished using sets of behaviors
as our semantic objects,
or by using non-determinism.

Another aspect of how heartbeat event fit within the preorder
is whether they should count as input or output moves.
Intuitively, categorizing $h$ as an input
would represent the environment interrupting the execution,
as a preemptive schduler would,
whereas categorizing $h$ as an output
would represent the system voluntarily yielding control
back to the scheduler,
as in cooperative threading schemes.
In practice,
this decision would influence how two behaviors
which encode different sets of schedules
would be related to one another in the preorder,
and also whether a behavior encoding multiple schedules
may still qualify as deterministic
in the sense of output determinism.

In fact,
we may eventually want could exploit the added expressivity
and use heartbeat events to mark points
where a process may be interrupted,
as opposed to atomic segments of the execution.
In that context,
\emph{input} heartbeat events
would allow a more interruptible implementation
to refine a less interruptible specification
(the specification \emph{mandates} points where
the process must accept an interruption gracefully),
whereas \emph{output} heartbeat events
would allow an implementation with fewer potential yield points
to refine a specification with more potential yield points
(the specification \emph{permits} points where
the process may yield control to the environment or scheduler).

Another concern is the handling of heartbeat events
in operators where diverging behavior may emerge
from combining individual non-diverging components.
The ``heartbeat version'' of this phenomenon
is that an operator may introduce
infinite sequences of non-heartbeat events
when combining behavior
without such sequences.
If such sequences represent
internal actions that would eventually be abstracted as $\tau$,
the abstracted version should admit a divergence observation, however
if the sequence contains no heartbeat events this will not be the case
(it will be interpreted as $\tau^*$ not $\tau^* \cup \{\Delta\}$).
This means heartbeat events would once again
have to be treated specially by such operators,
or the schedule made explicit.
This could be avoided
if the segments being intertwined
are expected to include at least one possible heartbeat.

%}}}

\subsection{Unsafe behavior} %{{{

Unsafe behavior
corresponds to deadlock ($\delta$) in the context of process algebra,
or undefined behavior ($\lightning$) in the context of CompCert.
Since anything correctly implements undefined behavior,
we want it to be a greatest element in our preorder.

\paragraph{Explicit $\lightning$ event}

In this approach we use an explicit representation of
unsafe behavior as its own event.
The corresponding definition of simulation needs to single out $\lightning$
so that whenever the specification hits it,
the simulation otherwise holds unconditionally.
With this representation,
I expect it will be relatively easy
for operators to preserve $\lightning$ where appropriate,
however it makes the definition of $\sqcap$
more complicated because we need to make sure that
$\lightning \sqcap x = x$ for all specifications $x$.

\paragraph{Representation as $\top$}

Since the distinguishing feature of $\lightning$ is that
it can be implemented by any possible behavior,
we could just \emph{represent it as}
the specification which permits any possible behavior.
Adding a $\top$ state to LTS that transitions to itself
with all possible outputs is straightforward enough.
This also makes the definition of $\sqcap$ easy,
and means that the simulation doesn't have to handle $\lightning$
in any special way.

One downside is that we lose the distinction between
the ``all possible \emph{safe} behaviors'' specification,
vs. ``undefined behavior'',
although this can be remedied by keeping a $\lightning$ event
that is only ever used for $\top$.
Another is that in some contexts,
non-increasing operators 
may have some trouble preserving $\top$;
for instance, consider what happens if you
inline external calls in $\top$.

\paragraph{Representation as $\varnothing$}

This is what happens in CompCert,
where it makes sense when defining small-step semantics:
if there is no rule for it,
then it is undefined behavior.
Also it fits in well with forward simulations,
since $\varnothing$ naturally becomes $\top$ in that context.

However, for regular (backward) simulations,
this means unsafe behavior has to be singled out:
in CompCert this adds both the $\mathsf{safe}(s)$
guards everywhere in the definition of backward simulations
(similar to the condition dicussed above for $\lightning$ events),
but also the progress condition,
which ensures that the implementation behavior
can only be unsafe whenever the specification is.

Another issue is that
two safe but distinct behaviors no longer have a meet,
so that we cannot define $\sqcap$.
In other words there is no ``all behaviors prohibited'' specification.
This may not seem like a specification we would want to write,
but it can be useful to foreclose some execution paths \emph{in restrospect},
especially depending on the result of an interaction with the environment.
And I suspect having a formal lattice structure
can be useful in many other ways.

%}}}

\subsection{Normal termination} %{{{

There is no such thing in our model,
since final states are merely a way to emit
a specific kind of event,
and any terminating we do is temporary,
simply waiting for the execution to be resumed
using the provided continuation.

%}}}

\subsection{System vs. environment actions} %{{{

The question of how to handle system vs. envronment actions is twofold:
how do we encode the distinction between them, and
how do we take it into account to define our simulations.

\subsubsection{Encoding} %{{{

\paragraph{With a separate polarity assignment}

This is what I have been doing so far.
We have a single type for events,
and define a separate polarity assignment function
which tells us whether a given event
is a system or environment action.

In theory this can gives us the flexibility to shift between
different ``views'' --- active sets of components to be regarded as ``the system''.
However in my model I still have distinct events
for outgoing and incoming calls,
because this is how my language interfaces are defined.
They could conceivably be collapsed
when we do the embedding of CompCert semantics,
with the understanding that we would have an explicit domain
for each component
which would determine what calls are incoming vs. outgoing.

\paragraph{As a $\mathsf{match\_traces}$ relation}

A variation on that theme is to
distinguish implicitely between inputs and outputs
by introducing a relation similar to CompCert's
$\mathsf{match\_traces}$.
It would relate any two environment actions,
but would relate system actions only to themselves.
The idea is that from a given state,
any concurrently possible transitions must be related
(output determinism),
and all related events must have possible transitions
(receptiveness).

This approach also makes it possible to define
composite events that have both a system and environment component,
as is done in CompCert.
It is most appropriate for the
``receptiveness as side-condition''
approach (see below).

\paragraph{As two different types}

This is what happens at the level of my language interfaces,
where queries and replies are given
as two different types.
The advantage is that we don't have to carry around
an extra polarity assignment.
This also gives the possibility to define
a type of
inherently receptive transition systems (see below).

%}}}

\subsubsection{Simulation relations} %{{{

\paragraph{Alternating simulation}

The most general and symmetric way to
handle the difference between system and environment actions
is to use an alternating simulation.
In that approach,
the system actions of the implementation must be \emph{permitted}
by corresponding system actions of the specification,
whereas environment actions of the implementation are \emph{required}
by corresponding environment actions in the specification.

The downside of this general approach is
the added complexity of simulations,
as well as the added difficulty of defining
monotonic operators
through rules that include
both system and environment actions
of their arguments.

\paragraph{Receptiveness as side-condition}

One way to eliminate this problem is to limit our attention to
transitions systems that always have all possible input transitions.
The loss of expressivity is not too big,
because we can always replace a misssing input $x$
by an $x$-labeled transition to an unsafe state
(this behavior is immediately below the original one in the preorder).
We could define a closure operator to that effect
(well, technically an opening operator I suppose).

In a context where transition systems are
both deterministic and receptive,
then the direction of the environment component of the simulation
does not matter anymore,
since there will be exactly one of each environment transition
in both the source and target,
and we only need to show that they will preserve
the simulation relation.
Hence, we can fall back on a simple simulation
that does not have a complicated polarity inversion.
This is morally the approach taken in CompCert.

One downside is that it still requires us to prove and carry around
receptiveness theorems,
and either show that our operators preserve receptiveness,
or close them in some way that will make their definition more complex.

\paragraph{Receptiveness by construction}

An alternative would be to make receptiveness intrisic
in our definition of transition systems.
A labeled transition system over
a type of environment actions $I$, and
a type of system actions $O$,
would be defined by giving a set of states $S$ together with:
\begin{itemize}
\item a system transition relation $\alpha^O \subseteq S \times O \times S$;
\item an environment transition function $\alpha^I : S \times I \rightarrow S$.
\end{itemize}
A simulation relation between $\alpha_1$ and $\alpha_2$
would be an $R \subseteq S_1 \times S_2$ such that:
\begin{itemize}
\item $\forall (s_1, s_2) \in R,
  \forall o \, s_1', (s_1, o, s_1') \in \alpha_1^O \rightarrow
  \exists s_2', (s_2, o, s_2') \in \alpha_2^O \wedge (s_1', s_2') \in R$;
\item $\forall s_1 s_2 i, (s_1, s_2) \in R \rightarrow
  (\alpha^I_1(s_1, i), \alpha^I_2(s_2, i)) \in R$.
\end{itemize}

%}}}

%}}}

\subsection{Omniscient vs. boundary views} %{{{

I have been careful to reserve the term \emph{output} (resp. \emph{input})
for the subset of system actions (resp. environment actions)
which cross the boundary between the system and the environment.

\paragraph{Boundary log}

In the ``boundary log'' paradigm,
outputs are the only system actions, and
inputs are the only environment actions,
so that these concepts coincide.
This enforces locality by construction:
the behavior of the system
cannot depend on non-local environment actions,
and the environment
cannot observe internal details of the system.

One issue with that approach is that
composition hides internal events,
hence introduces potential divergence,
which may be problematic depending on the
framework in which it is formulated.
In particular,
in traditional game semantics
divergence usually operates as $\bot$,
so this is easy to deal with,
but in our context it is a behavior on par with others.

\paragraph{Global log}

In the ``global log'' paradigm,
all events are part of the system behavior's description,
and whether they are considered as system or environment actions
depends on the view we take,
or an active set of components we're looking at.

One complication with global logs is that
in the absence of extra information,
we need to keep track of the execution stack whenever
we interpret a log,
so as to match returns with their corresponding function calls.

% means that actually everything is observable to everyone as far as the
% formalism is concerned but hey.

\paragraph{Mixed approaches}

These two paradigms exist on a spectrum and
it is possible to combine some apects of them.
The traditional game semantics approach
is to describe the behavior of components in terms of boundary logs,
but composition is defined by taking a global log of sorts
of the three games involved,
then hiding the interaction ``middle game''
to recover a boundary log for the composite system.

Another possibility is to have a ``structurally global'' log
where internal system events appear, but are hidden as $\tau$ moves.
Dually we can represent
non-local environment action as $\bar{\tau}$ moves.

%}}}

%}}}

\section{Embeddings} %{{{

In this section I will assume that we use
a single model of transition systems $\mathbb{D}$ and
a single set of actions $\Sigma$,
but use different preorders to
realize combinations of the possible choices
described in the previous section.

\subsection{Preorders} %{{{

I will write the simulation preorders I consider as $\le^w_z$,
where the superscript $w \subseteq \{\lightning, \varnothing\}$ will indicate
the encoding of ``going wrong''
(that is, which traces count as a $\top$ element),
and the subscript $z \subseteq \{*, \omega, \varnothing\}$ will indicate
how $\tau$ transitions ought to be handled.
For a notion of simulation $\le^w_z$,
I will call $\mathbb{D}^w_z$
the semantic domain equipped with that notion of preorder.

\paragraph{Simulations vs. trace containment} %{{{

When considering transition systems
we can distinguish simulation preorders
from the more general preorders based on sets of traces.
The distinction is in how they handle internal branching,
whereby there may be several transitions for the same observable action.
Consider for example
the Y-shaped transition system $a(b + c)$ vs.
the V-shaped $ab + ac$:
they generate the same set of traces but are not similar.
I will try to reserve the word \emph{refinement}
and the notation $\sqsubseteq$
for the more general trace-based preorders.

When the transition systems are non-branching,
the two notions coincide;
therefore, if we work under that assumption
we can gloss over the difference.
We can also remove internal branching by using the usual
powerset construction traditionally used for determinizing automata.
I will write this operation $\mathsf{flatten}$,
so that $\sqsubseteq$ may be related to $\le$ in the following way:
\[ x \sqsubseteq^w_z y \quad \Leftrightarrow \quad
  \mathsf{flatten}(x) \le^w_z \mathsf{flatten}(y) \,. \]
The $\mathsf{flatten}$ operator is monotonic, so that
${\le}^w_z \subseteq {\sqsubseteq}^w_z$.

Note that silent $\tau$ transitions complicate the matter
because they may introduce indirect branching,
as in $a + \tau a$.
For transition systems that are both non-branching and output-deterministic
this is not an issue,
but we will want to use non-determinism in our specifications.

%}}}

\paragraph{Simple simulations} %{{{

The preorder $\le$
corresponds to simple simulations.
There is no special interpretation for any event
(including $\tau$, $\Delta$, $h$, $\lightning$),
and we just need to show that any (output) action of the implementation
(the left-hand side LTS)
has a corresponding action in the specification
(the right-hand side LTS).

Simple simulations serve as a baseline,
and we will be able to embed more complex simulations
into simple simulations
by replacing complex observations by single events
($\tau^*e$ becomes $e$, $\tau^\omega$ becomes $\Delta$),
or by expanding $\lightning$ observations to $\Sigma^*$
(the $\top$ behavior in the context of $\le$).

%}}}

\paragraph{Going wrong} %{{{

When $\lightning \in w$,
we will alter the simulation slightly
so that any behavior that can start with the $\lightning$ action
will be considered as ``going wrong'':
as an additional alternative to other criteria,
the simulation will hold whenever
the specification can take a $\lightning$ transition.

Similarly, when $\varnothing \in w$,
then ``getting stuck'' (having no transition)
will be interpreted as going wrong.
In CompCert this corresponds to the $\mathsf{safe}$
guards in the definition of backward simulations,
as well as the progress condition---%
which asserts that if the implementation is able to take any step,
then the specification must be able to take a step as well.

%}}}

\paragraph{Silent $\tau$'s} %{{{

When $* \in z$,
we will consider $\tau$ actions
unobservable by the preorder
(with the possible exception of $\tau^\omega$, see below).
The corresponding simulation criterion will be that
any $\tau^* a$ sequence of moves in the implementation
will have a corresponding $\tau^* a$ sequence in the specification,
where the number of $\tau$s may differ between the two sequences.

By default,
this means that silent divergence will not be
distinguishable from $\varnothing$,
however we can independently add a
silent divergence observation by using $\omega \in z$.

%}}}

\paragraph{Divergence} %{{{

When $\omega \in z$,
then infinite sequences of $\tau$'s
are observable in their own right,
so that if the implementation admits such a sequence,
then the specification must admit one as well.

This is independent of $* \in z$,
so that $\le_{*\omega}$
implements the CompCert approach of
unobservable $\tau$'s except for divergence,
whereas $\le_{\omega}$
regards $\tau$'s as regular events
except for infinite sequences.
As an illustration of the differences consider the following:
\[
  \begin{array}{c@{\qquad}c@{\qquad}c@{\qquad}c}
    a                     \le                   a \tau^\omega &
    a                     \nleq                 \tau \tau a &
    a \tau^\omega         \nleq                 a &
    \tau a \tau^\omega    \nleq                 a \tau^\omega
    \\
    a                     \le_*                 a \tau^\omega &
    a                     \le_*                 \tau \tau a &
    a \tau^\omega         \le_*                 a &
    \tau a \tau^\omega    \le_*                 a \tau^\omega
    \\
    a                     \le_\omega            a \tau^\omega &
    a                     \nleq_\omega          \tau \tau a &
    a \tau^\omega         \nleq_\omega          a &
    \tau a \tau^\omega    \nleq_\omega          a \tau^\omega
    \\
    a                     \le_{*\omega}         a \tau^\omega &
    a                     \le_{*\omega}         \tau \tau a &
    a \tau^\omega         \le_{*\omega}         a &
    \tau a \tau^\omega    \le_{*\omega}         a \tau^\omega
  \end{array}
\]

Note that since $\omega$ merely adds an additional observation,
it can only be \emph{more} discriminating than
the corresponding notion of simulation without $\omega$,
so that ${\le^w_{z \omega}} \subseteq {\le^w_z}$.

%}}}

\paragraph{CompCert} %{{{

Glossing over some technical details,
CompCert simulations can be understood in our framework
in the following way:
\emph{forward simulations}
are a proof method for $\le^\varnothing_{*\omega}$;
\emph{backward simulations}
are a proof method for $\ge^\varnothing_{*\omega}$.

The mapping is not perfect because
CompCert uses \emph{lists} of events as transition labels,
and there are additional properties
required of CompCert transition systems
for the simulations to make sense
in terms of trace semantics.

%}}}

%}}}

\subsection{Galois connections} %{{{

There is not always a one-to-one correspondance between
the possible semantic domains.
For instance,
we may want to define an operator ${\Downarrow}_\omega : {\le_\omega} \rightarrow {\le}$
which detects $\tau^\omega$ in the source domain
and encodes it in the target domain
by inserting an explicit $\Delta$ event where $\tau^\omega$ appears,
and removing any pre-existing $\Delta$ event where $\tau^\omega$ is absent,
so that the target simulations no longer need to take
divergence into account specifically.
However,
this is not a perfect embedding
since pre-existing $\Delta$ events
will be collapsed with
detected occurences of $\tau^\omega$.
Likewise,
a reverse transformation which would expand $\Delta$ events into $\tau^\omega$
would collapse
occurences of $\tau^\omega$ in the right-hand side domain
(which would denote $\tau^*$ rather than divergence)
with any expanded $\Delta$ events.

However,
we can use the more general theory of (monotone) \emph{Galois connections}
to account for the relationship between these two domains,
and to investigate different order-preserving mappings between them
and their relationships.
I will write:
\[
    F \prescript{w}{z}\dashv^{w'}_{z'} G
\]
whenever
$F : {\le^w_z} \rightarrow {\le^{w'}_{z'}}$ and
$G : {\le^{w'}_{z'}} \rightarrow {\le^w_z}$
are two monotonic functions such that:
\[
    F(x) \le^{w'}_{z'} y \Leftrightarrow x \le^w_z G(y)
\]
for all $x, y \in \mathbb{D}$.
This can be visualized as follows:
\[
  \begin{tikzcd}[row sep=tiny, column sep=tiny]
    G(y) &  & y \arrow[ll, "G"'] \\
    & \Longleftrightarrow & \\
    x \arrow[uu, dashed, "\le^w_z"] \arrow[rr, "F"'] & &
    F(x) \arrow[uu, dashed, "\le^{w'}_{z'}"']
  \end{tikzcd}
\]
$F$ is called the \emph{lower adjoint}, and
$G$ is called the \emph{upper adjoint}.
They are mutual quasi-inverses ($FGF = F$, $GFG = G$),
and they are uniquely specified by each other.

Because adjoints are uniquely determined by each other,
given a semantic operator
we may want to investigate whether it participates
in Galois connections as a lower adjoint, as an upper adjoint, or both.
Such operators are
lower adjoints ($F$) if and only if they preserve all suprema, and
upper adjoints ($G$) if and only if they preserve all infima.
For instance, since $\Downarrow_\omega$ preserves all infima,
it is the upper adjoint of some Galois connection.
We will want to understand the corresponding lower adjoint,
which is given by:
\[
    {\Uparrow_\omega}(y) =
      \inf \{ x \in \mathbb{D} \:|\: y \le {\Downarrow_\omega}(x) \}
\]
In this case,
${\Uparrow}_\omega(y)$ inserts an infinite sequence of $\tau$'s
wherever a $\Delta$ event appears in $y$.
On the other hand,
$\Downarrow_\omega$ does not preserve suprema:
$\bigsqcup_n \tau^n = \tau^*$ but
$\bigsqcup_n {\Downarrow}_\omega(\tau^n) \neq {\Downarrow}_\omega(\tau^*)$.
Therefore it has no upper adjoint.

Every Galois connection gives rise to a closure operator $GF$,
and an interior operator $FG$.
In the case of $\Downarrow_\omega$,
the closure operator
${\Updownarrow}_\omega^- =
 {\Uparrow}_\omega {\Downarrow_\omega} :
 \mathbb{D} \rightarrow \mathbb{D}$
expands all $\Delta$ events by adding
$\tau^\omega$ wherever they appear,
and the interior operator
${\Updownarrow}_\omega^\circ =
 {\Downarrow}_\omega {\Uparrow}_\omega :
 \mathbb{D}_\omega \rightarrow \mathbb{D}_\omega$
removes all spurious $\Delta$ events
appearing where $\tau^\omega$ does not occur.

%}}}

\subsection{Internal activity} %{{{





%}}}

\end{document}
