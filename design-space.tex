\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}

\begin{document}

\section{What's our objective?} %{{{

There are several things we could shoot for
that could be called ``composable compilation''.
All of them would be an improvement on
the state of the art in terms of
having a compiler that helps us build certified systems
in a compositional way,
given that:
\begin{itemize}
  \item Compositional Compcert \cite{compcompcert}
    does not actually define the composite artefact
    that the proof is about;
  \item CompCertX can only be used in some ad-hoc ways
    in the context of CertiKOS;
  \item Separate CompCert \cite{sepcompcert}
    \emph{does} actually give a proof about
    the composite assembly program,
    but has no semantic expression of composition.%
    \footnote{%
      In that respect,
      maybe we can see the SepCompCert theorem
      as analoguous to the Concurrent CertiKOS thread linking theorem,
      if we consider the active thread set to be part of the program syntax.
    }
\end{itemize}

%}}}

\section{Component syntax} %{{{

I will avoid the loaded terms \emph{program} and \emph{module}
and use the term \emph{component}
to mean the kind of objects subject to horizontal composition.

\paragraph{CompCert programs} %{{{

The obvious approach is to follow SepCompCert and
use CompCert programs (list of definitions) as our components.
Syntactic composition, as defined in the latest CompCerts,
is the disjoint union of programs (taken as sets of definitions),
with external + internal functions
collapsing into the internal function definition.

%}}}

\paragraph{Explicit modules} %{{{

Another approach is to define a component as a set of modules,
with each module a CompCert program (list of definitions).
This way we can define a semantics where
internal, cross-module communication is observable
(perhaps as $\tau$ transitions).
This could simplify horizontal composition,
and linking the CompCert small-step version
to the more abstract version,
from the fact that no new silent moves
would be introduced.

%}}}

\paragraph{Multi-level approach} %{{{

Take components as sets of CompCert LTS,
which could be computer from CompCert programs as modules,
or given as specifications.
Then we define the semantics of such
``semantic programs'' by taking the (semantic)
horizontal composition of the modules.
This side-steps the need for the syntactic composition theorem;
but this is essentially Compositional CompCert.

%}}}

%}}}

\section{Semantic model} %{{{

There are two questions here:
\begin{enumerate}
\item What behaviors does our model express/distinguish?
\item How are they represented?
\end{enumerate}

\subsection{Notion of behavior} %{{{

As far as the first question goes,
we probably want to keep in line with what CompCert does,
so that:
\begin{itemize}
\item Only externally obsevable differences should be discriminated for:
  we want something like trace equivalence / similarity,
  rather than more structural / fine-grained equivalences like bisimulation;
\item Finite amounts of internal activity are unobservable;
\item However the semantics is divergence-sensitive,
  with divergence ($\Delta$) incomparable with other behaviors;
\item Unsafe behavior ($\delta$) is taken as $\top$;
\item Input and outputs are distinguished.
\end{itemize}
It would be conceivable to weaken this
(for instance to identify $\Delta$ with $\delta$,
as is done in CertiKOS).
On the other hand we cannot strenghten it
(for instance by making $\tau$ observable)
since we wouldn't be able to embed
the CompCert correctness proof in a stronger setting.
We could, however,
use stronger preorders as proof methods
for the weaker version
(as with forward simulations).

The second question (representation) is more open-ended.
We probably want to stay within the general realm of LTS.
Then, there's a question of
whether we allow internal actions ($\tau$),
how $\delta$, $\Delta$ are represented,
and how inputs are handled.
Each combination of choices yields
its own definition of simulation for
expressing the CompCert preorder described above
in that particular setting.

The critical point here is:
The choices are made in CompCert
are first and foremost oriented towards
making it easy to define languages.
They make the notion of simulation used
super-complicated (backward simulations),
but this can be avoided in most cases
through the use of stronger forms of simulations
as proof methods
(forward simulations and
lemmas for using stronger but simpler simulation diagrams).
Simulations are only ever used as a hypothesis
for proving trace containement
and composition theorems,
which gets slightly hairy but is manageable
and a one-off thing.

In our case,
we care a lot about monotonicity properties in particular,
because in addition to giving the one compiler correctness proof,
we want to do a fair amount of meta-theory.
This is why we're looking to embed everything
into a setting where simulations are simple.

%}}}

\subsection{Internal activity} %{{{

Here there are basically two choices.

\paragraph{$\tau$ transitions}

Internal activity is recorded as $\tau$ transitions,
but the preorder does not distinguish
between any two finite sequences in $\tau^*$.

The advantage is that it makes it easier to define
transition systems where not every step
records an externally observable event.
On the other hand,
the notion of simulation becomes more complex
because we need to be able to skip over
$\tau$ transitions on both sides.

This being said simple simulations,
which consider $\tau$ as an observable event
and operate in a lockstep fashion,
are a special case and can be used as a proof method.
It might also be the case that:
\begin{itemize}
  \item monotonicity wrt. simple simulations
    entails monotonicity wrt. the more complex simulations;
  \item we can always factor a complex simulation into
    one ``$\tau$-shuffling'' simulation and
    one simple simulation.
\end{itemize}
\emph{This would be very useful to know.}

\paragraph{No transitions}

There is no explicit internal activity.

The advantage is that
the associated notion of simulation is simple.

The inconvenient is that
the LTS must be defined so that
internal activity is bunched up into one transition
together with some externally observable event,
which may not yield a very natural/tractable definition
if it involves some internal ``big-stepping''.

%}}}

\subsection{Divergence} %{{{

This is a related but distinct question:
how do we represent a diverging behavior.

\paragraph{Infinite sequence of $\tau$ transitions}

The most direct way is to use an infinite sequence of $\tau$ transitions,
since $\tau^\omega$ is essentially where divergence comes from.
Obviously this can only work if there are $\tau$ transitions.

The advantage of this is that it is very natural and straightforward.
When defining operators we can just pass along $\tau$ transitions from
the LTS being combined,
maybe introduce our own $\tau$ transitions,
but we never have to worry about divergence
in the definition.

The major drawback is that we have to use \emph{measured simulations},
with the well-founded preorder and all that,
and that's very painful to use as a hypothesis
when proving monotonicity and the like.

\paragraph{Explicit $\Delta$ event}

Another approach is to represent divergence
with an explicit $\Delta$ event (I've also called it $\Uparrow$ before).
This yields a very natural definition of simulation,
since we want divergence to be on par with other behaviors.

This is possible with or without $\tau$ transitions.
In a context where we \emph{so} have $\tau$ transitions,
it would make it possible for $\tau^\omega$ to denote $\tau^*$
--- any \emph{finite} number of $\tau$'s ---
but not divergence.

\paragraph{Represented as $\varnothing$}

Finally representing divergence as $\varnothing$ (no transitions)
would be possible, though probably ill-advised in the general case.

One reason we would want to that is
that it would make some constructions easier to define,
for example the idea of defining composition
by projecting out the observable components of an interaction
(as is done in many typical game semantics papers):
in the case of an infinite mutual recursion with no
externally observable events,
we would just get nothing.

However there are some drawbacks.
The simulation would be slightly more complicated:
we would need to add a condition that
if the implementation has no transition,
then the specification also must have no transition.
Another issue is that we lose some expressivity,
since it is no longer possible to express a specification
permitting both divergence and another behavior,
though one could argue this is not needed in practice.
Finally this would be incompatible with
representing $\delta$ as $\varnothing$.

%}}}

\subsection{Unsafe behavior} %{{{

Unsafe behavior
corresponds to deadlock ($\delta$) in the context of process algebra,
or undefined behavior ($\lightning$) in the context of CompCert.
Since anything correctly implements undefined behavior,
we want it to be a greatest element in our preorder.

\paragraph{Explicit $\delta$ event}

In this approach we use an explicit representation of
unsafe behavior as its own event.
The simulation needs to single out $\delta$
so that whenever the specification hits it,
the simulation otherwise holds unconditionally.
With this representation,
I expect it will be relatively easy
for operators to preserve $\delta$ where appropriate,
however it makes the definition of $\sqcap$
more complicated because we need to make sure that
$\delta \sqcap x = x$ for all specifications $x$.

\paragraph{Representation as $\top$}

Since the distinguishing feature of $\delta$ is that
it can be implemented by any possible behavior,
we could just \emph{represent it as} any possible behavior.
Adding a $\top$ state to LTS that transitions to itself
with all possible outputs is straightforward enough.
This also makes the definition of $\sqcap$ easy,
and means that the simulation doesn't have to handle $\delta$
in any special way.

One downside is that we lose the distinction between
the ``all possible \emph{safe} behaviors'' specification,
vs. ``undefined behavior'',
although this can be remedied by keeping a $\delta$ event
that is only ever used for $\top$.
Another is that in some contexts,
non-increasing operators 
may have some trouble preserving $\top$;
for instance, consider what happens if you
inline external calls in $\top$.

\paragraph{Representation as $\varnothing$}

This is what happens in CompCert,
where it makes sense when defining small-step semantics:
if there is no rule for it,
then it is undefined behavior.
Also it fits in well with forward simulations,
since $\varnothing$ naturally becomes $\top$ in that context.

However, for regular (backward) simulations,
this means unsafe behavior has to be singled out:
in CompCert this adds both the $\mathsf{safe}(s)$
guards everywhere in the definition of backward simulations
(similar to the condition dicussed above for $\delta$ events),
but also the progress condition,
which has to make sure that the implementation behavior
can only be unsafe whenever the specification is.

Another issue is that
two safe but distinct behaviors no longer have a meet,
so that we cannot define $\sqcap$.

%}}}

\subsection{Normal termination} %{{{

There is no such thing in our model,
since final states are merely a way to emit
a specific kind of event,
and any terminating is temporary
until the execution is resumed
using the provided continuation.

%}}}

\subsection{System vs. environment actions} %{{{

According to \cite{cspgs},
distinguishing between system vs. environment actions
is the essence of game semantics
and a key ingredient for compositionality.

The question of how to handle system vs. envronment actions is twofold:
how do we encode the distinction between them, and
how do we take it into account to define our simulations.

\subsubsection{Encoding} %{{{

\paragraph{With a separate polarity assignment}

This is what I have been doing so far.
We have a single type for events,
and define a separate polarity assignment function
which tells us whether a given event
is a system or environment action.

In theory this can gives us the flexibility to shift between
different ``views'' or ``active sets''.
However note that in my model I still have distinct events
for outgoing and incoming calls,
because this is how my language interfaces are defined.
They could conceivably be collapsed
when we do the embedding,
with the understanding that we would have an explicit domain
for each component
which would determine what calls are incoming vs. outgoing.

\paragraph{As a $\mathsf{match\_traces}$ relation}

A variation on that theme is to
distinguish implicitely between inputs and outputs
by introducing a relation similar to CompCert's
$\mathsf{match\_traces}$.
It would relate any two environment actions,
but would relate system actions only to themselves.
The idea is that from a given state,
any concurrently possible transitions must be related
(output determinism),
and all related events must have possible transitions
(receptiveness).

This approach also makes it possible to define
composite events that have both a system and environment component,
as is done in CompCert.
It is most appropriate for the
``receptiveness as side-condition''
approach (see below).

\paragraph{As two different types}

This is what happens at the level of my language interfaces,
where queries and replies are given
as two different types.
The advantage is that we don't have to carry around
an extra assignment.
This also gives the possibility to define
inherently receptive transition systems (see below).

%}}}

\subsubsection{Simulation} %{{{

\paragraph{Alternating simulation}

The most general and symmetric way to
handle the difference between system and environment actions
is to use an alternating simulation.
In that approach,
the system actions of the implementation must be \emph{permitted}
by corresponding system actions of the specification,
whereas environment actions of the implementation are \emph{required}
by corresponding environment actions in the specification.

The downside of this general approach is
the added complexity of simulations,
as well as the added difficulty of defining
monotonic operators
through rules that include
both system and environment actions
of their arguments.

\paragraph{Receptiveness as side-condition}

One way to eliminate this problem is to limit our attention to
transitions systems that always have all possible input transitions.
The loss of expressivity is not too big,
because we can always replace a misssing input $x$
by an $x$-labeled transition to an unsafe state
(this behavior is immediately below the original one in the preorder).
We could define a closure operator to that effect
(well, technically an opening operator I suppose).

In a context where transition systems are
both deterministic and receptive,
then the direction of the environment component of the simulation
does not matter anymore,
since there will be exactly one of each environment transition
in both the source and target,
and we only need to show that they will preserve
the simulation relation.
Hence, we can fall back on a simple simulation
that does not have a complicated polarity inversion.

Morally this is the approach taken in CompCert.

\paragraph{Receptiveness by construction}

This is all good an well,
but it still requires us to prove and carry around
receptiveness theorems,
and either show that our operators preserve receptiveness,
or close them in some way that will make their definition more complex.

An alternative would be to make receptiveness intrisic
in our definition or transition systems.
Here a labeled transition system over
a type of environment actions $I$, and
a type of system actions $O$,
would be defined by giving a set of states $S$ together with:
\begin{itemize}
\item a system transition relation $\alpha^O \subseteq S \times O \times S$;
\item an environment transition function $\alpha^I : S \times I \rightarrow S$.
\end{itemize}
A simulation relation between $\alpha_1$ and $\alpha_2$
would be an $R \subseteq S_1 \times S_2$ such that:
\begin{itemize}
\item $\forall (s_1, s_2) \in R,
  \forall o \, s_1', (s_1, o, s_1') \in \alpha_1^O \rightarrow
  \exists s_2', (s_2, o, s_2') \in \alpha_2^O \wedge (s_1', s_2') \in R$;
\item $\forall s_1 s_2 i, (s_1, s_2) \in R \rightarrow
  (\alpha^I_1(s_1, i), \alpha^I_2(s_2, i)) \in R$.
\end{itemize}

%}}}

%}}}

\subsection{Omniscient vs. boundary views} %{{{

I reseve the term \emph{output} (resp. \emph{input})
for the subset of system actions (resp. environment actions)
which cross the boundary between the system and the environment.

\paragraph{Boundary log}

In the ``boundary log'' paradigm,
outputs are the only system actions, and
inputs are the only environment actions,
so that these concepts coincide.
This enforces by construction
the fact that the behavior of the system
cannot depend on non-local environment actions,
and the fact that the environment
cannot observe internal details of the system.

One issue with that approach is that
composition hides internal events,
hence introduces potential divergence,
which may be problematic depending on the
framework in which it is formulated.
In particular,
in traditional game semantics
divergence usually operates as $\bot$,
but in our context it is a behavior on par with others.

\paragraph{Global log}

In the ``global log'' paradigm,
all events are part of the system behavior's description,
and whether they are considered as system or environment actions
depends on the view we take,
or an active set of components we're looking at.

One complication with global logs is that
in the absence of extra information,
we need to keep track of the execution stack whenever
we interpret a log,
so as to match returns with their corresponding function calls.

% means that actually everything is observable to everyone as far as the
% formalism is concerned but hey.

\paragraph{Mixed approaches}

These two paradigms exist on a spectrum and
it is possible to combine some apects of them.
The traditional game semantics approach
is to describe the behavior of components in terms of boundary logs,
but composition is defined by taking a global log of sorts
of the three games involved,
then hiding the interaction ``middle game''
to recover a boundary log for the composite system.

Another possibility is to have a ``structurally global'' log
where internal system events appear, but are hidden as $\tau$ moves.
A dual approach can be used to have
non-local environment action appear as $\bar{\tau}$ moves.

%}}}

%}}}

\end{document}
