\chapter{Related Work}
\label{ch:related}

Finally, I will briefly discuss existing research work
relevant to the framework and goals described in this work.
I have already covered some of the foundational literature
in Section~\ref{sec:intro:litreview}.

\paragraph{Interaction Trees}

As a ``semantics toolbox'' of sorts,
interaction trees share some goals and techniques
with the model presented in this work.
In fact, an interaction tree $t : \kw{ITree}_E(X)$
can be interpreted into the framework as a strategy
$\langle t \rangle : E \rightarrow \{ {*} : X \}$.
However, strategies generalize ITrees in several ways:
\begin{itemize}
  \item Strategies are two-sided and encode incoming as well as outgoing interactions,
    forming the basis for layered composition.
  \item By design, ITrees must be executable programs,
    whereas strategies can be described logically using arbitrary Coq specifications.
  \item Strategies that exhibit the same external behavior are formally equal.
    By contrast, ITrees are compared using bisimulation equivalences.
    Equational reasoning requires Coq's setoid support,
    which can be slower and more fragile than rewriting with $\kw{eq}$.
  \item The strategies come with built-in notions of partial definition,
    refinement and data abstraction,
    whereas similar notions for ITrees
    have to be defined and tailored to a particular application.
\end{itemize}

\paragraph{Game Semantics}

The horizontal fragment of the framework is a particularly simple form of game semantics.
The framework's novelty resides in the vertical and spatial fragments,
for which, to my knowledge, there exists no precedent in the game semantics literature.
In particular, refinement conventions involve alternations of angelic and demonic choices;
it is surprising to find they can be modeled using a fairly standard approach,
although a rather unconventional ordering of plays must be used.
An interesting question for further research would be to investigate how far this can be pushed
and whether games more complex than effect signatures could admit their own forms of refinement conventions.

\paragraph{Refinement Calculus}

The refinement calculus \citep{refcal} was a source of inspiration for our framework.
One defining feature of the refinement calculus is \emph{dual nondeterminism},
which provides very powerful abstraction mechanisms.
At the same time, models like predicate transformers
do not deal with external interactions or state encapsulation.

\paragraph{CompCertO}

The semantic model of CompCertO~\citep{compcerto,compcerto-dr} introduced \emph{simulation conventions}
and the associated idea of a full-blown, two-dimensional refinement framework,
so it is worth pointing out the ways in which our framework generalizes the CompCertO model,
especially when it comes to refinement conventions:
\begin{itemize}
  \item CompCertO transition systems and simulation conventions use
    explicit states and Kripe worlds in their definitions,
    whereas strategies and refinement conventions provide canonical representations
    for the components' observable behaviors.
  \item
    Effect signatures are more general than the language interfaces used in CompCertO,
    which force all questions to use the same set of answers.
  \item
    CompCertO transition systems do not retain any history between successive incoming questions;
    as such, they cannot support the kind of state encapsulation which our framework enables.
    Likewise, simulation conventions only specify 4-way relationships
    between isolated pairs of questions and answers,
    but unlike refinement conventions they cannot be sensitive to the history of the computation.
\end{itemize}

\paragraph{Other CompCert-based Verification Frameworks}

CompCertM \citep{compcertm} is another project
which builds on CompCert
to provide a compositional verification framework.
Like CompCertO,
it introduces a better model of the interaction between
C and assembly programs
and more flexibility in simulation conventions.
However, while it permits some form of localized state,
CompCertM still does not support
full-blown data abstraction and state encapsulation
of the kind we have presented.
See \citep{compcerto,compcerto-dr}
for a detailed comparison between Compositional CompCert,
CompCertM and CompCertO.

We have also touched on
certified abstraction layers and CompCertX in \S\ref{sec:bg:cal}.
Subsequent work has extended CAL to support concurrency \cite{ccal}.
There are more recent treatments of CAL which,
like our work,
attempt to streamline the underlying theory
\citep{popl22,rbgs-cal},
%In particular a limited version of the construction ${-} \mathbin@ U$
%operating on a fixed set $U$ appears in \citet{rbgs-cal}.
but this work has not been mechanized
or interfaced with CompCert.

%Interaction trees \cite{itree,itrees} provide
%another framework for compositional semantics
%formalized in the Coq proof assistant
%which presents similarities with our own.
%though their interface with CompCert is also less comprehensive.

\paragraph{Separation Logic}

For the most part,
the frameworks discussed above
do not provide program-level verification facilities,
but rather focus on a more coarse-grained, module-level ``glue''.
Likewise,
we have assumed that elementary module correctness properties
such as $\phi_1$, $\phi_2$ and $\phi_\kw{bq}^\kw{min}$
were provided by the user%
\footnote{Our example is simple enough that,
  in our implementation,
  manual simulation proofs were
sufficient.}
and focused on the problem of
connecting such proofs.
Nevertheless,
program logics in general and separation logic in particular
are relevant to our work in the following ways.

First, it would be beneficial to incorporate
such program logics into our framework.
For example, \citet{popl15} provides
a rudimentary Clight program logic which
can be used to help prove abstraction layers correct.
It may be useful to investigate whether
the Clight separation logic provided by
the Verified Software Toolchain \citep{vst}
could be interfaced with our model.

Secondly,
spatial composition is in fact
the defining feature of separation logic.
Our treatment of memory separation
draws extensively from
separation algebra \citep{sepalg},
an approach to building models of separation logic.
More recently,
Conditional Contextual Refinement (CCR) \citep{ccr}
combined (vertical) refinement and (spatial) separation logic into
a unified, mechanized framework.
CCR however does not support state encapsulation
or certified compilation.

\paragraph{Multi-language Semantics}

We have demonstrated that our framework is able to reason across languages through non-trivial examples such as the one in Fig~\ref{fig:readwritehello}.
In Compositional CompCert and CompCertM,
assembly programs are given C-level semantics,
making it possible to directly reason about composite programs
(but only for Asm code, which behaves according to the C calling convention).
CAL uses the opposite approach and can translate
C-level layer specification into assembly behaviors.
Recent work on the DimSum framework \citep{dimsum}
attempts to give a more general account of
multi-language semantics
by introducing wrappers to translate between
different languages.

These various approaches all attempt
to represent \emph{horizontally} what
the simulation conventions of CompCertO represent vertically.
In our framework,
the notions of companion and conjoint
could provide a natural way to formalize
approaches of this kind,
so that, for example, the CompCertO calling convention
$\mathbb{C} : \mathcal{C} \leftrightarrow \mathcal{A}$
would be in companion/conjoint relationships with
adapter components
$\mathbb{C}_* : \mathcal{A} \rightarrow \mathcal{C}$
and
$\mathbb{C}^* : \mathcal{C} \rightarrow \mathcal{A}$.
The complexity of CompCertO's convention as presently stated
makes this challenging,
but we do not believe it to be a fundamental issue.

\paragraph{Event-based Semantics}
The DimSum framework~\citep{dimsum}
employs a language-agnostic, event-based semantics
as a generic framework
for multi-language semantics.
Both the DimSum framework
and our strategy model
feature rich compositional structures,
and support private states
across function invocations.
However, there are several key differences
set DimSum apart
from our approach.
First,
DimSum introduces
explicit angelic and demonic nondeterminism
alongside events.
These nondeterministic structures
facilitate the transformation and ordering
of event sequences
at different abstraction levels.
However,
this also adds complexity
due to the intricate
commuting properties between events
and nondeterministic choices.
In contrast,
our strategy model
adheres to a transitional approach
where plays solely consist of events.
Here, dual nondeterminism
is concealed within the construction of
refinement conventions and simulations,
activating only when necessary.
Second,
events in DimSum are not
well-bracketed,
allowing for modeling complex interactions
such as coroutines,
which are challenging to implement
within our current strategy model.
Generalization to
asynchronous games semantics
would be required to accommodate such behaviors.
Third,
the DimSum framework
does not support spacial composition.
Instead,
data abstraction must go through
the semantics wrapper,
which is a rather heavy mechanism.
Lastly,
the DimSum framework
features a four-pass compiler
that translates idealized source-
and target-level programs.
By contrast,
our strategy model
integrates a realistic optimizing compiler
that compiles C program into assembly.
