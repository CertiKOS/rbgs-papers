\section{Introduction}
\label{sec:intro}

Over the past decade, researchers have been able to formally verify
various key components of computer systems, including compilers
\cite{compcert,cakeml,vellvm}, OS kernels \cite{sel4,dscal15,certikos-osdi16},
file systems \cite{fscq} and processor designs \cite{safe,kami}.
Building on these successes, the research community is attempting to
construct large-scale, heterogeneous certified systems by using formal
specifications as interfaces between the correctness proofs of various
components \cite{deepspec}.  The ongoing design of suitable semantic
frameworks is an important step towards this goal.  However,
incorporating certified compilers into frameworks of this kind
presents a number of difficulties.

\subsection{Compositional Compiler Correctness}
\label{ssec:intro-ccc}

Compiler correctness is often formulated as a \emph{semantics
preservation} property, asserting that the semantics of the compiled
program $\kw{C}(p)$ are related in some particular way to the
semantics of the source program $p$:
\begin{equation} \label{eqn:semp}
  \llbracket p \rrbracket_\kw{S} \sim
  \llbracket \kw{C}(p) \rrbracket_\kw{T}
  \,.
\end{equation}
For whole-program compilers, semantics preservation is straightforward
enough.  In CompCert~\cite{compcert}, the semantics of the source and
target programs (denoted as $\llbracket \cdot \rrbracket_\kw{S}$
and $\llbracket \cdot \rrbracket_\kw{T}$)
are given as labeled transition systems, and the
relation $\sim$ is a simulation property.

However, practical applications involve program \emph{components}
which we want to compile and verify separately from each other.  In
principle, the use of a compositional semantics enables the
formulation of (\ref{eqn:semp}) at the level of individual components.
Unfortunately, traditional approaches to compositional semantics fare
poorly in the presence of advanced language features, or of the kind
of abstraction involved in the compilation process.  For CompCert,
early attempts along these lines have proven challenging
\cite{cpp15,compcompcert}.

As a result, common wisdom holds semantics preservation to be a lost
cause for compositional compiler correctness \cite{next700}.  Instead,
research has focused on compositional reasoning methods based on
contextual refinement, side-stepping the need for compositional
semantics preservation \cite{sepcompcert,compcertm}.

\subsection{Decomposing Heterogeneous Systems}
\label{ssec:intro-dhs}

Unfortunately, these methods share an intrinsic limitation: they
presuppose the existence of a completed system to be proven correct,
and compositionality only operates within its boundary.  This becomes
a serious impediment in the context of large-scale heterogeneous
systems.

\begin{example} \label{ex:nicdriver} 
Consider the problem of verifying a network interface card (NIC)
driver.  The NIC and its driver are closely coupled, but the details
of their interaction are irrelevant to the rest of the system; these
details should not leak into our large-scale reasoning.  Instead, we
wish to treat the NIC and its driver as a unit, and establish a direct
relationship between C calls into the driver and network
communication.  Together, the NIC and driver implement a specification
$\sigma : \li{Net} \rightarrow \mathcal{C}$, meaning they \emph{use}
the interface $\li{Net}$ modeling the network, and \emph{provide} the
interface $\mathcal{C}$ modeling C function calls.

The driver code could be specified ($\sigma_\kw{drv}$) and verified at
the level of CompCert semantics, whereas device I/O primitives
($\sigma_\kw{io}$) and the NIC itself ($\sigma_\kw{NIC}$) would be
specified as additional components in the context of a richer model:
\[
  \sigma_\kw{NIC} : \li{Net} \rightarrow \li{IO}
  \qquad
  \sigma_\kw{io} : \li{IO} \rightarrow \mathcal{C}
  \qquad
  \sigma_\kw{drv} : \mathcal{C} \rightarrow \mathcal{C}
\]
By reasoning about their interaction, it would be possible to
establish a relationship between the overall specification $\sigma :
\li{Net} \rightarrow \mathcal{C}$ and the composition $\sigma_\kw{drv}
\circ \sigma_\kw{io} \circ \sigma_\kw{NIC}$.  Then a \emph{compiler of
certified components} should help us transport specifications and
proofs obtained at the C level %with respect to the driver's C code to
the compiled code operating at the level of assembly ($\sigma' :
\li{Net} \rightarrow \mathcal{A}$).
\end{example}

Under existing contextual approaches, the NIC driver can only be
specified and verified in terms of its interactions at the boundary of
the C program code.  Since abstracting away from these interactions is
the role of a driver in the first place, this is a serious limitation.
To be sure, existing techniques could be extended to address this
specific problem.  For example, the NIC hardware model could be
brought within the scope of the ``whole program'' being considered,
and the exchange of Ethernet packets modeled as part of its observable
behavior.  However, this approach does not scale.

Example~\ref{ex:nicdriver} is by no means a contrived corner case. In
fact, patterns of this kind are pervasive even in more mundane
situations.  Programmers often use libraries which mediate access to
external resources (network services, file systems, user interfaces).
Proper high-level specifications for software components of this kind
must model these resources.  It would rapidly become burdensome to
expect the verification framework to fix in advance the dozens or
hundreds of kinds of resources which may be involved in the course of
verifying a large-scale system.

\subsection{Research Direction}

Fortunately, advances in compositional semantics offer a realistic
path to tackling problems of this kind.  In particular, game semantics
(\S\ref{sec:gamesem}) provides a general and expressive framework to
model interactions between typed components.
Yet work on compositional semantics in general,
and game semantics in particular,
has often been targeted towards developing
better a better theoretical understanding
of specific programming languages,
whereas practitioners of formal methods and verification
have relied on more concrete operational models.

Our ambition is to show that beyond theoretical interest,
game semantics
can serve as the foundation
of a powerful compositional verification framework,
making it possible to bring together strands of research
which to date have only been considered separately,
and to tackle the verification of large-scale heterogeneous systems.

In recent work, we proposed
integrating dual nondeterminism and refinement into game semantics,
extending it with powerful mechanisms to account for abstraction
\cite{koenig20,layered22}.
On a more practical level,
we have developed CompCertO \cite{compcerto},
a version of CompCert which
borrows ideas from game semantics and certified abstraction layers
to provide a correctness theorem for CompCert
stated in terms of a compositional semantics.

On a theoretical level,
this work points in the direction of a rich, two-dimensional algebra 
of systems and abstraction
which could give a unified account
of a broad range of verification approaches (\S2).
On a practical level,
it provides preliminary evidence
for the suitability of game semantics for the problem at hand.
However,
there remains many theoretical and practical challenges
in bringing this vision to life,
which the work outlined in this proposal seeks to address.

\subsection{Deficiencies of the CompCert Memory Model}
\label{ssec:intro-nmm}

[TODO: move some of this to Sec. 3;
replace this section with a broader discussion of
the limitations in existing work on CompCertO:
\begin{itemize}
  \item nominal memory model
  \item multi-threading / CCAL vs. CompCertO
  \item bridging the gap b/w high-level and transition system models,
    state encapsulation
  \item complexity of CompCertO's calling convention
  \item limitations of CompCertO's $\oplus$]
\end{itemize}

Memory models are critical components for formalizing the semantics of
imperative programming languages. They determine an abstraction over
memory and provide necessary operations for manipulating memory states
at the corresponding level of abstraction.

Among the memory models developed so far, the most representative one
is the \emph{block-based memory model}~\cite{leroy08,compcert-mem-v2}
defined in CompCert.  It provides a medium level of
abstraction---neither too abstract nor too concrete---by modeling
memory space as a collection of contiguous memory regions (also called
\emph{blocks}). Isolation between different memory blocks is simply
captured by giving each of those memory blocks a unique identifier
(called its \emph{block id}). Furthermore, pointers are naturally
defined as pairs of block ids and offsets to memory cells, and
definitions of pointer operations follow in a straightforward manner.
The block-based memory model has been highly successful. By uniformly
applying it to all of CompCert's languages, the developers of CompCert
were able to verify over 20 compiler passes containing advanced
optimizations.  It has also been widely adopted in other verification
projects, including various extensions of CompCert for supporting
compositionality and concurrency
(e.g.,~\cite{sevcik13,compcompcert,compcertm,cascompcert,wang2019,compcertelf20,sepcompcert,compcerts})
and formalization of language semantics for program verification
(e.g.,~\cite{appel11:vst,dscal15,ccal18}).

Despite its previous success, CompCert's block-based memory model is
still quite primitive and inflexible, making it difficult to support
end-to-end and compositional verified compilation: 

\begin{itemize}
\item 
\textbf{Inflexibility 1: Fixed Representation of Block IDs.}
%
In the block-based memory model, block ids are represented by a fixed
type, i.e., positive numbers. With this uniform and fixed
representation, it is impossible to distinguish between memory regions
playing different roles (such as the stack, heap, and global
data). Therefore, it is difficult to reason about specific parts of
memory in isolation.

\item
\textbf{Inflexibility 2: Sequential Numbering of Valid Blocks.}  
%
In any memory state, a special positive number named \nextblock
provides the next available block id. The global memory space is
divided by \nextblock into two parts: blocks with ids less than
\nextblock have already been allocated (called \emph{valid blocks}),
while the remaining blocks are waiting to be allocated (called
\emph{invalid blocks}). In any program semantics, the ids of valid
blocks must be numbered sequentially by $[1,\ldots,\nextblock-1]$.
This creates unintuitive and unnecessary dependency between valid
blocks playing different roles.

\item
\textbf{Inflexibility 3: Global Constraint for Allocation.}
%
In any memory state, there is only a single \nextblock for assigning
new block ids upon allocation. In the setting where multiple open
programs or concurrent threads work on the same memory state, every
open program or thread must keep track of how other programs or
threads modify \nextblock. This global constraint imposes a complex
dependency between open programs and threads.
\end{itemize}

Because of the complex dependencies created by the above
inflexibilities, verification of any compiler transformation must
treat the memory space \emph{as a whole}. This incurs significant
difficulties both in verified compilation of whole programs and in
that of open programs. Specifically, in the former setting, many
compiler transformations only work on certain sub-regions of memory
(\eg, transformations on stack frames). However, because of
\textbf{Inflexibilities 1 and 2}, the reasoning must be lifted to the
whole memory, therefore becoming significantly more involved and less
intuitive. In the latter setting, one would like to get a
compositional approach to verifying open or concurrent
programs. However, such an approach must be compatible with the
evolution of \nextblock which is inherently non-compositional by
\textbf{Inflexibilities 2 and 3}. This problem plagues many projects on
extending CompCert to support compositionality or concurrency (\eg,
CASCompCert~\cite{cascompcert} and Thread-Safe
CompCertX~\cite{ccal18}). To circumscribe it, various ad hoc
modifications to the block-based memory model were invented, leading to
verification results that are overly technical and less reusable.

\subsection{Intellectual Merit}
\label{ssec:intro-itm}

In addition to these deficiencies, the current CompCert is not end-to-end
in that it can only produce assembly code but not actual binary
machine code. There are still no certified linker and loader that can
work directly with ELF binaries.  CompCert also does not support secure
compilation: source program that is provably information-flow secure
can still be compiled into target code that contains information leaks.

In this effort, we propose to develop a novel verified compilation
toolchain that would address all of these shortcomings. Our
proposed research consists of the following five components:
\begin{itemize}
%%%%%%%%%%%%%%%
\item First, we will contribute new technologies for supporting {\em
  compositional verified compilation} by using novel game-semantic
  {\em language interfaces} to specify sophisticated {\em calling
    conventions}, and then using richer simulation conventions (based
  on logical relations) to support compositional proofs.

  [here shift to the PLDI submission --- extend CompCertO with encapsulated state?
  develop a clean mathematical framework?]
%%%%%%%%%%%%%%%
\item Second, we will develop a systematic, clean, and lightweight approach to
  eliminating the above inflexibilities of the block-based memory
  model. Our approach is based on \emph{nominal
  techniques}~\cite{pitts-nominal,gabby2002}. By generalizing block
  ids to {\em atomic names}, and valid blocks to {\em supports}, and by
  eliminating global constraints via {\em supports}, we will develop a
  novel {\em nominal memory model} that enables flexible memory
  structures for open and concurrent programs.

  [here focus on merging Nominal CompCert and CompCertO,
  maybe also PCM stuff]
%%%%%%%%%%%%%%%  
\item Third, we will develop an {\em end-to-end and compositional
  verified compiler} that can compile C components all the way into
  ELF object files. We will compile open programs and threads with
  nominal memory models into those with concrete machine-level memory
  layout. We will also build verified compositional assembler,
  linker, and loader that can work directly with ELF binaries.
%%%%%%%%%%%%%%%    
\item Fourth, basing upon our prior work on compositional semantics
  for certified abstraction layers~\cite{dscal15,ccal18,koenig20,layered22},
  we will support {\em verified security-preserving
  compilation}~\cite{costanzo16} by developing a general framework
  for the specification, abstraction, and refinement of heterogeneous
  components.
%%%%%%%%%%%%%%%  
\item Fifth, to demonstrate the power and real-world applicability of
  our new verified compiler, we will apply it to build more advanced
  certified concurrent OS kernels and hypervisors (e.g., CertiKOS~\cite{certikos-osdi16}
  and SeKVM~\cite{sekvm21a,sekvm21b,tao21}) and certified programming tools
    (e.g., DeepSEA~\cite{deepsea19} and VST~\cite{appel11:vst}).
\end{itemize}
