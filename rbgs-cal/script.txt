1.
Hello, I'm Jeremie,
I work on the certified kernel CertiKOS
and I'm going to present some of our research on
Refinement-Based Game Semantics.

2.
Certified software comes with a formal specification
and a mechanized proof of correctness.
The projects I've listed here
all use the Coq proof assistant
and there is interest in connecting them
as certified components
to create larger-scale certified systems.

However,
each one uses its own semantic models and
verification techniques,
so we need to identify a general-purpose model
which could embed all of them.
This model should support composition,
and help us bridge the gap between components
which operate at different abstraction levels.

3.
Our own work on CertiKOS illustrates
some of these concepts.
In our verification effort,
we divided the kernel into 37 layers,
which we specified and verfied individually.
Then we used their specifications as interfaces to connect them
and to derive a correctness property for the whole system.

Here I've shown the structure of a certified abstraction layer.
The underlay interface L_1 describes and specifies
the functionality provided by the layer below,
and the overlay interface L_2 describes
the functionality that we want to implement.
By verifying that the layer code M
correctly implements L_2 in terms of L_1,
we're able to derive the contextual refinement property
that I've shown on the right.
Because the overlay interface L_2 may be described
in terms more abstract terms than the underlay interface,
we also need to include a simulation relation R
which explains how the more abstract data structures in L_1
are realized in terms of the more concrete data structure in L_2.
--
So that's how certified abstraction layers work.
Our goal will be to expand this paradigm
to make it more flexible and
to cover a broader range of certified components.

4.
Fortunately,
there is a lot of interesting research that we can draw from
as we try to do that
For example, game semantics
can model the behavior of systems in very general and
compositional ways,
and the refinement calculus can express
a broad range of specifications and supports
stepwise refinement.
Yet if you look at a typical verification project,
you will find that they use fairly old-school technology.
There are a number of reasons for that.

5.
One is that some of these models,
in particular in game semantics,
are often fairly sophisticated
and that makes them hard to formalize in a proof assistant.
But more importantly,
for our purposes we need everything is a single framework,
and it's not clear how these different lines of research
could be combined to accomplish this.
For example,
in game semantics there hasn't been a lot of work on refinement.
On the other hand,
the refinement calculus only models
imperative programs and specifications.

6.
The goal of Refinement-Based Game Semantics
is to bridge this gap.
We synthesize ideas
from game semantics, the refinement calculus,
and other lines of work to build
general and compositional models.
These models support
stepwise refinement and data abstraction,
and we believe they are suitable for building
large-scale certfied systems.

7.
Before I explain in detail how our model works,
I will present the main ideas behind our approach.

8.
Traditionally,
programs and specifications are two very different things.
In a Hoare triple P, C, Q,
there is a strong distinction between the program C
and the pre- and post-conditions P and Q.
By contrast,
in refinement-based approaches,
programs and specifications will be treated as objects of the same kind.
We start from a specification S,
which can be very abstract and declarative,
and then incrementally refines it
until we obtain an executable program C_n.

In this context,
nondeterminism is a very useful tool
for expressing specifications.
There are two ways to interpret nondeterministic choice.
On one hand,
an angelic choice of specifications
means that the implementation must refine both of them.
It makes the specification stronger and
provides more guarantees to the user.
On the other hand,
demonic choice means that
the implementation can refine either one.
It weakens the specification
and gives us more implementation freedom.

9.
Some models offer both kinds of nondeterminism.
In the refinement calculus,
this takes the form of a
completely distributive lattice for the refinement ordering.
The properties of lattices mean that
the models is insensitive to angelic and demonic branching,
and distributivity means that angelic and demonic choices
also commute with each other.

Let's look at an example to see how this works.

9.
A function can be seen as a very simple system,
which performs one input followed by one output.
As a specification,
we can ask an input x to be mapped to an output y.

For instance,
a function f which doubles its argument
satisfies the specification that I've show here.

Using angelic nondeterminism,
we can strengthen this specification and
ask that our function both
  maps 0 to 0 and
  maps 1 to 2.

Going one step further,
with infinite choice,
we can ask all possible inputs x to be mapped to 2x,

and in fact this is how we encode
the function f itself as a specification.

On the other hand,
demonic nondeterminism works in the opposite direction.
For example this specification is very weak,
and only asks that our function map
one of the possible inputs x to its successor x+1.

We can narrow down the choice of x to either 0 or 1
to obtain a stronger specification,

We narrow it down further to retain only the right-hand side,
the right-hand side choice,
and since are function indeed maps 1 to 2,
it refines all of these specifications.

Finally,
we can use angelic and demonic choice together
to express more complicated constraints.
For example,
this specification expresses that f
maps all odd numbers to even numbers.
The bottom line is that on the left of the refinement relation,
angelic choice behaves similarly to the "for all" quantifier,
and demonic choice behaves like "there exists".

11.
This is gonna be very powerful when we talk about data abstraction.
For example,
consider the representation of integers
as pairs of natural number.
If you see the pair as the credit and debit columns of a bank account,
then the balance is the corresponding integer.

Now the question is,
what does it mean for a function g
defined on pairs of naturals to
implement a function f defined on integers.
We can express this relation
with the simulation diagram show here,
which says that related inputs are taken to related outputs,
but since this relation involves objects of different types,
it's not clear how it fits in our refinement framework.

With dual nondeterminism,
we can actually use R to translate the specification f
to obtain a corresponding specification on pairs of natural numbers.
We use angelic choice to decode input,
because the environment is free to choose the input representation.
On the other hand,
the output representation can be chosen by the system,
so we use demonic choice to encode the result of f.

This translation also has an upper adjoint
which works in the opposite direction
to translate g into a refinement of f,
like in abstract interpretation where we use Galois connexions
to handle data refinement.
--
Next I'm gonna talk about
refinement and nondeterminism
in the context of game semantics.

12.
In game semantics,
strategies are represented as
sets of plays (or traces),
and our elementary function specifications
can be seen a very simple form of plays.

Each move in a play is assigned to either
the environment, or the system.
In our case,
the function's input is a move of the environment
and the functions' output is a move of the system.

When we look at a set like this one,
we can use the polarity of moves to interpret
nondeterminism in the corresponding direction.
This set can be read as a specification which
requires 0 to be mapped to 0,
but allows 1 to be mapped to either 1 or minus 1.

The problem with that approach is that
the refinement ordering we get
is quite hard to describe formally,
and this gets much worse when we consider
complex plays which interleave
several moves of the system and the environment.
On the other hand,
if we revisit the way strategies are constructed,
and decouple nondeterminism from the polarity of moves,
we will get a model that is both simpler and more expressive.

13.
Our approach will be to think of plays as elementary specifications
If the environment behaves in the way described in the play,
then the system must also behave accordingly in response.

Using angelic nondeterminism,
we can range over all possible behaviors of the environment
to obtain a notion of strategy.

Then, adding demonic nondeterminism,
we introduce choices for the system as well,
and obtain a notion of strategy specification.
---
So rougly speaking,
strategy specifications will be sets of sets of plays,
but a nice way to describe this
is to use a monad to capture dual nondeterminism as an effect.

14.
Without going into much detail,
the FCD monad can extend any partially ordered set
with dual nondeterminism,

by contructing the free completely distributive lattice
generated by that poset.
Every element can be described as an angelic choice of demonic choices,
and sequential composition will accumulate the nondeterminism
in both computations.

15.
Now that I've described the main ideas behind our work,
I'm gonna talk about the game model that we constructed.

16.
Since we are only interested in modeling first-order computation,
we can use very simple game.
We will use signatures of the form that I have show here:
A signature is a set of questions,
and we assign to each question a set of possible answer.

As a running example,
we'll look at how a queue can be implemented
using an array.
A queue is simply a sequence of elements,
with two operations to enqueue an element at the end
and dequeue an element from the front.
Since the enq operation take an argument,
we will encode it in the signature as a
family of questions,
one for each possible argument value.
We will implement the queue using an array,
and two counters marking the beginning and end of the queue
within the array.
We have operations to access the array,
and operations which increment counters
and return their previous values.

17.
Given a signature,
we can define the corresponding set of plays.
Each play represents an execution of
a computation which
asks questions from the signature E
and gets answers from the environment.
A play will be sequence of questions and answers,
and the computation can also terminate
with an outcome in a given set A.
For clarity, we underline the moves of the system
and leave the moves of the environment as-is.

As an example,
I've shown one possible scenario
where an element is dequeued
from an array.
We increment the first counter
and find that its original value was 3.
We then access the array to query
the value at position 3 and return the answer.

18.
From this kind of play,
we can build a notion of interaction specification.
We will represent them using the
free completely distributive lattice
generated by our poset of plays.

Then to describe in more general terms
how an element can be dequeued from an array,
we can use angelic nondeterminism to range over
the possible answers we get from the environment.

19.
Interaction specifications are equipped with a monadic structure,
which allows us to decompose strategies sequentially.
If we have a computation x,
and specify for each possible outcome
how the computation should continue,
then we can define

a composite computation
which has the cumulated effects of both.

The unit is a simple computation which
immediately return a given value v,
and we can also define for each question in the signature
an elementary operation which asks that question and returns the answer.

By combining these,
we can describe our deq operation
in a more familiar way.

20.
So far we have described the client side of things.
But strategies in game semantics
are usually two-sided:
they play one game in the role of client,
and the other game in the opposite role.
Like certified abstraction layers,
they use one interface to provide another.
In our case,
we can provide the interface F
by specifying how to answer each possible question,
as an interaction specification which uses the interface E.
So a morphism from E to F
will first accept a question q in the signature F,
then ask a series of questions in E,
and finally produce an answer for the
original question q.

As an example,
I've shown a morphism which implements queue operations
by performing series of array operations.
The deq operation is as before,
and the enq operation works in a similar way.

21.
Morphisms add new dimension of composition to our model.
If we have a computation
which asks questions in a signature F,
and a morphism from E to F,
then we can combine them
so that every question of x
is translated by the morphism.
The result is a computation
asking question in E,
with produces the same outcome as x.

As an example,
if we have code for rotating a queue
which performs a deq operation and an enq operation,
we can use the morphism we have defined before
to translate these queue operations into
the corresponding array operations.
---
This basic substitution operation
can then be used to define
the composition of morphisms.

22.
So now we have almost everything we need
to describe certified abstraction layers.
However in CertiKOS,
our layer interfaces are specified using
sets of abstract states.
In this model,
we can add them by annotating
every question and answer
with the current state.
Then the queue and array can be specified
in the following way.

For the queue,
states are just sequences of values.
The specification does not perform any operation itself,
so it can be modelled as a morphism
from the empty signature.
The enqueue operation simply returns the unit value,
annotated with a state which has been updated
to include the new value.
The dequeue operation uses angelic nondeterminism
to decompose the queue into a head and tail.
The head is returned,
and then the tail becomes the new state of the queue.
--
If the queue is empty,
the result will be the bottom specification.
The implementation is free to do anything
because the client has breached their contract.
--
Now if we have some client code which uses queue operations,
like our rotation code from before,
we can extend it so that it maintains a current state at all times.
We get an initial state as input,
then every question is annotated with the current state.
When we get an answer, we use it to update the state,
and when the computation concludes,
we return the final state as well as its outcome.
This can then be composed with our queue specification
to obtain the behavior of the code in terms of queue states.

I won't describe the array operations in detail,
but we can use a similar approach to specify them.
And again,
we can compose our queue implementation M_q with the specification L_a
to obtain the behavior of the implementation
in terms of array states.

23.
The last step is data abstraction.
As before,
if we have layer interfaces which use different kinds of states,
we can define a simulation relation R between them.
This relation can then be embedded as adjoint morphisms,
which translate between the two kinds of states.

In the case of queues and arrays,
the relation will read out values stored in the array
between the two counters,
and compare them with the contents of the queue.
Applying this relation to our queue interface,
we will get a specification
for a queue implemented in terms of array states.
This specification uses angelic nondeterminism
to decode a queue from an array,
and demonic nondeterminism to choose a representation
for the new queue.

24.
And that's it!
Now we can use our model to describe certified abstraction layers.

For each layer interface,
we give a signature, a set of states,
and a morphism giving the specifications for its operations.

Then the layer implementation $M_q$

can be evaluated on top of the underlay $L_a$

and the simulation relation can be used to
translate states,

and to express the layer correctness property that we want.
--
This illustrates the expressivity of our model:
we're able to represents various kinds of components in a uniform way.
The model I have presented can also embed
a compositional semantics for the certified compiler CompCert and
interaction trees,
which are used for simliar purposes as the ones we've outlined.
And this doesn't have to be the end of the story.
In the paper, we sketch a model built on the same principles,
which allows state to be encapsulated and
retained from one environment question to the next.
And there's no reason why the principles that we have outlined
couldn't applied to more complex game models
which would support thinks like
high-order functions or concurrency.

26.
So wrapping up,
we've seen that game semantics and dual nondeterminism
go hand-in-hand in a beautiful way,
and that if we reinterpret traditional constructions in this light,
we can build very expressive models
that can be used for large-scale verification.
--
The model I've described built on these principles
introduces several innovations.
For the first time,
we combined a game semantics
with the full specification power of the refinement calculus.
We have shown that we can decouple nondeterminism
from the structure of plays,
and that our approach allows us to handle
the very different component of certified abstraction layers
in a uniform and algebraic way.

27.
So, I'll leave you with that.
--
I hope you enjoyed the talk, and
thank you for watching!

