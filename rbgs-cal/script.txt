1.
My name is Jeremie,
I work on the certified kernel CertiKOS
I'm going to present some of our research on
Refinement-Based Game Semantics.

2.
Certified software comes with a
mechanized proof of correctness.
The projects I've listed here
all use the Coq proof assistant
and there is interest in connecting them
as certified components
to create larger-scale certified systems.

However,
each one uses its own semantic models and
verification techniques,
so we need to identify a general-purpose model
which could embed all of them.
This model should support composition,
and help us bridge the gap between components
operating at different abstraction levels.

3.
Our own work on CertiKOS illustrates
some of these concepts.
In our verification effort,
we divided the kernel into 37 layers,
which we specified and verfied individually.
Using the specifications as interfaces,
we were then able to connect these proofs
and derive a correctness property for the whole system.

Here I've shown the structure of
a certified abstraction layer.
The underlay interface L_1 describes and specifies
the functionality provided by the layer below,
while the overlay interface L_2 describes
the functionality that the layer implements.
By verifying that the layer code M
correctly implements L_2 in terms of L_1,
we're able to derive the contextual refinement property
that I've shown on the right.
Because the overlay interface L_2 may be described
in terms of more abstract data structures,
we also need to include a simulation relation R
which explains how these high-level structures
are realized in terms of the lower-level structures
provided by L_1.

Our goal will be to expand this paradigm
to cover a broader range of certified components.

4.
Fortunately,
there is a lot of interesting research that we can draw from
to accomplish that task.
For example, game semantics
can model the behavior of systems in very general and
compositional ways,
and the refinement calculus can express
a broad range of specifications and supports
stepwise refinement.
Yet if you look at a typical verification project,
you will find that they use fairly old-school technology.
There are a number of reasons for that.

5.
One is that some of these models,
in particular in game semantics,
are often fairly sophisticated
and that makes them hard to formalize in a proof assistant.
But maybe more seriously,
for our purposes we need to combine
generality, refinement and abstraction
in a single framework,
and it's not clear how these different lines of research
could be combined to accomplish this.

6.
The goal of Refinement-Based Game Semantics
is to bridge this gap.
We're going to synthesize ideas
from game semantics, the refinement calculus,
and other lines of work to build
general and compositional models which support
stepwise refinement and data abstraction,
so that they're suitable for building
large-scale certfied systems.


8.
Traditionally,
programs and specifications are two very different things.
In a Hoare triple P, C, Q,
there is a strong distinction between the program C
and the pre- and post-conditions P and Q.
By contrast,
in refinement-based approaches,
we will treat programs and specifications in a uniform way.
We start from a specification S,
which can be very abstract and declarative,
and then incrementally refines it
until we obtain an executable program C_n.

In this context,
nondeterminism is a very useful tool
for expressing specifications.
There are two ways to interpret nondeterministic choice.
On one hand,
an angelic choice of specifications
means that the implementation must refine both of them.
It makes the specification stronger and
provides more guarantees to the user.
On the other hand,
demonic choice means that
the implementation can refine either one.
It weakens the specification
and gives us more implementation freedom.

9.
Some models offer both kinds of nondeterminism.
In the refinement calculus,
this takes the form of a completely distributive lattice
associated with the refinement ordering.
The properties of lattices mean that
the models is insensitive to either kind of branching,
and going further,
distributivity means that angelic and demonic choice
also commute with one another.

Let's look at an example to see how this works.

9.
A function can be seen as a very simple system,
which performs one input followed by one output.
As a very basic specification,
we can ask a given input x to be mapped to a given output y.

For instance,
the function f which double its argument
satisfies the specification that I've show here.

Using angelic nondeterminism,
we can strengthen this specification and
ask that our function both
  maps 0 to 0 and
  maps 1 to 2.

With infinitary choice operators,
we can go further and say that
all possible inputs x must be mapped to 2x,

and in fact this is how we encode
the function f itself as a specification.

On the other hand,
demonic nondeterminism works in the opposite direction.
For example this specification is very weak,
and only asks that our function maps
one possible input to its successor.

We can narrow down the choice of x to either 0 or 1
to obtain a stronger specification,

and if we narrow it down further to
the right-hand side choice,
we can check that our function f indeed
maps 1 to 2,
so that it refines all of these specifications.

Finally,
we can use angelic and demonic choice together
to express more complicated constraints.
For example,
this specification expresses that f
maps all odd numbers to even numbers.
The bottom line is that on the left of the refinement relation,
angelic choice behaves similarly to the "for all" quantifier,
and demonic choice behaves like "there exists".

11.
This is gonna be very powerful when we talk about data abstraction.
For example,
consider the representation of integers
as pairs of natural number.
If you see the pair as credit and debit columns of an account,
then the corresponding integer is the account's balance.

Now the question is,
what does it mean for a function g
defined on pairs of naturals to
implement a function f defined on integers.
We can express this relation
with the simulation diagram show here,
which says that related inputs are taken to
related outputs,
but since this relation involves objects with
two different types,
it's not clear how it fits in our refinement framework.

With dual nondeterminism,
we can actually use R to translate the specification f
to obtain a corresponding specification on pairs of naturals.
Because the user is free to choose the input representation,
we use angelic choice
to choose an integer corresponding to the input pair.
On the other hand,
the implementation is free to choose the output representation,
so we use demonic choice to choose a pair
related to the output of f.

This translation also has an upper adjoint
which works in the opposite direction
to translate g into a refinement of f,
and that reminiscent of the way
Galois connexions are used in abstract interpretation.

Next I'm gonna talk about
refinement and nondeterminism
in the context of game semantics.

12.
In game semantics,
strategies are represented as
sets of plays (or traces).
For example,
our elementary function specifications
are a very simple form of plays.

Each move in a play is assigned to either
the environment, or the system.
In our case,
the function's input is a move of the environment
and the functions' output is a move of the system.

When we look at a set like this one,
we can use the polarity of moves to interpret
nondeterminism in the corresponding direction.
This set can be read as a specification which
requires 0 to be mapped to 0,
but allows 1 to be mapped to either 1 or minus 1.

The problem with that approach is that
the refinement ordering we get
is quite hard to describe formally,
and this gets much worse when we consider
complex plays which interleave
several moves of the system and the environment.
On the other hand,
if we reinterpret the foundations of game semantics
and decouple nondeterminism from the polarity of moves,
we will get a model that is both simpler and more expressive.

13.
Our approach will be to think of plays as elementary specifications
If the environment behaves in the way described in the play,
then the system must also behave accordingly in response.

Using angelic nondeterminism,
we can range over all possible behaviors of the environment
to obtain a notion of strategy.

Then, adding demonic nondeterminism,
we introduce choices for the system as well,
and obtain a notion of strategy specification.

So rougly speaking,
strategy specifications will be sets of sets of plays,
but a nice way to describe this
is to use a monad representing dual nondeterminism as an effect.

14.


15.
Now that I've described the main ideas behind our work,
I'm gonna talk about the game model that we constructed.

16.
Since we are only interested in modeling first-order computation,
we can use a very simple notion of game.
We will use signatures of the form that I have show here.
A signature is a set of questions,
and we assign to each question a set of corresponding answer.

As a running example,
we'll look at how a queue can be implemented
using an array.
A queue is simply a sequence of elements,
with two operations to enqueue an element at the end
and dequeue an element from the front.
Since the enq operation take an argument,
we will encode it in the signature as a
family of questions,
one for each possible argument value.
We will implement the queue in terms of an array and two counters.
We have operations to access the array,
and two operations which increment
a counter and return its previous value.

17.
Given a signature,
we can then define the corresponding set of plays.
Each play represents an execution of
a computation which
asks questions from the signature E
and gets answers from the environment.
A play will be sequence of questions and answers,
and the computation can also terminate
with an outcome in a given set A.
For clarity,
we underline the moves of the system
and leave the moves of the environment as-is.

As an example,
I've shown one possible scenario
where an element is dequeued
from an array.
We increment the first counter
and find that its original value was 3.
We then access the array to query
the value at position 3
and return the answer we get.

18.
From this kind of play,
we can build a notion of interaction specification.
We will represent them using the
free completely distributive lattice
generated by our poset of plays.

Then to describe in more general terms
how an element can be dequeued from an array,
we can use angelic nondeterminism to range over
the possible answers we get from the environment.

19.
This model actually defines a monad,
and this allows us to decompose strategies
sequentially.
If we have a computation x,
and specify for each possible outcome
how the computation should continue,
then we can define

a composite computation
which has the cumulated effects of both.

The unit is a simple computation which
immediately return a given value v,
and we can also define for each question
and elementary operation which asks that question
and returns the answer.

By combining these,
we can describe our deq operation
in a more familiar way.

20.
So far we have described the user side of things.
But strategies in game semantics
are usually two-sided:
they play one game in the role of client,
and the other game in the opposite role.
Like certified abstraction layers,
they use one interface to provide another.
In our case,
we can provide the interface F
by specifying how to answer each possible question,
as interactions which can use the interface E.
So a morphism from E to F
will first accept a question q in the signature F,
then ask a series of questions in E,
and finally produce an answer for the
original question q.

As an example,
I've shown a morphism which implements queue operations
by performing series of array operations.
The deq operation is as before,
and the enq operation works in a similar way.

21.
Morphisms add a new dimension of compositionality
to our model.
If we have a computation
which asks questions in a signature F,
and a morphism from E to F,
then we can combine them
so that every question of x
is translated by the morphism.
The result is a computation
asking question in E,
with produces the same outcome as x.

As an example,
if we have code which rotates a queue
by performing a deq operation
followed by an enq operation,
we can use the morphism we have defined before
to translate these queue operations into
the corresponding array operations.

This basic substitution operation
can then be used to define
the composition of morphisms.

22.
So now we have almost everything we need
to describe certified abstraction layers.
However in CertiKOS,
our layer interfaces are specified using
sets of abstract states.
In this model,
we can add them by annotating
every question and answer
with the current state.
Then the queue and array can be specified
in the following way.

For the queue,
states are just sequences of values.
The specification does not perform any operation itself,
so it can be modelled as a morphism
from the empty signature.
The enqueue operation simply returns the unit value,
annotated with a state which has been updated
to include the new value.
The dequeue operation uses angelic nondeterminism
to decompose the queue into a head and tail.
The head is returned,
and the tail becomes the new state of the queue.

I won't describe the array operations in detail,
but we can use a similar approach to specify them.

23.
The last step is data abstraction.
As before,
if we have two layer interfaces which use different kinds of states,
we can define a simulation relation R between them.
This relation can then be embedded as
adjoint morphisms,
which will translate between the states
of the two interfaces.

In the case of queues and arrays,
the relation will read out values stored in the array
between the two counters,
and compare them with the contents of the queue.
Applying this relation to our queue interface,
we will get a specification
for a queue implemented in terms of array states.
This specification uses angelic nondeterminism
to decode a queue from an array,
and demonic nondeterminism to choose a representation
for the new queue.

24.
And that's it!
We can now describe certified abstraction layers in our model.

For each layer interface,
we give a signature, a set of states,
and a morphism giving specifications for its operations.
Then the code $M_q$ running on the underlay interface

can be lifted to pass around its state,
and the simulation relation can be used to

translate the states of the overlay specification.
The layer correctness property can then be expressed
as a refinement between these two morphisms.
However,
certified abstraction layers
are a very specific use case
for our semantic model.

